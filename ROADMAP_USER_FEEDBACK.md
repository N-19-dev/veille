# Roadmap d'impl√©mentation - Retours utilisateurs
**Date**: 2025-12-18
**Bas√© sur**: 14 r√©ponses utilisateurs

## üìä √âtat des lieux : CE QUI EXISTE D√âJ√Ä

### ‚úÖ Points forts actuels

**Sources & Contenu**
- ‚úÖ **69 sources configur√©es** (blogs tech, engineering blogs, newsletters)
- ‚úÖ **12 sources communautaires** de qualit√© (Joe Reis, Seattle Data Guy, etc.)
- ‚úÖ **D√©tection REX automatique** (34 patterns : "how we built", "our experience", etc.)
- ‚úÖ **Classification technique/rex** : articles communautaires vs corporate

**Cat√©gorisation**
- ‚úÖ **8 cat√©gories th√©matiques** align√©es avec les besoins utilisateurs :
  - üèõÔ∏è Warehouses & Query Engines (Modern Data Stack)
  - üîÑ Orchestration, ETL & Data Movement
  - üìê Data Modeling, Governance & Quality
  - üóÑÔ∏è Data Lakes, Storage & Formats
  - ‚òÅÔ∏è Cloud, Infra & Observability
  - üêç Python, Analytics & Tools
  - ü§ñ AI for Data Engineering (AI/LLM)
  - üì∞ Tech / Cloud / IA News

**Filtrage qualit√©**
- ‚úÖ **Scoring multi-crit√®res** : s√©mantique (55%), source (20%), qualit√© (15%), tech (10%)
- ‚úÖ **Seuils par cat√©gorie** (55-58 pour le contenu principal, 70+ pour news)
- ‚úÖ **Filtrage hors sujet** automatique (threshold 100)

**Interface**
- ‚úÖ **Filtrage par type** : REX / Technical
- ‚úÖ **Recherche** dans tous les articles
- ‚úÖ **Top 3 hebdomadaire**

---

## ‚ùå CE QUI MANQUE (Demandes utilisateurs)

### üî¥ Priorit√© 1 - Filtrage "Anti-Bruit"

**Probl√®me identifi√© par 64% des r√©pondants** : tutos d√©butants et pubs ind√©sirables

#### 1.1 D√©tecter et exclure les tutos d√©butants
- ‚ùå Mots-cl√©s √† d√©tecter : "introduction to", "getting started", "tutorial for beginners", "hello world", "d√©buter avec"
- ‚ùå Patterns √† filtrer : "step-by-step guide", "from scratch", "for dummies"
- ‚ùå Badge de niveau : D√©butant/Interm√©diaire/Avanc√©

#### 1.2 D√©tecter et exclure les publicit√©s d√©guis√©es
- ‚ùå Identifier : mentions r√©p√©t√©es de produits commerciaux
- ‚ùå Filtrer : liens affili√©s, contenu sponsoris√©
- ‚ùå D√©tecter : langage marketing ("r√©volutionnaire", "game-changer")

#### 1.3 Filtrer les news business non techniques
- ‚ùå Exclure : lev√©es de fonds, acquisitions, nominations
- ‚ùå Conserver : annonces de nouvelles versions/features techniques

### üü† Priorit√© 2 - Nouvelles sources (50% utilisent ces plateformes)

- ‚ùå **Reddit** r/dataengineering (mentionn√© par 7 r√©pondants)
- ‚ùå **HackerNews** (mentionn√© par 7 r√©pondants)
- ‚ùå **LinkedIn posts** d'influenceurs Data (mentionn√© par 9 r√©pondants)
- ‚ùå **YouTube** cha√Ænes techniques (mentionn√© par quelques r√©pondants)

### üü° Priorit√© 3 - Am√©liorations UX

- ‚ùå **Sources visibles** : afficher la source de chaque article
- ‚ùå **Filtres avanc√©s** : par source, par niveau, par date
- ‚ùå **Bookmarks** : sauvegarder les articles int√©ressants
- ‚ùå **Mode digest** : newsletter hebdomadaire personnalis√©e
- ‚ùå **Historique de lecture** : marquer les articles lus

---

## üéØ PLAN D'IMPL√âMENTATION PROGRESSIF

### Phase 1Ô∏è‚É£ : Quick Wins (Semaine 1-2)

**Objectif** : Am√©liorer le filtrage sans changer l'architecture

#### 1. Filtrage anti-bruit niveau d√©butant
```python
# Ajouter dans content_classifier.py
BEGINNER_KEYWORDS = [
    "introduction to", "getting started", "tutorial for beginners",
    "hello world", "step-by-step", "from scratch", "for dummies",
    "d√©buter avec", "introduction √†", "premier pas"
]

def detect_beginner_content(title: str, content: str) -> bool:
    """D√©tecte si l'article est niveau d√©butant"""
    text = (title + " " + content).lower()
    return any(keyword in text for keyword in BEGINNER_KEYWORDS)
```

#### 2. D√©tection de contenu promotionnel
```python
MARKETING_KEYWORDS = [
    "game-changer", "revolutionary", "disruptive",
    "unlock", "transform", "revolutionize",
    "sponsored", "partner content", "affiliate"
]

def detect_promotional_content(title: str, content: str) -> int:
    """Score de marketing : 0-100"""
    text = (title + " " + content).lower()
    score = sum(10 for keyword in MARKETING_KEYWORDS if keyword in text)
    return min(score, 100)
```

#### 3. Badge de niveau technique dans le frontend
```typescript
// Ajouter dans ArticleCard.tsx
type TechLevel = 'beginner' | 'intermediate' | 'advanced';

const LevelBadge = ({ level }: { level: TechLevel }) => {
  const colors = {
    beginner: 'bg-green-100 text-green-800',
    intermediate: 'bg-yellow-100 text-yellow-800',
    advanced: 'bg-red-100 text-red-800'
  };
  return <span className={`badge ${colors[level]}`}>{level}</span>;
};
```

**Livrables** :
- [x] D√©tection automatique niveau d√©butant
- [x] Filtre anti-publicit√©
- [x] Badge de niveau visible sur chaque article
- [x] Exclusion automatique du contenu d√©butant

---

### Phase 2Ô∏è‚É£ : Nouvelles sources (Semaine 3-4)

**Objectif** : Int√©grer Reddit et HackerNews

#### 1. Scraper Reddit r/dataengineering
```python
# Nouveau fichier: scrapers/reddit_scraper.py
import praw

def fetch_reddit_posts(subreddit: str, limit: int = 50):
    """R√©cup√®re les top posts de r/dataengineering"""
    reddit = praw.Reddit(...)
    posts = reddit.subreddit(subreddit).hot(limit=limit)

    return [{
        'title': post.title,
        'url': post.url,
        'score': post.score,
        'comments': post.num_comments,
        'created': post.created_utc
    } for post in posts]
```

#### 2. Scraper HackerNews
```python
# Nouveau fichier: scrapers/hackernews_scraper.py
import requests

def fetch_hackernews_top(limit: int = 50):
    """R√©cup√®re les top stories HackerNews"""
    top_stories = requests.get('https://hacker-news.firebaseio.com/v0/topstories.json').json()

    articles = []
    for story_id in top_stories[:limit]:
        story = requests.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json').json()
        if story.get('url'):
            articles.append(story)

    return articles
```

**Livrables** :
- [ ] Int√©gration Reddit avec filtrage par upvotes
- [ ] Int√©gration HackerNews avec filtrage par score
- [ ] D√©duplication des URLs entre sources
- [ ] Crawl automatique quotidien

---

### Phase 3Ô∏è‚É£ : Am√©liorations UX (Semaine 5-6)

**Objectif** : Interface plus riche et personnalisable

#### 1. Affichage des sources
```typescript
// Am√©liorer ArticleCard.tsx
<div className="source-info">
  <img src={faviconUrl} alt={source} />
  <span>{source}</span>
  <LevelBadge level={level} />
</div>
```

#### 2. Filtres avanc√©s
```typescript
// Nouveau composant: AdvancedFilters.tsx
const AdvancedFilters = () => {
  return (
    <div className="filters">
      <FilterBySource sources={allSources} />
      <FilterByLevel levels={['beginner', 'intermediate', 'advanced']} />
      <FilterByDate range={['today', 'week', 'month']} />
      <FilterByCategory categories={allCategories} />
    </div>
  );
};
```

#### 3. Bookmarks (localStorage)
```typescript
// Nouveau hook: useBookmarks.ts
export const useBookmarks = () => {
  const [bookmarks, setBookmarks] = useState<string[]>([]);

  useEffect(() => {
    const saved = localStorage.getItem('bookmarks');
    if (saved) setBookmarks(JSON.parse(saved));
  }, []);

  const addBookmark = (articleUrl: string) => {
    const updated = [...bookmarks, articleUrl];
    setBookmarks(updated);
    localStorage.setItem('bookmarks', JSON.stringify(updated));
  };

  return { bookmarks, addBookmark };
};
```

**Livrables** :
- [ ] Sources visibles et cliquables
- [ ] Filtres avanc√©s multi-crit√®res
- [ ] Syst√®me de bookmarks
- [ ] Historique de lecture (localStorage)

---

### Phase 4Ô∏è‚É£ : Features avanc√©es (Semaine 7-8)

**Objectif** : Personnalisation et engagement

#### 1. Mode digest personnalis√©
```python
# Nouveau fichier: generate_digest.py
def generate_personalized_digest(user_preferences: dict):
    """G√©n√®re un digest personnalis√© bas√© sur les pr√©f√©rences"""
    articles = query_articles(
        categories=user_preferences['categories'],
        sources=user_preferences['sources'],
        level=user_preferences['level'],
        exclude_beginner=user_preferences['exclude_beginner']
    )

    return format_digest_email(articles)
```

#### 2. LinkedIn posts scraping
```python
# Scraper LinkedIn (n√©cessite authentification)
# Option 1: Via API LinkedIn (payant)
# Option 2: Via RSS des profils publics (limit√©)
# Option 3: Via scraping (risqu√©, TOS violation)
```

**Livrables** :
- [ ] Digest hebdomadaire personnalis√©
- [ ] Syst√®me de pr√©f√©rences utilisateur
- [ ] LinkedIn posts (si faisable l√©galement)
- [ ] Export vers Notion/Obsidian

---

## üéØ M√âTRIQUES DE SUCC√àS

Pour valider que les am√©liorations r√©pondent aux attentes :

### M√©triques quantitatives
- **Taux de satisfaction** : NPS > 50
- **Articles lus/session** : > 3 articles
- **Taux de retour** : > 40% reviennent chaque semaine
- **Temps moyen** : > 5 min par visite
- **Bookmarks** : > 20% des articles bookmark√©s

### M√©triques qualitatives
- **Feedback sur filtrage** : "Le contenu est plus pertinent"
- **R√©duction du bruit** : "Moins de tutos d√©butants"
- **D√©couvrabilit√©** : "J'ai trouv√© des contenus que je n'aurais pas vus ailleurs"

---

## üöÄ COMMENCER PAR O√ô ?

### Quick Wins imm√©diats (Cette semaine)

1. **Ajouter le filtrage anti-d√©butant** dans `content_classifier.py`
2. **D√©tecter le contenu promotionnel** avec un score marketing
3. **Afficher les badges de niveau** dans le frontend
4. **Am√©liorer l'affichage des sources** dans ArticleCard

### Prochaines it√©rations

1. **Reddit & HackerNews** : Nouvelles sources tr√®s demand√©es
2. **Filtres avanc√©s** : Permettre personnalisation
3. **Bookmarks** : Feature d'engagement essentielle

---

## üìù NOTES IMPORTANTES

### Alignement avec les retours utilisateurs

‚úÖ **64% rejettent les tutos d√©butants** ‚Üí Priorit√© 1 : filtrage anti-bruit
‚úÖ **50% utilisent Reddit/HackerNews** ‚Üí Priorit√© 2 : nouvelles sources
‚úÖ **57% int√©ress√©s Modern Data Stack** ‚Üí D√©j√† couvert par cat√©gories existantes
‚úÖ **50% rejettent les pubs** ‚Üí Priorit√© 1 : d√©tection promotionnelle

### Contraintes techniques

‚ö†Ô∏è **LinkedIn scraping** : Risque de violation TOS, n√©cessite API officielle
‚ö†Ô∏è **YouTube** : Complexe (transcription, qualit√© variable), √† prioriser plus tard
‚ö†Ô∏è **Reddit API** : Rate limits (60 req/min), n√©cessite authentification OAuth
‚ö†Ô∏è **HackerNews API** : Pas de rate limit mais 1 req par item (lent)

---

## üéâ OBJECTIF FINAL

Devenir **LA r√©f√©rence en veille Data Engineering** avec :

- ‚úÖ Contenu 100% pertinent (pas de bruit)
- ‚úÖ Sources fiables et reconnues
- ‚úÖ Filtrage intelligent et personnalisable
- ‚úÖ Interface riche et agr√©able
- ‚úÖ Communaut√© engag√©e et satisfaite

**Prochaine √©tape** : Impl√©menter la Phase 1 (Quick Wins) cette semaine.
