{
  "ai_data_engineering": [
    {
      "url": "https://blog.langchain.com/in-software-the-code-documents-the-app-in-ai-the-traces-do/",
      "title": "In software, the code documents the app. In AI, the traces do.",
      "summary": "TL;DR In traditional software, you read the code to understand what the app does - the decision logic lives in your codebase In AI agents, the code is just scaffolding - the actual decision-making happens in the model at runtime Because of this, the source of truth for what",
      "published_ts": 1768066767,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.blog.langchain.com/in-software-the-code-documents-the-app-in-ai-the-traces-do/",
      "title": "In software, the code documents the app. In AI, the traces do.",
      "summary": "TL;DR In traditional software, you read the code to understand what the app does - the decision logic lives in your codebase In AI agents, the code is just scaffolding - the actual decision-making happens in the model at runtime Because of this, the source of truth for what",
      "published_ts": 1768066767,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/AI-Reshaping-Consumer-Shopping-Habits",
      "title": "A Revolution Unfolding: AI Reshaping Consumer Shopping Habits",
      "summary": "Emerging AI agents and open protocols like AP2 and ACP are fundamentally reshaping consumer shopping habits by automating research, discovery, and the path to purchase for brands and retailers.",
      "published_ts": 1768004040,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/thumbtack-powering-safe-smart-home-services-databricks-genai",
      "title": "Thumbtack Powering Safe, Smart Home Services on Databricks with GenAI",
      "summary": "Building the Most Trusted Home Care PlatformThumbtack‚Äôs mission is simple but ambitious: ...",
      "published_ts": 1767999600,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/dignified-python-10-rules-to-improve-your-llm-agents",
      "title": "Dignified Python: 10 Rules to Improve your LLM Agents Writing Python",
      "summary": "Learn how Dagster's \"Dignified Python\" principles help developers align AI agents with intentional, readable, and performant Python. Ten rules from our Claude prompt that you can adopt.",
      "published_ts": 1767978756,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/strengthening-gitlab-com-security-mandatory-multi-factor-authentication/",
      "title": "Strengthening GitLab.com security: Mandatory multi-factor authentication",
      "summary": "To strengthen the security of all user accounts on GitLab.com, GitLab is implementing mandatory multi-factor authentication (MFA) for all users and API endpoints who sign in using a username and password. Why this is happening This move is a vital part of our Secure by Design commitment . MFA provides critical defense against credential stuffing and account takeover attacks, which remain persistent threats across the software development industry. Key information to know What is changing? GitLab is making MFA mandatory for sign-ins that authenticate with a username and password. This introduces a critical second layer of security beyond just a password. Does this apply to me? Yes, it applies if: You sign in to GitLab.com with a username and a password, or use a password to authenticate to the API. No, it does not apply if: You exclusively use social sign-on (such as Google) or single sign-on (SSO) for access. ( Please note: If you use SSO, but also have a password for direct login, you will still need MFA for any non-SSO, password-based login.) When is the rollout? The implementation will be a phased approach over the coming months, intended to both minimize unexpected interruptions and productivity loss for users and prevent account lockouts. Groups of users will be asked to enable MFA over time. Each group will be selected based on the actions they‚Äôve taken or the code they‚Äôve contributed to. You will be notified in the following ways: ‚úâÔ∏è Email notification - prior to the phase where you will be impacted üîî Regular in-product reminders - 14 days before ‚è±Ô∏è After a specific time period (this will be shared via email) - blocked from accessing GitLab until you enable MFA What action do I need to take? If you sign in to GitLab.com with a username and a password: We highly recommend you proactively set up one of the available MFA methods today, such as passkeys, an authenticator app, a WebAuthn device, or email verification. This ensures the most secure and seamless transition: Go to your GitLab.com User Settings . Select the Account section. Activate two-factor authentication and configure your preferred method (e.g., authenticator app or a WebAuthn device). Securely save your recovery codes to guarantee you can regain access if needed. If you use a password to authenticate to the API: We  highly recommend you proactively switch to a personal access token (PAT). Read our documentation to learn more. FAQ What happens if I don't enable MFA by the deadline? You'll be required to set up MFA before you can sign in. Does this affect CI/CD pipelines or automation? Yes, unless you're using PATs or deploy tokens instead of passwords. I use SSO but sometimes sign in directly, do I need MFA? Yes, MFA is required for any password-based authentication, including fallback scenarios. Specific timelines and further resources will be shared as rollout dates approach. Thank you for your attention to this important change.",
      "published_ts": 1767916800,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.youtube.com/shorts/EJPisMWTMTA",
      "title": "The Power BI MCP Server is changing the game of how you do Power BI!",
      "summary": "Check out the full video: https://youtu.be/7UapKxtxQUo\n\nüì¢ Become a member: https://guyinacu.be/membership \n\n*******************\n\nWant to take your Power BI skills to the next level? We have training courses available to help you with your journey.\n\nüéì Guy in a Cube courses: https://courses.guyinacube.com/\n\n*******************\nLET'S CONNECT!\n*******************\n\n-- https://bsky.app/profile/guyinacube.bsky.social\n-- http://twitter.com/guyinacube\n-- http://www.facebook.com/guyinacube\n\n\n#PowerBI #GuyInACube",
      "published_ts": 1767886218,
      "source_name": "Guy in a Cube (YouTube)",
      "content_type": "technical"
    },
    {
      "url": "https://blog.ovhcloud.com/agentic-ai-from-a-security-perspective/",
      "title": "Agentic AI from a security perspective",
      "summary": "Large Language Models (LLMs) and generative AI technologies are everywhere, infiltrating both our personal and professional daily lives. Well-known services are already diverting most internet users away from their old browsing habits, and online information consumption is being profoundly transformed, most likely with no possible return to past behaviours. Issues related to intellectual property laws [‚Ä¶]",
      "published_ts": 1767884313,
      "source_name": "OVHcloud Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/evaluating-model-behavior-through-chess",
      "title": "Evaluating AI Models Through Chess: What Stateful Games Reveal",
      "summary": "Chess provides a structured, stateful environment for evaluating AI model behavior over time. Learn how simulated tournaments expose failure modes that benchmarks miss.",
      "published_ts": 1767818021,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.octo.com/choisir-le-bon-framework-la-bonne-librairie-javascript--un-guide-pratique-pour-chaque-type-de-projet",
      "title": "Choisir le bon framework, la bonne librairie JavaScript : un guide pratique pour chaque type de projet",
      "summary": "Tu h√©sites entre React, Vue, Angular ou Remix ? Ne choisis pas √† l‚Äôaveugle ! Ce guide d√©voile le framework JavaScript vraiment adapt√© √† ton projet. Comparatifs explosifs, perfs, s√©curit√©, tests‚Ä¶ Spoiler : le meilleur n‚Äôest pas toujours celui que tu crois üëÄ",
      "published_ts": 1767807670,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.atspotify.com/2026/1/why-we-use-separate-tech-stacks-for-personalization-and-experimentation/",
      "title": "Why We Use Separate Tech Stacks for Personalization and Experimentation",
      "summary": "The technical and practical rationale for a clear separation between these domains. The post Why We Use Separate Tech Stacks for Personalization and Experimentation appeared first on Spotify Engineering .",
      "published_ts": 1767796871,
      "source_name": "Spotify Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/2025-owasp-top-10-whats-changed-and-why-it-matters/",
      "title": "OWASP Top 10 2025: What's changed and why it matters",
      "summary": "The OWASP Foundation has released the eighth edition of its influential \"Top 10 Security Risks\" list for 2025 ,\nintroducing significant changes that reflect the evolving landscape of application security. Based on analysis\nof more than 175,000 Common Vulnerabilities and Exposures (CVEs) records and feedback from security practitioners across the globe, this update addresses\nmodern attack vectors. Here's everything you need to know about what's changed, why these changes matter,\nand how to protect your systems. üí° Join GitLab Transcend on February 10 to learn how agentic AI transforms software delivery. Hear from customers and discover how to jumpstart your own modernization journey. Register now. What's new in 2025? The shift from 2021 (the last time the list came out) to 2025 represents more than minor adjustments, it's a fundamental shift in application security.\nTwo entirely new categories entered the list and one category was consolidated into another, which highlights emerging risks\nthat traditional testing often misses. These additions and shifts can be seen in the chart below: Two new categories A03: Software Supply Chain Failures : Expands the 2021 category \"Vulnerable and Outdated Components\" to encompass the entire software supply chain, including dependencies, build systems, and distribution infrastructure. Despite having the fewest occurrences in testing data, this category has the highest average exploit and impact scores from CVEs. A10: Mishandling of Exceptional Conditions : Focuses on improper error handling, logical errors, and failing open scenarios. This addresses how systems respond to abnormal conditions. Major ranking changes Security Misconfiguration surged from #5 (2021) to #2 (2025), now affecting 3% of tested applications. Server-Side Request Forgery (SSRF) has been consolidated into A01: Broken Access Control. Cryptographic Failures dropped from #2 to #4. Injection fell from #3 to #5. Insecure Design moved from #4 to #6. Why these changes were made The OWASP methodology combines data-driven analysis with community insights. The 2025 edition analyzed 589\nCommon Weakness Enumerations (CWEs), which is a substantial increase from the approximately 400 CWEs in 2021.\nThis expansion reflects the growing complexity of modern software systems and the need to capture emerging threats. The community survey component addresses a fundamental limitation: testing data essentially looks into the past.\nBy the time security researchers develop testing methodologies and integrate them into automated tools, years may\nhave passed. The two community-voted categories ensure that emerging risks identified by frontline practitioners\nare included, even if they're not yet prevalent in automated testing data. The rise of Security Misconfiguration highlights an industry trend toward configuration-based security,\nwhile Software Supply Chain Failures acknowledges the rise of sophisticated attacks targeting compromised packages. Using GitLab Ultimate for vulnerability detection and management GitLab Ultimate provides comprehensive security scanning to detect risks across the\n2025 OWASP Top 10 categories. For instance, the end-to-end platform analyzes your project's source code, dependencies, and infrastructure\ndefinitions. It also uses Advanced Static Application Security Testing (SAST) to detect injection flaws,\ncryptographic failures, and insecure design patterns in source code. Infrastructure as Code (IaC) scanning finds\nsecurity misconfigurations in your deployment definitions. Secret Detection prevents the leakage of credentials, and Dependency Scanning uncovers libraries with known vulnerabilities in your software supply chain, which directly\naddresses the new A03 category for Software Supply Chain Failures. In addition: Dynamic Application Security Testing (DAST) probes your deployed application for broken access control,\nauthentication failures, and injection vulnerabilities by simulating attack vectors. API Security Testing probes your API endpoints for input validation weaknesses and authentication bypasses. Web API Fuzz Testing uncovers how your application handles exceptional conditions by generating unexpected inputs, which directly\naddresses the new A10 category for mishandling of exceptional conditions. Security scanning integrates seamlessly into your CI/CD pipeline , running when code is pushed from a feature\nbranch so developers can remediate vulnerabilities before they reach production. Security findings are consolidated in\nthe Vulnerability Report , where security\nteams can triage, analyze, and track remediation. GitLab also allows you to leverage AI agents such as Security Analyst Agent , available in GitLab Duo Agent Platform, to quickly determine what are the most critical vulnerabilities and how to take action on\nthem. You can enforce additional controls through merge request approval policies and pipeline execution policies to ensure security scanning runs consistently across your organization. Customer Success and Professional Services teams at GitLab ensure you derive value from an investment in GitLab in a timely manner. Deliver secure software faster with security testing in the same platform developers already use.\nTo learn more, visit our application security testing solutions site . The OWASP Top 10 2025: Complete breakdown A01: Broken Access Control What it is Failures in enforcing policies that prevent users from acting outside their intended permissions,\nleading to unauthorized access. Impact on your system Unauthorized information disclosure Complete data destruction or data modification Privilege escalation (users gaining admin rights) Viewing or editing other users' accounts API access from unauthorized or untrusted sources Notable CWEs CWE-22: Path Traversal CWE-200: Exposure of Sensitive Information to an Unauthorized Actor CWE-352: Cross-Site Request Forgery (CSRF) A02: Security Misconfiguration What it is Systems, applications, or cloud services configured incorrectly from a security perspective. Impact on your system Exposure of sensitive information through error messages Unauthorized access through default accounts Unnecessary services or features enabled Outdated security patches Server does not send security headers or directives Notable CWEs CWE-16: Configuration CWE-521: Weak Password Requirements CWE-798: Use of Hard-coded Credentials A03: Software Supply Chain Failures What it is Breakdowns or compromises in building, distributing, or updating software through vulnerabilities or malicious changes in dependencies, tools, or build processes. Impact on your system: Compromised packages introducing backdoors Malicious code injected during build processes Vulnerable dependencies cascading through your application Use of components from untrusted sources in production Changes within your supply chain are not tracked Notable CWEs CWE-1395: Dependency on Vulnerable Third-Party Component CWE-1104: Use of Unmaintained Third Party Components A04: Cryptographic Failures What it is Failures related to lack of cryptography, insufficiently strong cryptography, leaking of cryptographic keys, and related errors. Impact on your system: Sensitive data exposure (passwords, credit cards, health records) Man-in-the-middle attacks Data breach through weak encryption Key compromise leading to system-wide exposure Regulatory compliance failures (GDPR, PCI DSS) Notable CWEs CWE-327: Use of a Broken or Risky Cryptographic Algorithm CWE-330: Use of Insufficiently Random Values A05: Injection What it is System flaws allowing attackers to insert malicious code or commands (SQL, NoSQL, OS commands, LDAP, etc.) into programs. Impact on your system Data loss or corruption through SQL injection Complete database compromise Server takeover through command injection Cross-site scripting (XSS) attacks Information disclosure Denial of service Notable CWEs CWE-89: SQL Injection CWE-78: OS Command Injection A06: Insecure Design What it is Weaknesses in design representing different failures, expressed as missing or ineffective control design‚Äîarchitectural flaws rather than implementation bugs. Impact on your system Weak password reset flows Missing authorization steps Flawed business logic allowing bypasses Inadequate threat modeling leading to blind spots Design patterns that fail under attack scenarios Notable CWEs CWE-209: Generation of Error Messages Containing Sensitive Information CWE-522: Insufficiently Protected Credentials CWE-656: Reliance on Security Through Obscurity A07: Authentication Failures What it is Vulnerabilities allowing attackers to trick systems into recognizing invalid or incorrect users as legitimate. Impact on your system Account takeover and credential stuffing Session hijacking Brute force attacks succeeding Weak password recovery mechanisms exploited Multi-factor authentication bypass Notable CWEs CWE-287: Improper Authentication CWE-306: Missing Authentication for Critical Function CWE-521: Weak Password Requirements A08: Software or Data Integrity Failures What it is Code and infrastructure failing to protect against invalid or untrusted code/data being treated as trusted and valid. Impact on your system Unsigned updates allowing malicious code injection Insecure deserialization leading to remote code execution CI/CD pipeline compromise Auto-update mechanisms exploited Tampered software artifacts Notable CWEs CWE-345: Insufficient Verification of Data Authenticity CWE-346: Origin Validation Error CWE-347: Improper Verification of Cryptographic Signature A09: Security Logging & Alerting Failures What it is Insufficient logging and monitoring with inadequate alerting, which makes rapid response difficult. Impact on your system Attacks go undetected for extended periods Breach investigation becomes impossible Compliance violations from lack of audit trails Delayed incident response Inability to determine scope of compromise Notable CWEs CWE-117: Improper Output Neutralization for Logs CWE-532: Insertion of Sensitive Information into Log File CWE-778: Insufficient Logging A10: Mishandling of Exceptional Conditions What it is Programs failing to prevent, detect, and respond to unusual and unpredictable situations, which leads to crashes, unexpected behavior, or vulnerabilities. Impact on your system Information disclosure through verbose error messages Denial of service from unhandled exceptions State corruption from improper error handling Race conditions exploited Systems failing open instead of closed Application crashes exposing sensitive data Notable CWEs CWE-248: Uncaught Exception CWE-390: Detection of Error Condition Without Action CWE-391: Unchecked Error Condition Prevention and remediation best practices GitLab provides tools to enable you to not only quickly find and remediate vulnerabilities within the OWASP Top 10,\nbut also to prevent them from making it into your production system. By following these best practices you can enhance\nand maintain your security posture: Automated security scanning for all repositories Perform SAST Scanning to detect insecure design patterns like plaintext password storage, inadequate error handling, and missing encryption during code review, catching design flaws early in the development lifecycle. Perform Secret Detection to identify credentials in configuration files, environment variables, and code, preventing plaintext password storage and ensuring secrets are properly managed through GitLab's CI/CD variables with masking and encryption. Perform DAST Scanning to detect broken access control vulnerabilities Perform Dependency Scanning to scan project dependencies against vulnerability databases, identifying known CVEs in direct and transitive dependencies across multiple package managers (npm, pip, Maven, etc.). Perform Container Scanning to analyze Docker images for vulnerable base layers and packages, ensuring container supply chain security before deployment. Perform IaC Scanning to check your infrastructure definition files for known vulnerabilities. Leverage API Security Tools to secure and protect web APIs from unauthorized access, misuse, and attacks. Perform Web API Fuzz Testing to discover bugs and potential vulnerabilities that other QA processes might miss. View vulnerabilities detected in MR with diff from feature branch to main branch. Understand your security posture Generate a software bill of materials (SBOM) for complete dependency visibility and compliance requirements. Leverage the Vulnerability Report to sort through and triage vulnerabilites via consolidated view of security vulnerabilities found in your codebase. Quickly take action on vulnerabilities using detailed remdiation guidance and risk assessment data . Use Security Iventory to visualize which assets you need to secure and understand the actions you need to take to improve security. Leverage Compliance Center to manage compliance standards adherence reporting, violations reporting, and compliance frameworks. Use Security Inventory to viewing enabled security scanners and vulnerabilities. Set up prevention and maintain documentation Configure Security Policies to block merges or deployments when high-severity vulnerabilities are detected in dependencies, enforcing security standards automatically. Use Compliance Frameworks to enforce organizational security standards through automated policy checks that verify encryption requirements, credential management practices, and secure workflow implementations are followed. Use GitLab Wiki and repository documentation to maintain security design principles, approved patterns, and architectural decision records that guide developers toward secure-by-design implementations . Implement merge request approval rules requiring security architect review for features involving authentication, authorization, encryption, or sensitive data handling, ensuring design-level security validation. Create tests to verify input validation and allowlist approaches for file paths Use GitLab Issues and Epics to document security requirements and threat models during the design phase, creating a traceable record of security decisions and ensuring security considerations are addressed before implementation begins. View and set Security Policies scoped to instance, group, or project. Leverage AI Use Code Suggestions for proactive guidance during development, suggesting secure design patterns like proper password hashing (bcrypt, Argon2), encrypted storage mechanisms, and appropriate error handling that doesn't leak sensitive information. Use Security Analyst Agent to review detected insecure design vulnerabilities in context, explaining the architectural implications, assessing risk based on your application's threat model, and providing remediation strategies that address root design flaws rather than just symptoms. Review your code using AI to help ensure consistent code review standards in your project. Leverage Security Analyst Agent to quickly triage and assess security vulnerabilities. Key takeaways for development teams Supply chain security is critical : With A03's addition and high-impact scores, securing your software supply chain is no longer optional. Implement SBOM tracking, dependency scanning, and integrity verification throughout your pipeline. Configuration matters more than ever : The rise to #2 shows that configuration-based security is now a primary attack vector. Automate configuration verification and implement IaC with security baked in. Traditional threats persist : While Injection and Cryptographic Failures dropped in ranking, they remain critical. Don't deprioritize them just because they've fallen on the list. Error handling is security : The new A10 category emphasizes that how your application handles failures is a security concern. Implement secure error handling from the start. Testing must evolve : The expanded CWE coverage (589 vs. 400 in 2021) means testing strategies must be comprehensive. Combine SAST, DAST, source code analysis, and manual penetration testing for effective coverage. Explore our GitLab Security and Governance Solutions and security scanning documentation to start strengthening your\nsecurity posture today.",
      "published_ts": 1767744000,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/amazon-emr-serverless-eliminates-local-storage-provisioning-reducing-data-processing-costs-by-up-to-20/",
      "title": "Amazon EMR Serverless eliminates local storage provisioning, reducing data processing costs by up to 20%",
      "summary": "In this post, you'll learn how Amazon EMR Serverless eliminates the need to configure local disk storage for Apache Spark workloads through a new serverless storage capability. We explain how this feature automatically handles shuffle operations, reduces data processing costs by up to 20%, prevents job failures from disk capacity constraints, and enables elastic scaling by decoupling storage from compute.",
      "published_ts": 1767739540,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/scaling-sales-agents-engineering-next-gen-ai-for-the-enterprise-era/",
      "title": "Scaling Sales Agents: Engineering Next-Gen AI for the Enterprise Era",
      "summary": "By Rakesh Nagaraju, Shweta Joshi, Guru Prasad, Renuka Prasad and Ashraya Raj Mathur. In our Engineering Energizers Q&A series, we highlight the engineering minds driving innovation across Salesforce. Today we meet Shweta Joshi, Software Engineering Architect for the Engagement Agent, a generative-AI powered system automating personalized sales follow-ups and outreach across large lead volumes. Her [‚Ä¶] The post Scaling Sales Agents: Engineering Next-Gen AI for the Enterprise Era appeared first on Salesforce Engineering Blog .",
      "published_ts": 1767716463,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.youtube.com/shorts/OlE0R4wCPgg",
      "title": "Let's install the Power BI MCP Server!",
      "summary": "Check out the full video here: https://youtu.be/umZReVr10Xw\n\nüì¢ Become a member: https://guyinacu.be/membership \n\n*******************\n\nWant to take your Power BI skills to the next level? We have training courses available to help you with your journey.\n\nüéì Guy in a Cube courses: https://courses.guyinacube.com/\n\n*******************\nLET'S CONNECT!\n*******************\n\n-- https://bsky.app/profile/guyinacube.bsky.social\n-- http://twitter.com/guyinacube\n-- http://www.facebook.com/guyinacube\n\n\n#PowerBI #GuyInACube",
      "published_ts": 1767713417,
      "source_name": "Guy in a Cube (YouTube)",
      "content_type": "technical"
    },
    {
      "url": "https://analyticshour.io/2026/01/06/288-our-llm-suggested-we-chat-about-mcp-kinda-meta-no/",
      "title": "#288: Our LLM Suggested We Chat about MCP. Kinda‚Äô Meta, No?",
      "summary": "If there‚Äôs one thing that we absolutely knew would be coming along with the increased interest and use of AI, it would be‚Ä¶ more acronyms! And, along with the acronyms, we pretty much could predict that we see a lot of online flexing through casual dropping of said acronyms as though they‚Äôre deeply understood by everyone who‚Äôs anyone. We tackled one such acronym on this episode: MCP! That‚Äôs ‚Äúmodel context protocol‚Äù for those who like their acronyms written out, and Sam Redfern joined us to help us wrap our heads around the topic. You see, MCP is kinda‚Äô like some other more familiar acronyms like API and XML. But, it‚Äôs also like‚Ä¶ fingers? Sam‚Äôs enthusiasm and explanation certainly had us ready to dive in! This episode‚Äôs Measurement Bite from show sponsor Recast is an explanation of model robustness from Michael Kaminsk y! Links to Resources Mentioned in the Show Cursor History doesn‚Äôt repeat itself, but it often rhymes Zed Agentic Engineering Series MeasureCamp GitHub‚Äôs Official MCP Server Zed ACP Opencode.ai (Podcast) Good Hang with Amy Poehler (including the Rachel Dratch episode ) Bhavik (Bhav) Patel Manas Datta Superweek Christopher Berry Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models Normally, the image we drop on an episode is a photo taken by a human, and we attribute it accordingly. This time, given the topic, we just couldn‚Äôt resist, though: we threw the entire transcript at Nano Banana with a minimalist prompt to see what it came back with. Episode Transcript 00:00:05.75 [Announcer]: Welcome to the Analytics Power Hour. Analytics topics covered conversationally and sometimes with explicit language. 00:00:15.15 [Michael Helbling]: Hi, everyone. Welcome. It‚Äôs the Analytics Power Hour. This is episode 288. We spent the last decade putting walls around our data, securing it, governing it, putting labels on it. And now the AI revolution walks up and is like, hey, can I see all that? Today, we‚Äôre going to discuss Moedel Context Protocol, or MCP. I mean, it‚Äôs an open standard. It promises to stop all the copy, paste madness and let AI talk directly to your data systems. Is it the end of the data silo or just the beginning of a new governance headache? Well, we‚Äôre going to try to establish an MVP for PMF of MCP all in one hectic hour. All right, let me introduce my co-host, Val Kroll. How are you doing? I‚Äôm good. This is going to be an interesting one, yeah. All my acronyms, yeah, that was fun. All right, Tim Wilson, always a pleasure. Likewise. All right, I just used the acronyms to make myself sound smart. That‚Äôs all. Let‚Äôs get that in early. And I‚Äôm Michael Helbling. All right, well, we need a guest, someone to help us dive into this topic. And we‚Äôve got a great one. Sam Redfern is a staff data scientist at Canva, currently working on search and recommendations there and previously marketing measurement. Prior to that, he has held data roles at both Meta and IAG, and today he is our guest. Welcome to the show, Sam. 00:01:36.31 [Sam Redfern]: Thank you very much, Michael, Tim, and Val. We‚Äôre really excited to be here. First time caller, long time listener. Oh, that‚Äôs awesome. 00:01:43.76 [Michael Helbling]: Well, we‚Äôll ask questions and take our answers off the air. No, no. 00:01:48.48 [Michael Helbling]: So. 00:01:51.27 [Michael Helbling]: Sam, I‚Äôm excited to talk to you about this, because obviously, all things AI are very of the moment, and everyone sees the term MCP. But I think if we just take a step back, maybe you could just fill us in on what exactly an MCP is, model context protocol, where did it come from, give us some background on the whole concept to establish the conversation today. 00:02:17.34 [Sam Redfern]: No worries. Super pumped to be talking about this. Let‚Äôs take a step back and think about what this is solving, in a sense. We‚Äôve had access to these large language models tools for a little while now. In the early days of GPT2 and GPT3, before chat GBT, these things were like word calculators in a sense. I really like the analogy that it‚Äôs like you put the numbers into your calculator and you get the equation out. This is the same for almost words. Early large language models acted like that. And the innovation in the open AI space was to basically feed the output back into the input and make this resumable format. And what‚Äôs interesting about this whole, so let‚Äôs step back from MCP and branding and letting technical teams come up with the term for things because this is how we got NFTs as a term at the same time. But the core problem to be solved with these is that we‚Äôve got something that feels a little bit like a person in the sense that you will give it some words and it will respond with some words back. Could you give that agent or that large language model the ability to do something other than just converse? And so the first application of tool use, in a sense, in large language models, actually came from the open-source LangChain team. And for those who don‚Äôt know what LangChain is, it‚Äôs a framework for building agentic experiences. And so you can have your anthropic model your open AI model or whatever you want, and then you allows you to piece together bits of technology to add context into the large language model to try and get it output. And so in April 2023, PR was submitted to the Landtrait project, allowing it to open up for the large language model to take a browser URL and to go have the back end application request the contents of that HTML and then bring it back into the context window of the agent itself. And if you think about it as like the core thing that it‚Äôs trying to do is it‚Äôs trying to get this I think about it as fingers in a sense, is it‚Äôs trying to give the large language the ability to touch something, a bit of information, bring it closer to it for it to understand. That‚Äôs at its core what these MCP tools are. MCP is like a brand term through Anthropoc. To say it‚Äôs a standard is to be very generous. But I‚Äôm really bullish about the concept of giving these large language models access to tools for them to be able to solve problems. 00:05:22.34 [Val Kroll]: So I have to admit that I really did think that MCP was just like the API for LLMs. But the more I was looking into this, it was kind of understanding that those fingers like you use in your analogy is really giving it more access than just here‚Äôs this endpoint. And it‚Äôs just a one-time thing. Can you talk about some of the things that you give it access to with those fingers or to grab to kind of give a little bit more color to what it actually does or what it‚Äôs capable of, if that makes sense? 00:05:54.59 [Sam Redfern]: Yeah, absolutely. So this is in the weeds of how they work. I think the context to understand is if you‚Äôve ever done work inside of large enterprises and you‚Äôve tried to create an application access username and password, it‚Äôs a total pain in the butt. to go get this thing. And then your infrastructure team is like, well, you‚Äôre going to change the password every three weeks. And then you have to have a cryptographic token to do something. And it‚Äôs in some sort of space. And the reason why, actually, Anthropoc had a previous attempt at tool use in October 2024, a month before MCPs were announced, where they had a system that would take over your browser and move your cursor around. And the reason why MCPs are running on your local computer is that your user account has access to all these systems. That‚Äôs why the early paradigms of these systems are as close as possible to the end user‚Äôs system. So the analogy I give on the fingers, in a sense, is inside these MCPs, you can give it access to any number of‚Ä¶ The standard allows you to sort of use representatives any number of tools. And you have bits of information, right? So when the agent starts, it‚Äôs basically given a list of all the tools that the MCP server has available to the agent. And so has the name of the tool. And so let‚Äôs just do a really simple example of like us, like in our agent environment, we have two tools, right? One of them is called a saw and the other one‚Äôs called a drill, right? In our SOAR tool, we would describe the name as SOAR. We would say the description is it cuts wood in a single direction across a line. Then the inputs to that is the position of the wood and the depth. That is a finger for lack of a better term. That‚Äôs our first tool and our second tool might be drill. That allows you to drill a hole through the piece of wood. 00:08:03.11 [Val Kroll]: Okay, so I have to ask one more. Sorry, I‚Äôm hogging the air. But I guess the one other thing that I‚Äôm struggling to grasp a little bit is what was the need for standardization of this, like the protocol? Can you talk about what that is solving? Because what you shared in that analogy was great. I‚Äôm absolutely going to use that, and I‚Äôll give you credit every time. but like why was there a need to standardize outside of like you know enterprises you know would feel more secure with that or you know the governance would be easier but is there any more more to it than just that piece? 00:08:39.22 [Sam Redfern]: In the adoption curve, we are so far away from the governance piece on this stuff. There‚Äôs a bunch of companies right now that are trying to put governance around these systems, and I‚Äôm sure at some point we‚Äôll talk to maybe some of the potential downsides of the standard if we want to call it that. But the reason why Anthropic went down this path is In the technical details around how the LLM is trained, they have been doing this work of training the large language model to use this like special escape set of characters. So when the large language model is like Okay, I think what I need to do is use the saw tool and then it has this string of characters saw tool string of characters, and that indicates to the sort of the agent that‚Äôs hosting the large language model. Okay, I have to take the text below this. and send it into the tool itself with the input parameters that it needs for it. Anthropic had this huge lead because they‚Äôve done the work of training their large language models to for tool use and using their reinforcement techniques to basically say, this is what you have to do. And this was this huge lead that anthropic had for a couple of years, in a sense. Everything feels like it‚Äôs a couple of years. It‚Äôs really about eight months, right? Until other people started trying to solve this problem. Opening, I had their own sort of call procedures kind of method, like I think they were called functions. And it was a very similar kind of thing. Like anyone could have come up with a standard. The core problem they were trying to solve is how do you give the large language model a hand for it to basically make decisions about what information is pulled towards it or what information or what actions it takes when it‚Äôs pushing out. 00:10:36.40 [Tim Wilson]: This is slowly coming a little more into focus and still pretty damn fuzzy for me. I know recently it seems like there‚Äôs been a lot of chatter about Google Analytics having an MCP server. Is that the right terminology? That is something that that the Google team said, we‚Äôre going to produce this to basically make the, this is a saw. This is what it does. These are the inputs. And it‚Äôs just a much a lot. I mean, your analogy was very, very simple. Is it as simple as that, which has me going back to saying, well, when Val said, It‚Äôs like an API for LOMs. It sure sounds like an API for LOMs. And I‚Äôm missing where that analogy is breaking down. 00:11:35.02 [Sam Redfern]: I think you can use it. There‚Äôs a lot of analogies of talking about these NCP tools as being the early dates of APIs and stuff like that as well. I think there‚Äôs an extra bit of the near direction of where these systems are moving, which is more interesting in the API part, but just to come back to that. The way I think about it, APIs is a great way of talking about it, and there‚Äôs lots of people doing weird fun things with these tools right now. If you remember, I think some of us on the call are old enough to remember the early days of web 2.0 and people were making APIs for like the weather and it was open and everything was fine and you know we were very far from like the standardized way of we think about this sort of stuff now right like it‚Äôs uh The way you design an API is very standardized now. I think the thing that‚Äôs different is one, we‚Äôre dealing with this huge amount of non-determinism, right? And we‚Äôre dealing with all of these different terms and terminologies that exist. So I think everyone on the podcast might have heard of the term agent, right? And so an agent is the idea where you have a resumable output. You have like some text that is the system prompt, and then you have this resumable conversation. There‚Äôs another term that‚Äôs being formed right now called a harness. A harness is an idea where you have an agent and you have a tool plugged into the side of it. That has a domain of knowledge attached to it at the same time. Cursor is an agent. The claw desktop is an agent. Oh, sorry, sorry. Cursor is a harness, right? It‚Äôs got access to all these different tools. I think the, I actually think of NTP and where it‚Äôs at right now is more akin to these digital document formats like XML, right? So we started with XML and the number of people who are writing XML these days is almost none. However, the amount of change of this standardized document format then brought us to JSON and now it has unfortunately brought us to YAML and Markdown. We are at the XML stage of this development is that this is going to tool use conceptually attaching to large language models through agents and harnesses is That is going to stay for a long time. Whether it‚Äôs the MTP standard or someone comes along with a better standard, then we‚Äôll see how that goes. 00:14:12.44 [Michael Helbling]: You know how developers got the AI engineer role? It‚Äôs time for the rest of us. I think we‚Äôre witnessing the rise of the AI analyst. 00:14:22.32 [Tim Wilson]: OK, does that just mean asking a chatbot to do math? Because I have Excel for that, Michael. 00:14:28.43 [Michael Helbling]: Well, no, Tim. I‚Äôm talking about Ask Why. It‚Äôs full stack analytics. I ask a question in plain English, and the product prism orchestrates the whole thing. You can pull in data from Excel or BigQuery. 00:14:42.10 [Tim Wilson]: Hold on. You‚Äôre sending BigQuery data to an LLM? Security is going to have a heart of track. 00:14:48.17 [Michael Helbling]: Well, that‚Äôs the best part. Ask why doesn‚Äôt upload your data. Explain? Well, it creates a semantic layer. It sends the context to the LLM. The LLM writes the code, and that code runs locally on your data. Your actual numbers never touch their servers, so it‚Äôs totally traceable. 00:15:07.61 [Tim Wilson]: So, I get the automation, but my data stays safe and secure? 00:15:13.21 [Michael Helbling]: Exactly. Plus, it remembers context, so as you automate routine tasks, it stores those, so you don‚Äôt have to explain it all again the next time you do that same task. 00:15:23.83 [Tim Wilson]: OK, I‚Äôm listening. 00:15:25.81 [Michael Helbling]: Where do I get it? Well, it‚Äôs even beta right now, and you can go to ask-y.ai. That‚Äôs ask-y.ai. You can get ahead of the curve and join the ranks of the AI analysts. 00:15:39.33 [Tim Wilson]: And because we like you guys, use code APH when you sign up, and our friends at Ask Why will put you at the top of their wait list. 00:15:48.17 [Michael Helbling]: Yep, stop pasting data into black boxes. Get Ask Why. 00:15:52.99 [Tim Wilson]: that I‚Äôve had the XML question as well as whether it was, because I remember that being coming from an HTML world and then XML came out and it‚Äôs like, look, XML doesn‚Äôt give a shit about what you‚Äôre rendering in a browser, but it is this structured world. So I feel like, and then JSON, I sort of understood because of the XML. And that makes sense because there were, there was talk of saying different, applications uses would say using this XML structure, let‚Äôs define kind of specifically how that is going to be used in the context of this financial services thing. So you‚Äôre saying that is also a useful if imperfect analogy? 00:16:40.80 [Sam Redfern]: Yeah, look. Uh, history sort of rhymes more than copies, right? Like, um, you know, it‚Äôs, it‚Äôs going to, um, uh, like, It‚Äôs the first time in software that we‚Äôve had this amount of non-determinism to deal with, right? You think about what success has meant in software development or data before, and it‚Äôs like some human‚Äôs ability to remember some random function as part of a library and be able to write that code as fast as possible and do it in as perfect as close to grammar as possible. The problem with this new world that we‚Äôre going into is the skill of dealing with non-determinism is not closely overlapping with that historical set of skills. It‚Äôs going to feel different and people who talk about the vibes of a model, there‚Äôs some truth in it in a sense. Getting a feel and it‚Äôs true with tool design. When you‚Äôre building an MCP, so we‚Äôll just go back to the MCP paradigm, And so you‚Äôre thinking about the tools that you‚Äôre building and the fingers that you‚Äôre giving this agent and this harness access to. When you‚Äôre starting out, you‚Äôre doing kind of the early playful part of programming, in a sense, right? Where you‚Äôre just like, oh, does this connect to this? And when I run through it, what problems do I see with it? I do think that, like, It feels like those early days of these standards and people are playing around with them. And then when you want to get serious about the thing that you‚Äôre building, you‚Äôre then looking through the agent logs, you‚Äôre seeing what tools it‚Äôs calling, you‚Äôre seeing the parameters, you‚Äôre seeing how many times it correctly passes in the correct parameters and everything like that. 00:18:30.07 [Tim Wilson]: So let me hit the non-determinism point. And maybe I‚Äôm going to go back to using the Google Analytics MCP as an example that the The client, the application that‚Äôs hooked, the agent that is hooking in and using the MCP, say it‚Äôs a LLM, it‚Äôs core. And maybe I‚Äôm missing the non. I‚Äôm thinking of that as being it‚Äôs a it‚Äôs a probabilistic thing, but it‚Äôs hitting the MCP to get stuff back. Is the MCP necessarily also kind of non deterministic? Or is it no, like the inputs may be kind of floating around a little bit, but and or maybe it just depends on what it‚Äôs an MCP for. If in the Google Analytics example, I would say if the input is users in the last month that the MCP would say, well, as long as that, or is it that input‚Äôs going to come in with the little squishiness in it, and it‚Äôs up to the MCP to say, I got to figure out what I should go and pull and return. 00:19:39.91 [Sam Redfern]: Yeah, and I think this is a good point to talk about this interesting dimensions that‚Äôs coming up recently. If I was going on a league team, which I‚Äôm definitely not, and I don‚Äôt think I‚Äôve used Google Analytics for over a decade, and I missed many of the big transitions. I‚Äôm probably not the best person to talk about GA, but So the Google Analytics MCP is going to have this problem where they have these dimensions and measures and breakdowns, and then they‚Äôre obviously trying to do it on the cheap on the inside. And so they‚Äôre only storing some of the information. So they‚Äôre going to have a basic report tool, so it‚Äôs going to be called the report tool. And the name of it is going to be traffic analysis. And it‚Äôs going to be put the date range in, and then it‚Äôs just going to return back a very minimized array of what the traffic has been. 00:20:41.10 [Tim Wilson]: Report tool, that would be like a saw, but there‚Äôs going to be separately a drill. When you‚Äôre saying a tool, that is a tool that‚Äôs available as part of the MCP. Okay. 00:20:53.24 [Sam Redfern]: That‚Äôs right. Now, where this gets interesting and anthropic announced for lack of a bit of time on the, I think a couple of days ago, announces code execution with MCPs and allowing you to actually write code. This is the thing that I‚Äôve actually found over the last couple of weeks, which is it‚Äôs way better getting the LLM to write code to interact with APIs or SQL or something along those lines than it is to actually give it access to all of these tools and all of these intermediary steps. If and you might have seen this from your own experience of, you know, you‚Äôre probably spending a little bit less time hacking away at a piece of sequel, getting it to form exactly what you want it to these days. You know, you‚Äôre probably spending a bit more time of like here‚Äôs all the table space that I‚Äôm working on. here is a thing that I want to query, here‚Äôs what the outputs look like, and then you‚Äôre sort of having this sort of feedback loop where you‚Äôre doing that work. And my guess is, is that if you wanted to build a more sophisticated MCP, and if you were Google, you would actually lean into this concept where you would let the agent go build a little piece of Python code or JavaScript or something along those lines to query a bunch of known API endpoints to form the data back in the way that you want it to. Snowflake has its MCP server and Canva would make something called data MCP, which takes all of this data information we have and allows the LLM access to understanding how to use it. you‚Äôre really doing this piece of work around context engineering and you‚Äôre trying to think about like what is the LLM going to put into this tool for then it to get this output out. And so Michael to say, and so your sort of question here is that the MCP tool itself is deterministic, right? So it is an application in the traditional software sense. I think a lot of these MTP tools are sending data outside of out to the internet, right? They‚Äôre connected into API or a SQL database or something along those lines, but you can have deterministic tools inside of the MTP server as well that‚Äôs locally that‚Äôs connected. So you could just have like a calculator tool and it just adds numbers together and then returns exactly the right number out. You know, we‚Äôve heard the joke of counting the number r of r‚Äôs and strawberry, right? So you could have a little Python function that just counts the number r‚Äôs and strawberry and it‚Äôs just like, all right. the R counter tool, put the word strawberry in, and this is how many R‚Äôs you get back. And so this is the whole idea of, and this is, I understand I‚Äôm going to do shout outs later on, but Zed, which is an ID, has an agentic engineering series. And what they‚Äôre saying, And I just think that they have this really great framing of the problem that we‚Äôre working on today, which is how do you take the advantages of non-deterministic systems and couple it with the advantage of deterministic systems to get something more than the sum of its components? 00:24:00.95 [Val Kroll]: So I would love to take a little bit of a turn because you‚Äôve teased it a little bit, but I‚Äôm so eager to hear some of your favorite use cases and examples and that you can talk about. And I know you mentioned before, it‚Äôs still early days, but if you could talk about some of the things that you felt like weren‚Äôt possible for or too much effort for what it was worth in the past, but now it‚Äôs unlocked this or solved this for you. 00:24:27.20 [Sam Redfern]: I‚Äôll talk a little bit about some of the stuff we‚Äôve got working inside of Canberra at the moment. We use a large database vendor, which I will keep their name out of just so they don‚Äôt get in trouble. They‚Äôre going down this path of building out their own separate MCP tools and stuff like that. But what‚Äôs interesting and what I think is a big opportunity for people in this space is that building these tools for your organization is actually the critical skill that we‚Äôre going to see in the coming period of time. Every organization or company is really different, in a sense. And they made a database choice five years ago, and they made like a data transformation tool twice like four years ago. And you have all the incremental knowledge and information that‚Äôs built on top of that, which is going to kind of be unique to your organization in a sense, right? And I think there‚Äôs going to be vendors who are building tools in this space. But if you‚Äôre sort of a midsize company, like I think something to really think about is building customized versions of these tools that actually work with the flows of your organization and really having teams that are like building, thinking about these agenting and engineering practices on how to actually automate parts of the work that people don‚Äôt want to do. So some of the use cases that I‚Äôm sort of playing around at the moment, just so just the last week I‚Äôve been working on using the Altair Python visualization library and actually building a Python based sort of sandbox environment for it to run the Altair code. And so The way this works is you just put the SQL statement of the data that you want to pass into the Altair code, and then you have a Python sandbox part of the field of the tool. It just puts 300 lines of Python into it, and it builds the visualizations out of it, right? So using these really nice Python visualization libraries has always been a pain in the butt unless you tattoo the way every part of the application works on your arm so you can know exactly how it works. But again, we don‚Äôt have to worry so much about grammar anymore, because if you feed these systems examples, they can come back and help you visualize this way faster. And I think that‚Äôs where, you know, Zed has this post where they, their concept is leverage, not magic, right? And what we‚Äôre trying to do is we‚Äôre trying to take our staff members and we‚Äôre trying to make them move faster and explore more in a shorter period of time to get to a better end outcome. Just on other sort of like interesting fun use cases, I‚Äôve been, At home, I‚Äôve got my own little sort of home lab kind of thing and stuff like that. And part of playing around with that is there‚Äôs all these command switches and stuff like that. And so I built a custom NCP server for home that documents all the different applications that I use in the CLI. And it has all of the context of it. I basically put in a free text field of what I‚Äôm trying to do. It then uses a search engine to search over the data. It then takes that context, puts it into the LLM, and then the LLM goes and gives me all the command switches and stuff like that. Another one is, I think the reason why Moe recommended me for this is I gave a presentation at MeasureCamp about, at MeasureCamp, you have to come up with a talk title, otherwise people don‚Äôt show up. I said that MCP is the real apex predator for your job in 2026. And obviously, you know, I don‚Äôt think that‚Äôs true. But so I wrote that someone was like, you need to give a talk sound. I‚Äôm just like, fine. So I wrote that part at 11 o‚Äôclock, I then vibe coded up a maybe the game battleship. And so I made a version of battleship that uses MCP tools as the the fingers that the LLMs can use to play against each other. And then I built an agent harness where I could get different vendors, MCP tools to actually fight the game battleship against each other. And then you could actually watch the turn by turn thing of watching them compete against each other. And they would then have their thinking of like how they thought about the strategy of the other player. And then you could like change the prompt and be like, okay, you are going to be the random player and you‚Äôre just going to do the most random things you possibly can. And then the other one is like the most strategic player of like, this is the common moves in battleship. I‚Äôm going to do this and stuff like that. And it‚Äôs just, it‚Äôs like, like, it‚Äôs a whole new paradigm of like weird things to play around with. And like MCPs are just like this, this layer for you to like do this joining between deterministic and non-deterministic systems. But once you start playing with these systems and you‚Äôre finding different ways of interesting things to do with them, like, you know, it‚Äôs, it‚Äôs, it‚Äôs, It puts the fund back into sort of like the early stages of programming again. 00:29:38.16 [Val Kroll]: So you giving the context of like deterministic, non-deterministic is really that‚Äôs helping it crystallize a little bit in my mind what some of this is. But I do want to go back to when you first started talking about some of these examples, you were talking about how every organization is different. Everyone‚Äôs working with a different stack. Everything has to be kind of in context of what‚Äôs going on inside your organization. Can I just go back and just repeat a little bit one of my earlier questions. What is the value then of the standardization versus it being custom inside of your organization? Is it just about the ability to leverage those other tools that are using MCP or is there internal benefit too? I guess I might be just, it‚Äôs not clicking for me. I would love to just hear you share. 00:30:27.67 [Sam Redfern]: No, no, no, no. Okay, so MCP is the thing that allows you at the moment until someone comes up with a better standard. And we should probably talk about some of the downsides of MCP as well. Just all the criticism, sorry, the criticisms as they stand of MCP, but MCP at the moment allows you to bridge that gap between deterministic and non-deterministic systems. You‚Äôve got the vendors, and the vendors are going out, and they are building their own customers. MCP is right, because what they want, because what they‚Äôre getting, is they‚Äôre getting pressure from leadership teams of like, okay, we need to get some AI in this organization right now. I have called desktop on my computer and I want to be able to query this information directly from Claude and have that come back to me in a sense, right? And MCPs is like a path you can go down there, but the problem is that you‚Äôre wrestling with the non-deterministic nature of these systems when you do that type of connection in a sense, right? And what you actually, this is where building up that practice inside of your organization is really important because when you get into the detail of how these systems work, you realize that giving senior leadership teams access to the raw thing that directly queries the database is problematic for lots of different reasons around you‚Äôre not able to check it, you can‚Äôt give it a system prompt, all that sort of stuff. So MCPs, the standard, if we can call it that, is just the little connecting block, in a sense. the practice of what I‚Äôm talking about about the non-standardization inside of organizations is that you can totally go use the vendor solutions, right? But it‚Äôs always going to feel like it‚Äôs through a fuzzy piece of glass, because unless you‚Äôre doing exactly what the vendor has, right? Like, if you‚Äôre If you‚Äôre 100% Google shop, and you‚Äôve never used anything other than Google, then maybe the Google stack is going to be great because that‚Äôs what they‚Äôre going to be using internally. And they‚Äôre going to be copying from that of how they‚Äôre building it. But I don‚Äôt know about you, but most organizations I ever interact with is it sort of like a collage of different solutions. And the money is in getting them to connect together, right? Yeah, templatized versus custom kind of. 00:32:38.05 [Tim Wilson]: Altair, take Canva, take Snowflake, take Google Analytics and MCP for those. Is there the option that Canva or Altair or Snowflake or Google, and those are intentionally very wildly different types of platforms, they can sort of create an MCP, one of these connectors that say this is kind of generic, but there‚Äôs also an option that I as a user of Canva with access, I could also build my own MCP, my own connector, or is it okay? 00:33:20.36 [Sam Redfern]: So you‚Äôre raising on a really good topic, which is called context pollution, right? So there was a criticism of GitHub‚Äôs MCP server where it has like 24 some number of tools, right? And the majority of people use like three, right? And so I push. If you get into the details of how NTPs work is that they repaste the entire list of tools and the tool descriptions and all this sort of stuff with every single message that goes through, right? Because they are trying to solve the context engineering problem of, like, The LLN needs to really know what all the tools are at every single turn. And you can go and add 70 tools to an agent harness. And you should do that to watch it not work, because it‚Äôs very entertaining. And you can suddenly watch all of your contacts disappear and have all sorts of problems, right? 00:34:24.92 [Michael Helbling]: You got extra tokens to spare. 00:34:27.04 [Sam Redfern]: Go ahead. You know, somebody would go win the token awards or whatever it is. I did the other day, right? You know, some people need the token trophy. That‚Äôs great. But the like your job as an engineer or a technology person inside of an organization is to do it in the most sensible, reliable way possible, right? And you‚Äôre trying to harness these nondeterministic systems to get the best outcome. And part of the reason why you go and build that custom harness that is fit for purpose for your organization and has different flatters depending on exactly the task that you‚Äôre trying to get into is you‚Äôre trying to, it‚Äôs in the name, you‚Äôre trying to take that harness and you‚Äôre trying to constrain this non-determinic system to only work in this particular domain. I think a lot of people, when they talk about these MCPs, they‚Äôre talking about it from their experience of having clawed desktop on their computer and connecting it up to JIRA or some other thing like that. That‚Äôs a totally valid use case. I don‚Äôt have to open JIRA anymore. I think the favorite part of my job now is when someone assigns me a ticket, it‚Äôs the only thing I have connected to the clawed desktop on my computer. and I interact with all of my JIRA tickets through Claude. I‚Äôve solved the Atlassian interface problem by just never having to open it. That‚Äôs for me as a human here, but when we‚Äôre talking about making these systems do useful things in your organization that you can‚Äôt convince engineers to pick up, or it‚Äôs really boring work, or it‚Äôs testing work, or something like that. That‚Äôs where these systems kind of shine, right? We‚Äôre not trying to put someone out of a job or anything like that. What we‚Äôre trying to do is we‚Äôre trying to get that as tasks that are not particularly enjoyable, like documentation, testing tracking, all this sort of stuff, like building custom harnesses around that to help engineers make the best possible decision when they‚Äôre building something, That‚Äôs the real advantage of these tools. 00:36:30.89 [Michael Helbling]: Yeah. And I think, Tim, also, the Google Analytics example is tricky because it‚Äôs very limited. And it basically is an API layer to Google Analytics. It‚Äôs not really giving you more MCP-ish type of interaction. So I think it adds to the confusion a little bit. Because it‚Äôs like, you can do all the same things you can do with the Google Analytics MCP server with their API. It‚Äôs just call this function. But instead of you writing the query to the API, the LLM does it for you. But it‚Äôs not more stuff. 00:37:04.57 [Tim Wilson]: How does the GitHub MCP server? So I have two questions. One, it sounds like a lot of platforms maybe of those, if they‚Äôre 24, and I‚Äôm assuming it‚Äôs not exactly 24. However many tools there are within the GitHub MCP server that a lot of them are just a layer to the API, and maybe there are some that aren‚Äôt. But if you were saying, we do want to use a GitHub MCP server, this has got too much, would it be? I‚Äôm going to get their MCP server and I‚Äôm going to whittle it down and then probably check it back into GitHub just to make things confusing. But do MCP servers get, there could be the official Uber generic one developed by the platform and then somebody says, yeah, I need to make one that‚Äôs just a much narrower scope and maybe add some flavor on it or does it not work that way? 00:37:58.20 [Sam Redfern]: Yeah, great question Tim. And this is why the agent, like talking about agentic engineering and harnesses is really important because in a harness, you say, these are the MCP servers only have these tools, right? And so you can take the GitHub MCP server and you can say, here‚Äôs your three tools, deal with it. Sorry, I just, you know, let‚Äôs not get into the accuracy of AI overviews, but according to in June 2025, apparently exposes 51 plus tools. Okay. No, okay. You know, like, but this is like, if you‚Äôre in a world where you Do you narrow that down to a very limited set of the tools? And you can see this in the cloud desktop thing. If you load an NCP server on the cloud desktop, you have little switches where you can turn on and off tools in a sense. The intention of these systems is that you narrow the scope down to exactly the problem that you‚Äôre working on and just that. But what‚Äôs interesting, Tim, is why bother having the GitHub? If you‚Äôre doing the coding yourself and you‚Äôre using it inside a cursor or something along those lines, it‚Äôs like, why bother adding the GitHub MCP server at all? Why not just get the LLM to execute something in the command line with all the command switches of the GitHub CLI? 00:39:13.50 [Tim Wilson]: Also, is this getting us to the downsides? 00:39:18.24 [Val Kroll]: I was just going to say, he risks you a bit chomping at the bit, Sam. Let‚Äôs hear it. 00:39:24.80 [Tim Wilson]: I mean, there‚Äôs part of it feels like this is, it‚Äôs new enough and wild, wild west to know there‚Äôs like, there could be a governance issue that it would be very easy to embed MCPs into an organization and they‚Äôre not well thought out. They‚Äôre not well built. They, they, I mean, that just, it seems like there‚Äôs just a governance thing like anything that gets rolled out. that one Sam creates something and all of a sudden the entire organization is dependent on it. And maybe Sam‚Äôs not very good at, you know, or just half-assed it on a weekend or something like, I don‚Äôt know. So that‚Äôs my, I‚Äôm throwing that out that it seems like there‚Äôs a governance risk when you‚Äôre being, you‚Äôre able to do this stuff so quickly and roll it out. Is that one of the downsides? 00:40:13.13 [Sam Redfern]: I mean, building, and this is something Canva does really well, right? Like building AI tools for people to use in the application is just really different to building stuff internally. Like we are still at the early days of this stuff and Canva‚Äôs ecosystem team building out really strong solutions to the space of whether canvas empty server or client or something like those lines. So there was like the professional teams who are like building this sort of stuff for external consumption. So in open AI, you can, you know, you can. use the LLM to interact with it. You can use GPT-5 or whatever they call it these days to interact with Canva and modify your designs and stuff like that. That‚Äôs all using this style of tool technology in a sense, right? And there‚Äôs a lot of governance that‚Äôs been there, right? There‚Äôs a lot of thinking about permissioning, thinking about what information we‚Äôre giving to the LLM, what actions we‚Äôre giving to it, what are the actual actions that change something. One of the downsides of giving the LLM access to your terminal command line is that it could just delete all the files in the directory or something on the inside. I think my favorite one is where an engineer is trying to get the LLM to write the code so it passes all the tests and so it solves the problem by deleting the tests and it‚Äôs just like problem solved, I‚Äôm done. a technically correct, the best type of correct, but actually, no, that‚Äôs not what I wanted. And so this is why that agent harness framework is really useful because that‚Äôs where we‚Äôre like, here is this domain of a problem. Here is this very finance set of tools. Here‚Äôs how I want you to sort of exactly work on this particular part of the problem. And I don‚Äôt want you to have this like long chain where you‚Äôre sort of like jumping between things. I just want to create the agents of instance, have it solve one or two problems, and for the agent instance to end, so then we can move on to the next problem. That is why we‚Äôre trying to solve and harness this non-deterministic nature. Some of the criticisms of this MTP standard is like, one, it‚Äôs not a standard, right? Like a standard, if we think about it as from the Internet Enduring Task Force, the W3C is like a collection of for-profit companies coming together and sending some of their best engineers to basically have like a very disgruntled call with a bunch of other engineers from other sort of for-profit companies, right? Like I don‚Äôt, as far as I understand, it‚Äôs kind of just anthropic building this internally, polishing stuff on their blog, they picked up FastMTP, which is like an open source thing. And they just said, all right, this is the standard. We‚Äôre going to use this as our standard library and sort of extend it out and stuff like that. you know, coming back to that governance is like, this is not a structure that feels like it‚Äôs ready for like, you, it‚Äôs buyer beware on the internal corporate governance stuff. And the way you design these systems is really important. And so some of the data tools I build internally, it has no ability to write information to the database, right? Because that is completely like, we‚Äôre just not ready for that kind of world, right? And maybe in really select kind of instances where there‚Äôs a really strong, hard-to-surrounder and you have like a checking endpoint and all sorts of other stuff like that and things like that as well, things like Langchain are really useful. But we need to, that‚Äôs why I think most of the value of this space is still in the internal application use cases, in a sense. That‚Äôs where you can do a more experimentation and worry less about the strangeness of the internet and the internet and AI and all the problems attached to that. But when you‚Äôre developing these tools internally, you have a team of you know, a handful of engineers and like, you can make their lives like tangibly better because they don‚Äôt have to put context into their mind at a particular part of the problem. And they can just have that answer come back. And even if it‚Äôs right 90% of the time, it‚Äôs probably better than when you got your junior data scientists sort of do it in the first place anyway. So, you know, that‚Äôs that‚Äôs the like, that‚Äôs the challenge. As far as One of the other criticisms of Anthropox MCP is that it doesn‚Äôt have authentication baked in. Moest of the MCP, so it‚Äôs counterintuitive having this term of MCP server and client in a sense, right? And what happens is you‚Äôre literally running a little application. You‚Äôre like, you know, Python run this like Damon or something along those lines. And then that is just talking to the it‚Äôs like running a local web server in a sense, right? And that is the security model that has been sold for in the early days. And that‚Äôs why the Zed has their ACP agent. What is it Zed? ACP. I think in a couple of years, we‚Äôre going to be talking about the agent client protocol as maybe a better way of building this sort of stuff. Everyone sort of agrees that the ACP protocol is probably a better representation of where we‚Äôre going in this space. And it is an open standard. I don‚Äôt think they‚Äôre sort of like the W3C or the internet engineering password style of standard. Back to that XML example, I don‚Äôt know about you. I don‚Äôt read a lot of XML these days. We‚Äôre going to be moving to something else, but I‚Äôm very bullish on the concept of tool use in these applications and giving large language models these fingers to do things. 00:45:53.98 [Tim Wilson]: Is there any movement? It sounds like Zed has stepped in and done a little bit of this. Is there any movement to say, like the W3C had its different groups and came together that we should get? 00:46:08.82 [Val Kroll]: Didn‚Äôt Google and Microsoft and OpenAI, didn‚Äôt they all adopt it? Am I totally misunderstanding what that means? 00:46:16.54 [Tim Wilson]: that it‚Äôs another thing to say, here‚Äôs our, here‚Äôs our, we got to solve authentication. We got to have a recommendation and a standard for how authentication is going to be handled means they can‚Äôt just say, like it‚Äôs not there. That‚Äôs something that new that needs to be incorporated in a way that they say, yeah, we think this is generally going to work for most. We can all work with this, right? Cause that‚Äôs, I mean, it‚Äôs not, it‚Äôs not a static. I mean, I guess let me ask that question. It‚Äôs MCP. How static is it? Like when HTML came out, it wasn‚Äôt like, okay, we‚Äôre done. Well, there was a bunch of other stuff that was needed and browsers added functionality. And so it was kind of, it naturally had to evolve and is MCP the same thing that it needs to, yeah. 00:47:04.94 [Sam Redfern]: Yeah. So there‚Äôs a really interesting part of this, which is that there is a, I think there is a recommended output format to MCP servers that as part of their standard. But I think what‚Äôs interesting about this is that like because of the non-deterministic nature of these systems. And because you can, I don‚Äôt know if you‚Äôve ever played around with this, but it‚Äôs always good fun is to, you can have the start of your question in XML, then you can do the middle bit in YAML and then the end bit in JSON. And the large language model doesn‚Äôt skip a beat. And it‚Äôs just like, oh yeah, sure, I understand this, right? Like it doesn‚Äôt matter as much is kind of the context of this problem, right? Because We required these standards in the early days of the internet because they were purely deterministic systems with incredibly strong grammars. I just don‚Äôt think it matters as much anymore. That‚Äôs why I don‚Äôt think there‚Äôs been the same pressure to standardize because you don‚Äôt need to standardize in the same way. The only thing that matters is do you pass the tool call threshold in your large language model. And I think it may be, you know, like rather, rather than a very deliberate standard like TCP IP or IPv6, I think it‚Äôs going to be more along the lines of the QWERTY keyboard, which is like, we just kind of picked it because it was there first, not because it‚Äôs better. and MCP will probably change to something else in the future, right? But the primitives that make it interact with the large language model, I think, are now baked in enough that it would be surprised to see if we move away from that. And all we‚Äôre going to do is we‚Äôre going to find new ways of taking those primitives and doing this code execution thing. So I gave my example of this charting sort of like MCP extension that I built. Like all we‚Äôre going to do is we‚Äôre going to take the same primitives, but then we‚Äôre going to like do wildly different things with them that people didn‚Äôt think was possible before. 00:49:06.08 [Tim Wilson]: And at some point, that will have shifted to a point that it‚Äôs got a new label. And it‚Äôs like, oh, remember, it was just MCPs. Now we have something else, which is grounded in all that we learned from MCP. Okay. 00:49:17.86 [Sam Redfern]: Yeah. Now it‚Äôs going to be ACP. It‚Äôs going to be, who knows, whatever. I look forward to watching this name change over time. I‚Äôm sure there will be an XKCD comic at some point. There‚Äôs the end pop on standard XKCD comic, and we are not immune from that paradigm, which has been true in software for long enough. 00:49:40.98 [Tim Wilson]: I‚Äôve brought up that particular strip, I think, on two of the last four episodes. One was on semantic layers and one was on 00:49:48.77 [Michael Helbling]: Yeah, we‚Äôll check back in in six months because certainly things will have shifted quite a bit. All right, we do have to start to wrap up. And as we do that, let me jump into a quick break with our friend Michael Kaminsky from Recast, the Media Mix Moedeling and GeoLift platform helping teams forecast accurately and make better decisions. Michael‚Äôs been sharing bite-sized marketing lessons over the past few months to help you measure smarter. Over to you, Michael. 00:50:17.39 [Michael Kaminsky (Recast)]: When we perform statistical analysis of data, what we really care about is that we are discovering actual truths about the world, not random artifacts of the particular data set we‚Äôre looking at or the analytical methods we‚Äôre choosing. We want generalizable analyses, the kind where independent researchers answering the same question would converge on similar results. This is all another way of talking about a hugely important idea in model building or statistical analysis. Robustness. Without robustness, even a small tweak at assumptions or small changes of the data will spit out dramatically different results. Results that aren‚Äôt showing true causation or reflecting reality, but just picking up random noise. So how do we put this into practice when doing statistical analyses? We can randomly resample from our dataset or even randomly drop small amounts of data and see if the results are being driven by one particular outlier observation. Similarly, if we‚Äôre running a regression analysis with control variables, we can check how sensitive the results are to different control combinations. If the findings change dramatically depending on which controls we include, we should be skeptical of the overall results. The more robust our results are as things change, the more we feel confident that other analysts or researchers will end up drawing the same conclusions and the better chance we have of finding some underlying truth. 00:51:31.49 [Michael Helbling]: Thanks, Michael. And for those who haven‚Äôt heard, our friends at ReCast just launched their new incrementality testing platform, GeoLift, by ReCast. It‚Äôs a simple, powerful way for marketing and data teams to measure the true impact of their advertising spend. And even better, you can use it completely free for six months. Just visit www.getrecast.com slash geolift to start your trial today. All right. Well, one of the things we‚Äôd like to do is go around the horn and share a last call, something that might be of interest to our users. Sam, you‚Äôre our guest. Do you have a last call you‚Äôd like to share? 00:52:06.23 [Sam Redfern]: Well, I mean, obviously, thinking about this sort of this agentic engineering thing. OK, so I‚Äôm going to get in trouble by doing two. One of them is go to z.dev and go read about and always a problem. Go to z.dev. and go check out under resources and they have their agentic engineering series about the future of software development. I think it‚Äôs a great grounding of where we‚Äôre going in this industry and I think they lay out a really great vision of what this could be. The last one is that I don‚Äôt actually like Zed‚Äôs agent. I think one of the most important things here is to go get your hands dirty with these systems. They are just so much fun. And if you‚Äôre a bit of an old techie, it doesn‚Äôt matter as much about the grammar anymore and really just spend some money on tokens and explore it. And in that vein, I actually think the best agent you can get for nothing is OpenCode. And so I think it‚Äôs opencode.ai. Yeah. OpenCode, I think, right now is one of the best agent harnesses that you can possibly go and build things. They‚Äôve got really interesting things like the ability to define subagents that you can give different prompts and contacts to. And so if you want a really great base agent to play around with, to go and then build really interesting harnesses, can‚Äôt recommend the OpenCode thing enough. Nice. Thank you. 00:53:29.31 [Michael Helbling]: All right, Val, what about you? What‚Äôs your last call? 00:53:32.87 [Val Kroll]: So mine‚Äôs a total left turn. But I have just been really enjoying lately the Good Hang podcast with Amy Poehler. It‚Äôs been around not quite a year yet, I don‚Äôt think. But if you are just in the need of a good laugh, I am telling you, you will walk away from those with a stomach ache. The Rachel Dratch episode, I legit did a spit take. It is So funny. So anyways, she just has like lots of different celebrities on to talk about all different topics and it‚Äôs quite enjoyable. So does she have an MCP server? No, I‚Äôm saying keeping it light. We‚Äôre keeping it light. Yeah, that‚Äôs great. You need that. 00:54:18.51 [Michael Helbling]: All right, Tim, what about you? What‚Äôs your last call? 00:54:23.62 [Tim Wilson]: So I‚Äôm going to do kind of a mix of like the human side of things, just because we‚Äôre starting off the year. So now hopefully people are looking forward to what human people they‚Äôre going to go see in various places like in-person conferences. So I will plug that I am, I‚Äôm getting to return to Super Week this year, which I have missed for the last couple of years. And that‚Äôs a missed in-person and in Spirit, so superweek.hu. It‚Äôs February 2nd through 6th in Budapest. And then I‚Äôm going to double that up with just a couple of good follows. I feel like we need more humor on LinkedIn. And there are two guys who are both very reliably putting in just short, random, funny things, and also some good content. So I‚Äôm going to plug Bov Patel, Bovick Patel, and Manas, Dada, DA, TTA. He does all sorts of like something like finger guns, but every time he does something, he has a different sort of something guns at the end of it. So they‚Äôre just good follows to put a little less bloviating in your LinkedIn feed would be those two guys. And what did they teach you about B2B sales there, Tim? And they definitely make cracks about that along the way. 00:55:47.29 [Michael Helbling]: What about you, Michael? What‚Äôs your last call? Well, I‚Äôll be curious, Tim, to hear whether or not MCP servers come up at Super Week, which I‚Äôm sure they will. My last call is AI related. I just was hanging out with my good friend Christopher Berry a week or so ago, and he turned me on to a paper that some folks wrote about how to jailbreak large language models, because sometimes you just need it to give you the recipe for gunpowder or something. And apparently, a really great way to do that is just talk to it in poetry. So, if you add a poem, it will just tell it back to you as a poem and give you the information you want. No questions asked. So, not saying you should do that, but that‚Äôs something you should be aware of. We‚Äôll link the paper in the show notes. So, can I sneak in with one last thing? 00:56:37.69 [Sam Redfern]: Yeah, of course. It‚Äôs related to jailbreaking large-language models. There‚Äôs a community inside of Canberra, a channel where people share their tips and tricks for interacting with these large-language models. There was a thread on how do you get better outputs. Yelling at large-language models, surprisingly works, bribing large-language models surprisingly works. My favorite one is to tell the agent that a much smarter and more sophisticated agent is about to come and check its work. and it should hurry up and make sure that there‚Äôs no mistakes before it gets checked. 00:57:12.16 [Tim Wilson]: I so wanted to know that there was some Moe related threat in there that you‚Äôd be like, look, if you get this wrong, Moe kiss is going to be disappointed. And they‚Äôre like, that, oh my God, that is the ultimate hack. 00:57:25.77 [Michael Helbling]: Add some weights to that name inside of all the large language models inside of Canva. That‚Äôs probably a good idea. All right, Sam, this has been outstanding. Thank you so much for taking the time to come on the show. Talk about this topic. This has been great. Thank you very much for having me. It‚Äôs been a great time. Yeah, no, and I‚Äôm sure our listeners of which there are many will have a lot of questions or things like that. We‚Äôd love to hear from you. You can reach out to us on our LinkedIn page or through the Measures Slack chat group or via email at contact at analyticshour.io. as you‚Äôre listening or listening to this episode, also leave reviews and ratings. We like to get those as well on whatever platform you listen on. If you want, we also still have some stickers, then Tim will send them to you if you request a sticker over on analyticshour.io. Reach out to us that way. Awesome. Really great. I think this is a more technical topic, but I think it‚Äôs still very relevant to everybody in the data space because of the sort of the intersection of AI and data. It‚Äôs sort of a thing we‚Äôre all talking about. So, Sam, thank you for helping demythologize some of this, if you will, and bring some practical knowledge. I think it‚Äôs a huge service. And like you said, it‚Äôs changing every day. So, you know, apologies in advance for how outdated this podcast will be in about three weeks, but that‚Äôs just the way it works. You got to get started somewhere. The AI nodes take Christmas off. That‚Äôs right. Stop updating your LLMs for crying out loud. Yeah. The big news today was that Sam Altman issued a code red for open AI because Gemini is doing so well and they‚Äôve got to get back to getting hard work done. So, okay. Yeah. 00:59:14.15 [Tim Wilson]: Stop this 996 nonsense, right? 00:59:16.81 [Michael Helbling]: Yeah, right. We need some work-life balance. Take a break. Before the AIs take our jobs, we need some work-life balance. No, I‚Äôm just kidding. All right. I know that as you go out there and you‚Äôre working with data and you‚Äôre trying to use AI, it‚Äôs always complex and challenging and you‚Äôre learning a lot. It feels like the early days of analytics all over again. But I know I speak for both of my co-hosts, Tim and Val, when I say, no matter which MCP you‚Äôre using, don‚Äôt forget to keep analyzing. 00:59:43.59 [Announcer]: Thanks for listening. Let‚Äôs keep the conversation going with your comments, suggestions, and questions on Twitter at @analyticshour on the web at analyticshour.io, our LinkedIn group, and the Measure Chat Slack group. Music for the podcast by Josh Crowhurst. Those smart guys wanted to fit in, so they made up a term called analytics. Analytics don‚Äôt work. 01:00:08.09 [Charles Barkley]: Do the analytics say go for it, no matter who‚Äôs going for it? So if you and I were on the field, the analytics say go for it. It‚Äôs the stupidest, laziest, lamest thing I‚Äôve ever heard for reasoning in competition. 01:00:28.43 [Tim Wilson]: Rock flag and non-determinism. The post #288: Our LLM Suggested We Chat about MCP. Kinda‚Äô Meta, No? appeared first on The Analytics Power Hour: Data and Analytics Podcast .",
      "published_ts": 1767674142,
      "source_name": "Analytics Power Hour",
      "content_type": "rex"
    },
    {
      "url": "https://about.gitlab.com/blog/vulnerability-triage-made-simple-with-gitlab-security-analyst-agent/",
      "title": "AI-powered vulnerability triaging with GitLab Duo Security Agent",
      "summary": "Security vulnerabilities are discovered constantly in modern applications. Development teams often face hundreds or thousands\nof findings from security scanners, making it challenging to identify which vulnerabilities pose the greatest risk and should\nbe prioritized. This is where effective vulnerability triaging becomes essential. In this article, we'll explore how GitLab's integrated security scanning capabilities combined with the GitLab Duo Security Analyst Agent can transform vulnerability management from a time-consuming manual process into an intelligent, efficient workflow. üí° Join GitLab Transcend on February 10 to learn how agentic AI transforms software delivery. Hear from customers and discover how to jumpstart your own modernization journey. Register now. What is vulnerability triaging? Vulnerability triaging is the process of analyzing, prioritizing, and deciding how to address security findings discovered in\nyour applications. Not all vulnerabilities are created equal ‚Äî some represent critical risks requiring immediate attention, while\nothers may be false positives or pose minimal threat in your specific context. Traditional triaging involves: Reviewing scan results from multiple security tools Assessing severity based on CVSS scores and exploitability Understanding context such as whether vulnerable code is actually reachable Prioritizing remediation based on business impact and risk Tracking resolution through to deployment This process becomes overwhelming when dealing with large codebases and frequent scans. GitLab addresses these challenges through\nintegrated security scanning and AI-powered analysis. How to add integrated security scanners in GitLab GitLab provides built-in security scanners that integrate seamlessly into your CI/CD pipelines. These scanners run automatically\nduring pipeline execution and populate GitLab's Vulnerability Report with findings from the default branch. Available security scanners GitLab offers the following security scanning capabilities: Static Application Security Testing (SAST) : Analyzes source code for vulnerabilities Dependency Scanning : Identifies vulnerabilities in project dependencies Container Scanning : Scans Docker images for known vulnerabilities Dynamic Application Security Testing (DAST) : Tests running applications for vulnerabilities Secret Detection : Finds accidentally committed secrets and credentials Infrastructure-as-Code (IaC) Scanning : Analyzes infrastructure as code for misconfigurations API Security Testing : Test web APIs to help discover bugs and potential security issues Web API Fuzzing : Passes unexpected values to API operation parameters to cause unexpected behavior and errors in the backend Example: Adding SAST and Dependency Scanning To enable security scanning, add the scanners to your .gitlab-ci.yml file. In this example, we are including SAST and Dependency Scanning templates which automatically run those scanners on the test stage.\nEach scanner can be overwritten using variables (which differ for each scanner). For example, the SAST_EXCLUDED_PATHS variable\ntells SAST to skip the directories/files provided. Security jobs can be further overwritten using the GitLab Job Syntax . include:\n  - template: Security/SAST.gitlab-ci.yml\n  - template: Security/Dependency-Scanning.gitlab-ci.yml\n\nstages:\n  - test\n\nvariables:\n  SAST_EXCLUDED_PATHS: \"spec/, test/, tests/, tmp/\" Example: Adding Container Scanning GitLab provides a built-in container registry where you can store container images for each GitLab project. To scan those containers for vulnerabilities,\nyou can enable container scanning. This example shows how a container is built and pushed in the build-container job running in the build stage\nand how it is then scanned in the same pipeline in the test stage: include:\n  - template: Security/Container-Scanning.gitlab-ci.yml\n\nstages:\n  - build\n  - test\n\nbuild-container:\n  stage: build\n  variables:\n    IMAGE: $CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG:$CI_COMMIT_SHA\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build -t $IMAGE .\n    - docker push $IMAGE\n\ncontainer_scanning:\n  variables:\n    CS_IMAGE: $CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG:$CI_COMMIT_SHA Once configured, these scanners execute automatically in your pipeline and report findings to\nthe Vulnerability Report . Note: Although not covered in this blog, in merge requests, scanners show the diff of vulnerabilities from a feature\nbranch to the target branch. Additionally, granular security policies can be created to prevent vulnerable code\nfrom being merged (without approval) if vulnerabilities are detected, as well as force scanners to run, regardless of how the .gitlab-ci.yml is defined. Triaging using the Vulnerability Report and Pages After scanners run, GitLab aggregates all findings in centralized views that make triaging more manageable. Accessing the Vulnerability Report Navigate to Security & Compliance > Vulnerability Report in your project or group. This page displays all\ndiscovered vulnerabilities with key information: Severity levels (Critical, High, Medium, Low, Info) Status (Detected, Confirmed, Dismissed, Resolved) Scanner type that detected the vulnerability Affected files and lines of code Detection date and pipeline information Filtering and organizing vulnerabilities The Vulnerability Report provides powerful filtering options: Filter by severity, status, scanner, identifier, and reachability Group by severity, status, scanner, OWASP Top 10 Search for specific CVEs or vulnerability names Sort by detection date or severity View trends over time with the security dashboard Manual workflow triage Traditional triaging in GitLab involves: Reviewing each vulnerability by clicking into the detail page Assessing the description and understand the potential impact Examining the affected code through integrated links Checking for existing fixes or patches in dependencies Setting status (Confirm, Dismiss with reason, or create an issue) Assigning ownership for remediation This is an example of vulnerability data provided to allow for triage including the code flow: When on the vulnerability data page, you can select Edit vulnerability to change its\nstatus as well as provide a reason. Then you can create an issue and assign ownership for remediation. While this workflow is comprehensive, it requires security expertise and can be time-consuming when dealing with hundreds\nof findings. This is where GitLab Duo Security Analyst Agent, part of GitLab Duo Agent Platform , becomes invaluable. About Security Analyst Agent and how to set it up GitLab Duo Security Analyst Agent is an AI-powered tool that automates vulnerability analysis and triaging.\nThe agent understands your application context, evaluates risk intelligently, and provides actionable recommendations. What Security Analyst Agent does The agent analyzes vulnerabilities by: Evaluating exploitability in your specific codebase context Assessing reachability to determine if vulnerable code paths are actually used Prioritizing based on risk rather than just CVSS scores Explaining vulnerabilities in clear, actionable language Recommending remediation steps specific to your application Reducing false positives through contextual analysis Prerequisites To use Security Analyst Agent, you need: GitLab Ultimate subscription with GitLab Duo Agent Platform enabled Security scanners configured in your project At least one vulnerability in your Vulnerability Report Enabling Security Analyst Agent Security Analyst Agent is a foundational agent .\nUnlike the general-purpose GitLab Duo agent, foundational agents understand the unique workflows, frameworks, and best practices\nof their specialized domains. Foundational agents can be accessed directly from your project without any additional configuration. You can find Security Analyst Agent in the AI Catalog : To dive in and see the details of the agent, such as its system prompt and tools: Navigate to gitlab.com/explore/ . Select AI Catalog from the side tab. Select Security Analyst Agent from the list. The agent is integrated directly into your existing workflow without requiring additional configuration beyond the defined\nprerequistes. Using Security Analyst Agent to find most critical vulnerabilities Now let's explore how to leverage Security Analyst Agent to quickly identify and prioritize the vulnerabilities\nthat matter most. Starting an analysis To start an analysis, navigate to your GitLab project (ensure it meets the prerequistes). Then\nyou can open GitLab Duo Chat and select the Security Agent . From the chat, select the model to use with the agent and make sure to enable Agentic mode. A chat will open where you can engage with Security Analyst Agent by using the agent's conversational\ninterface. This agent can perform: Vulnerability triage : Analyze and prioritize security findings across different scan types. Risk assessment : Evaluate the severity, exploitability, and business impact of vulnerabilities. False positive identification : Distinguish genuine threats from benign findings. Compliance management : Understand regulatory requirements and remediation timelines. Security reporting : Generate summaries of security posture and remediation progress. Remediation planning : Create actionable plans to address security vulnerabilities. Security workflow automation : Streamline repetitive security assessment tasks. Additionally, these are the tools which Security Analyst Agent has at its disposal: For example, I can ask \" What are the most critical vulnerabilities and which vulnerabilities should I address first? \"\nto make it easy to determine what is important. The agent will respond as follows: Example queries for effective triaging Here are powerful queries to use with the Security Analyst Agent: Identify critical issues: \"Show me vulnerabilities that are actively exploitable in our production code\" Focus on reachable vulnerabilities: \"Which high-severity vulnerabilities are in code paths that are actually executed?\" Understand dependencies: \"What are the most critical dependency vulnerabilities and are patches available?\" Get remediation guidance: \"Explain how to fix the SQL injection vulnerability in user authentication\" You can also directly assign developers to vulnerabilities. Understanding agent recommendations When Security Analyst Agent analyzes vulnerabilities, it provides: Risk assessment : The agent explains why a vulnerability is critical beyond just the CVSS score, considering your\napplication's specific architecture and usage patterns. Exploitability analysis : It determines whether vulnerable code is actually reachable and exploitable in your\nenvironment, helping filter out theoretical risks. Remediation steps : The agent provides specific, actionable guidance on how to fix vulnerabilities, including code\nexamples when appropriate. Priority ranking : Instead of overwhelming you with hundreds of findings, the agent helps identify the top issues\nthat should be addressed first. Real-world workflow example Here's how a typical triaging session might look: Start with the big picture : \"Analyze the security posture of this project and highlight the top 5 most critical vulnerabilities.\" Dive into specifics : For each critical vulnerability identified, ask \"Is this vulnerability actually exploitable in our application?\" Plan remediation : \"What's the recommended fix for this SQL injection issue, and are there any side effects to consider?\" Track progress : After addressing critical issues, ask \"What vulnerabilities should I prioritize next?\" Benefits of agent-assisted triaging Using Security Analyst Agent transforms vulnerability management: Time savings : Reduce hours of manual analysis to minutes of guided review Better prioritization : Focus on vulnerabilities that actually pose risk to your specific application Knowledge transfer : Learn security best practices through agent explanations Consistent standards : Apply consistent triaging logic across all projects Reduced alert fatigue : Filter noise and false positives effectively Get started today Vulnerability triaging doesn't have to be an overwhelming manual process. By combining GitLab's integrated security scanners\nwith GitLab Duo Security Analyst Agent, development teams can quickly identify and prioritize the vulnerabilities that\ntruly matter. The agent's ability to understand context, assess real risk, and provide actionable guidance transforms security scanning\nfrom a compliance checkbox into a practical, efficient part of your development workflow. Instead of drowning in hundreds\nof vulnerability reports, you can focus your energy on addressing the issues that actually threaten your application's security. Start by enabling security scanners in your GitLab pipelines, then leverage Security Analyst Agent to make intelligent,\ninformed decisions about vulnerability remediation. Your future self ‚Äî and your security team ‚Äî will thank you. Ready to get started? Check out the GitLab Duo Agent Platform documentation and security scanning documentation to begin transforming your\nvulnerability management workflow today.",
      "published_ts": 1767657600,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/microsofts-strategic-ai-datacenter-planning-enables-seamless-large-scale-nvidia-rubin-deployments/",
      "title": "Microsoft‚Äôs strategic AI datacenter planning enables seamless, large-scale NVIDIA Rubin deployments",
      "summary": "CES 2026 showcases the arrival of the NVIDIA Rubin Platform, along with Azure‚Äôs proven readiness for deployment. The post Microsoft‚Äôs strategic AI datacenter planning enables seamless, large-scale NVIDIA Rubin deployments appeared first on Microsoft Azure Blog .",
      "published_ts": 1767654000,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
      "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
      "summary": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI\nNVIDIA today released\nCosmos Reason 2\n, the latest advancement in open, reasoning vision language models for physical AI. Cosmos Reason 2 surpasses its previous version in accuracy and tops the\nPhysical AI Bench\nand\nPhysical Reasoning\nl",
      "published_ts": 1767653811,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
      "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
      "summary": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture\nDiscover more in\nour official blogpost\n, featuring an interactive experience\nThe journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we're ",
      "published_ts": 1767604611,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/building-trust-in-agentic-tools-what-we-learned-from-our-users/",
      "title": "Building trust in agentic tools: What we learned from our users",
      "summary": "As AI agents become increasingly sophisticated partners in software development, a critical question emerges: How do we build lasting trust between humans and these autonomous systems? Recent research from GitLab's UX Research team reveals that trust in AI agents isn't built through dramatic breakthroughs, but rather through countless small interactions called inflection points that accumulate over time to create confidence and reliability. Our comprehensive study of 13 agentic tool users from companies of different sizes identified that adoption happens through \"micro-inflection points,\" subtle design choices and interaction patterns that gradually build the trust needed for developers to rely on AI agents in their daily workflows. These findings offer crucial insights for organizations implementing AI agents in their DevSecOps processes. Traditional software tools earn trust through predictable behavior and consistent performance. AI agents, however, operate with a degree of autonomy that introduces uncertainty. Our research shows that users don't commit to AI tools through single \"aha\" moments. Instead, they develop trust through accumulated positive micro-interactions that demonstrate the agent understands their context, respects their guardrails, and enhances rather than disrupts their workflows. This incremental trust-building is especially critical in DevSecOps environments where mistakes can impact production systems, customer data, and business operations. Each small interaction either reinforces or erodes the foundation of trust necessary for productive human-AI collaboration. Four pillars of trust in AI agents Our research identified four key categories of micro-inflection points that build user trust: Safeguarding actions Trust begins with safety. Users need confidence that AI agents won't cause irreversible damage to their systems. Essential safeguards include: Confirmation dialogs for critical changes: Before executing operations that could affect production systems or delete data, agents should pause and seek explicit approval Rollback capabilities: Users must know they can undo agent actions if something goes wrong Secure boundaries: For organizations with compliance requirements, agents must respect data residency and security policies without constant manual oversight Providing transparency Users can't trust what they can't understand. Effective AI agents maintain visibility through: Real-time progress updates: Especially crucial when user attention might be needed Action explanations: Before executing high-stakes operations, agents should clearly communicate their planned approach Clear error handling: When issues arise, users need immediate alerts with understandable error messages and recovery paths This transparency transforms AI agents from mysterious black boxes into comprehensible partners whose logic users can follow and verify. Remembering context Nothing erodes trust faster than having to repeatedly teach an AI agent the same information. Trust-building agents demonstrate memory through: Preference retention: Accepting and applying user feedback about coding styles, deployment patterns, or workflow preferences Context awareness: Remembering previous instructions and project-specific requirements Adaptive learning: Evolving based on user corrections without requiring explicit reprogramming Our research participants consistently highlighted frustration with tools that couldn't remember basic preferences, forcing them to provide the same guidance repeatedly. Anticipating needs Trust emerges when AI agents proactively support user workflows. Agents could support the user in the following ways: Pattern recognition: Learning user routines and predicting tasks based on time of day or project context Intelligent agent selection: Automatically recognizing which specialized agents are most relevant for specific tasks Environment analysis: Understanding coding environments, dependencies, and project structures without explicit configuration These anticipatory capabilities transform AI agents from reactive tools into proactive partners that reduce cognitive load and streamline development processes. Implementing trust-building features For organizations deploying AI agents, our research suggests several practical implementations: Start with low-risk environments: Allow users to build trust gradually by beginning with non-critical tasks. As confidence grows through positive micro-interactions, users naturally expand their reliance on AI capabilities. Design for continuous orchestration of agents, which includes intervention: Unlike traditional automation, AI agents should know when to pause and seek human input. This intervention assures users they maintain ultimate control while benefiting from AI efficiency. Agents also need autonomy level controls so that they can calibrate autonomy for different types of action, in different contexts. Maintain audit trails: Every agent action should be traceable, allowing users to understand not just what happened, but why the agent made specific decisions. Personalize the experience: Agents that adapt to individual user preferences and team workflows create stronger trust bonds than one-size-fits-all solutions. The compounding impact of trust Our findings reveal that trust in AI agents follows a compound growth pattern. Each positive micro-interaction makes users slightly more willing to rely on the agent for the next task. Over time, these small trust deposits accumulate into deep confidence that transforms AI agents from experimental tools into essential development partners. This trust-building process is delicate ‚Äì a single significant failure can erase weeks of accumulated confidence. That's why consistency in these micro-inflection points is crucial. Every interaction matters. Supporting these micro-inflection points is a cornerstone of having software teams and their AI agents collaborate at enterprise scale with intelligent orchestration. Next steps Building trust in AI agents requires intentional design focused on user needs and concerns. Organizations implementing agentic tools should: Audit their AI agents for trust-building micro-interactions Prioritize transparency and user control in agent design Invest in memory and learning capabilities that reduce user friction Create clear escalation paths for when agents encounter uncertainty Key takeaways Trust in AI agents builds incrementally through micro-inflection points rather than breakthrough moments Four key categories drive trust: safeguarding actions, providing transparency, remembering context, and anticipating needs Small design choices in AI interactions have compound effects on user adoption and long-term reliance Organizations must intentionally design for trust through consistent, positive micro-interactions Help us learn what matters to you: Your experiences and insights are invaluable in shaping how we design and improve agentic interactions. Join our research panel to participate in upcoming studies. Explore GitLab‚Äôs agents in action: GitLab Duo Agent Platform extends AI's speed beyond just coding to your entire software lifecycle. With your workflows defining the rules, your context maintaining organizational knowledge, and your guardrails ensuring control, teams can orchestrate while agents execute across the SDLC. Visit the GitLab Duo Agent Platform site to discover how intelligent orchestration can transform your DevSecOps journey. Whether you're exploring agents for the first time or looking to optimize your existing implementations, we believe that understanding and designing for trust is the key to successful adoption. Let's build that future together!",
      "published_ts": 1767571200,
      "source_name": "GitLab Engineering",
      "content_type": "rex"
    }
  ],
  "cloud_infra_observability": [
    {
      "url": "https://aws.amazon.com/blogs/big-data/use-amazon-sagemaker-custom-tags-for-project-resource-governance-and-cost-tracking/",
      "title": "Use Amazon SageMaker custom tags for project resource governance and cost tracking",
      "summary": "Amazon SageMaker announced a new feature that you can use to add custom tags to resources created through an Amazon SageMaker Unified Studio project. This helps you enforce tagging standards that conform to your organization‚Äôs service control policies (SCPs) and helps enable cost tracking reporting practices on resources created across the organization. In this post, we look at use cases for custom tags and how to use the AWS Command Line Interface (AWS CLI) to add tags to project resources.",
      "published_ts": 1767920651,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/aws-reinvent-takeaways-sanofi",
      "title": "My AWS re:Invent Takeaway: The End of ‚ÄúJust Build an Agent‚Äù",
      "summary": "Each year, AWS re:Invent takes place at roughly the same time and in roughly the same place as the National Finals Rodeo. The result is a uniquely Vegas blend of Patagonia tech vests, oversized hoodies, and wranglers over cowboy boots, which makes the Cosmo Chandelier Bar game of ‚Äúwhich conference does this guy belong to‚Äù all too easy.",
      "published_ts": 1767883025,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/observe-ai-powered-observability",
      "title": "Snowflake Announces Intent to Acquire Observe to Deliver AI-Powered Observability",
      "summary": "Snowflake announces intent to acquire Observe to deliver AI-powered observability, reduce telemetry costs, and unify logs, metrics, and traces at scale.",
      "published_ts": 1767880800,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.uber.com/blog/from-monitoring-to-observability-cloud-native/",
      "title": "From Monitoring to Observability: Our Ultra-Marathon to a Cloud-Native Platform",
      "summary": "Managing a global corporate network at Uber‚Äôs scale can feel a bit like running an ultra-marathon. There are long stretches of smooth sailing, but you‚Äôre always preparing for the unexpected mountain pass or sudden change in weather. For years, our engineering teams have navigated this terrain with a traditional, monolithic monitoring system. We knew we needed to switch to a modern pair of carbon-fiber running shoes. This meant a complete overhaul: a journey to replace our legacy system with a cloud-native observability platform built for speed, flexibility, and endurance on an open-source stack.",
      "published_ts": 1767719913,
      "source_name": "Uber Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/happy-new-year-aws-weekly-roundup-10000-aideas-competition-amazon-ec2-amazon-ecs-managed-instances-and-more-january-5-2026/",
      "title": "Happy New Year! AWS Weekly Roundup: 10,000 AIdeas Competition, Amazon EC2, Amazon ECS Managed Instances and more (January 5, 2026)",
      "summary": "Happy New Year! I hope the holidays gave you time to recharge and spend time with your loved ones. Like every year, I took a few weeks off after AWS re:Invent to rest and plan ahead. I used some of that downtime to plan the next cohort for Become a Solutions Architect (BeSA). BeSA is [‚Ä¶]",
      "published_ts": 1767633037,
      "source_name": "AWS Blog",
      "content_type": "technical"
    }
  ],
  "data_modeling_governance": [
    {
      "url": "https://www.getdbt.com/blog/ai-agents-and-the-data-lake",
      "title": "AI agents and the data lake",
      "summary": "Lauren Anderson, the head of Okta's enterprise data platform, on why central governance and the semantic layer are so essential.",
      "published_ts": 1768144620,
      "source_name": "dbt Blog",
      "content_type": "technical"
    },
    {
      "url": "https://roundup.getdbt.com/p/ai-agents-and-the-data-lake-w-lauren",
      "title": "AI agents and the data lake (w/ Lauren Anderson)",
      "summary": "The head of Okta's enterprise data platform on why central governance and the semantic layer are so essential",
      "published_ts": 1768140185,
      "source_name": "The Analytics Engineering Podcast (dbt)",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataengineeringweekly.com/p/a-critique-of-iceberg-rest-catalog",
      "title": "A Critique of Iceberg REST Catalog: A Classic Case of Why Semantic Spec Fails",
      "summary": "How a Semantically Correct API Becomes Operationally Unreliable at Scale",
      "published_ts": 1767938258,
      "source_name": "Data Engineering Weekly",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/create-aws-glue-data-catalog-views-using-cross-account-definer-roles/",
      "title": "Create AWS Glue Data Catalog views using cross-account definer roles",
      "summary": "In this post, we demonstrate how to use cross-account IAM definer roles with AWS Glue Data Catalog views. We show how data owner accounts can create and manage views in a central governance account while maintaining security and control over their data assets.",
      "published_ts": 1767912319,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/ai-is-reshaping-devsecops-attend-gitlab-transcend-to-see-whats-next/",
      "title": "AI is reshaping DevSecOps: Attend GitLab Transcend to see what‚Äôs next",
      "summary": "AI promises a step change in innovation velocity, but most software teams are hitting a wall. According to our latest Global DevSecOps Report , AI-generated code now accounts for 34% of all development work. Yet 70% of DevSecOps professionals report that AI is making compliance management more difficult, and 76% say agentic AI will create unprecedented security challenges. This is the AI paradox: AI accelerates coding, but software delivery slows down as teams struggle to test, secure, and deploy all that code. Productivity gains meet workflow bottlenecks The problem isn't AI itself. It's how software gets built today. The traditional DevSecOps lifecycle contains hundreds of small tasks that developers must navigate manually: updating tickets, running tests, requesting reviews, waiting for approvals, fixing merge conflicts, addressing security findings. These tasks drain an average of seven hours per week from every team member, according to our research. Development teams are producing code faster than ever, but that code still crawls through fragmented toolchains, manual handoffs, and disconnected processes. In fact, 60% of DevSecOps teams use more than five tools for software development overall, and 49% use more than five AI tools. This fragmentation creates collaboration barriers, with 94% of DevSecOps professionals experiencing factors that limit collaboration in the software development lifecycle. The answer isn't more tools. It's intelligent orchestration that brings software teams and their AI agents together across projects and release cycles, with enterprise-grade security, governance, and compliance built in. Seeking deeper human-AI partnerships DevSecOps professionals don't want AI to take over ‚Äî they want reliable partnerships. The vast majority (82%) say using agentic AI would increase their job satisfaction, and 43% envision an ideal future with a 50/50 split between human and AI contributions. They're ready to trust AI with 37% of their daily tasks without human review, particularly for documentation, test writing, and code reviews. What we heard resoundingly from DevSecOps professionals is that AI won't replace them; rather, it will fundamentally reshape their roles. 83% of DevSecOps professionals believe AI will significantly change their work within five years, and notably, 76% think this will create more engineering jobs, not fewer. As coding becomes easier with AI, engineers who can architect systems, ensure quality, and apply business context will be in high demand. Critically, 88% agree there are essential human qualities that AI will never fully replace, including creativity, innovation, collaboration, and strategic vision. So how can organizations bridge the gap between AI‚Äôs promise and the reality of fragmented workflows? Join us at GitLab Transcend: Explore how to drive real value with agentic AI On February 10, 2026, GitLab will be hosting Transcend, where we'll reveal how intelligent orchestration transforms AI-powered software development. You'll get a first look at GitLab's upcoming product roadmap and learn how teams are solving real-world challenges by modernizing development workflows with AI. Organizations winning in this new era balance AI adoption with security, compliance, and platform consolidation. AI offers genuine productivity gains when implemented thoughtfully ‚Äî not by replacing human developers, but by freeing DevSecOps professionals to focus on strategic thinking and creative innovation. Register for Transcend today to secure your spot and discover how intelligent orchestration can help your software teams stay in flow.",
      "published_ts": 1767830400,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.rudderstack.com/blog/how-to-improve-data-quality",
      "title": "How to improve data quality: 10 best practices for 2026",
      "summary": "Learn how to improve data quality with modern best practices for enterprises for success in 2026",
      "published_ts": 1767725477,
      "source_name": "Rudderstack Blog",
      "content_type": "technical"
    }
  ],
  "etl_orchestration": [
    {
      "url": "https://www.databricks.com/blog/chaos-scale-templatizing-spark-declarative-pipelines-dlt-meta",
      "title": "From Chaos to Scale: Templatizing Spark Declarative Pipelines with DLT-META",
      "summary": "Declarative pipelines give teams an intent driven way to build batch and streaming workflows...",
      "published_ts": 1767825900,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dlthub.com/blog/building-semantic-models-with-llms-and-dlt",
      "title": "Autofiling the Boring Semantic Layer: From Sakila to Chat-BI with dltHub",
      "summary": "Build one semantic model and reuse it across APIs, chatbots, and apps. Let LLMs handle the tedious mapping so you can ship data products that quietly just work.",
      "published_ts": 1767744000,
      "source_name": "dlt Blog",
      "content_type": "technical"
    },
    {
      "url": "https://airbyte.com/blog/search-fetch-write-agents",
      "title": "Search, Fetch, and Write: A Primer | Airbyte",
      "summary": "Why AI agents fail when built on batch data systems and how search, fetch, and write enable real-time, entity-centric agentic data infrastructure.",
      "published_ts": 1767744000,
      "source_name": "Airbyte Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/building-scalable-aws-lake-formation-governed-data-lakes-with-dbt-and-amazon-managed-workflows-for-apache-airflow/",
      "title": "Building scalable AWS Lake Formation governed data lakes with dbt and Amazon Managed Workflows for Apache Airflow",
      "summary": "Organizations often struggle with building scalable and maintainable data lakes‚Äîespecially when handling complex data transformations, enforcing data quality, and monitoring compliance with established governance. Traditional approaches typically involve custom scripts and disparate tools, which can increase operational overhead and complicate access control. A scalable, integrated approach is needed to simplify these processes, improve data reliability, [‚Ä¶]",
      "published_ts": 1767739050,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/how-to-enforce-data-quality-at-every-stage-a-practical-guide-to-catching-issues-before-they-cost-you",
      "title": "How to Enforce Data Quality at Every Stage of Your Data Pipeline",
      "summary": "Learn how to enforce data quality across ingestion, transformation, and consumption. Catch data issues early and prevent broken dashboards with Dagster.",
      "published_ts": 1767723794,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/dagster-airflow",
      "title": "Dagster vs Airflow: Feature Comparison",
      "summary": "Discover why Dagster outpaces Airflow with modern UX, modular pipeline design, asset tracking, and easier maintenance for growing teams.",
      "published_ts": 1767650692,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://seattledataguy.substack.com/p/common-data-pipeline-patterns-youll",
      "title": "Common Data Pipeline Patterns You‚Äôll See in the Real World",
      "summary": "A practical look at the many ways data pipelines show up inside real companies",
      "published_ts": 1767643086,
      "source_name": "Seattle Data Guy",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/when-sync-isnt-enough",
      "title": "When Sync Isn‚Äôt Enough: Async Execution Inside Dagster",
      "summary": "Explore why and how to use an async executor in Dagster to efficiently run highly concurrent, I/O-bound workflows like inference, embeddings, and real-time enrichment.",
      "published_ts": 1767624240,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    }
  ],
  "hors_sujet": [],
  "lake_storage_formats": [
    {
      "url": "https://aws.amazon.com/blogs/big-data/aws-analytics-at-reinvent-2025-unifying-data-ai-and-governance-at-scale/",
      "title": "AWS analytics at re:Invent 2025: Unifying Data, AI, and governance at scale",
      "summary": "re:Invent 2025 showcased the bold Amazon Web Services (AWS) vision for the future of analytics, one where data warehouses, data lakes, and AI development converge into a seamless, open, intelligent platform, with Apache Iceberg compatibility at its core. Across over 18 major announcements spanning three weeks, AWS demonstrated how organizations can break down data silos, [‚Ä¶]",
      "published_ts": 1767825865,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    }
  ],
  "news": [
    {
      "url": "https://www.databricks.com/blog/how-7-eleven-transformed-maintenance-technician-knowledge-access-databricks-agent-bricks",
      "title": "How 7‚ÄëEleven Transformed Maintenance Technician Knowledge Access with Databricks Agent Bricks",
      "summary": "Empowering Technicians Across Every Store7‚ÄëEleven‚Äôs maintenance technicians keep...",
      "published_ts": 1767999600,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.youtube.com/watch?v=RG4VyQARSfg",
      "title": "Stackoverflow is basically dead",
      "summary": "Yup, it's bad!\nHere's the source for the material I showed: https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/\n\nIf you want to learn Data Engineering check out: LearnDataEngineering.com",
      "published_ts": 1767989760,
      "source_name": "Andreas Kretz (YouTube)",
      "content_type": "technical"
    },
    {
      "url": "https://www.rudderstack.com/blog/data-trust-clickstream-discrepancy",
      "title": "Data trust is death by a thousand paper cuts",
      "summary": "Data trust breaks via small data paper cuts. Learn how to debug clickstream discrepancies and prevent drift, bots, and pipeline misconfigurations in the AI era.",
      "published_ts": 1767973353,
      "source_name": "Rudderstack Blog",
      "content_type": "technical"
    },
    {
      "url": "https://juhache.substack.com/p/boring-engineering-manifesto",
      "title": "The Boring Engineer Manifesto",
      "summary": "Ju Data Engineering Weekly - Ep 92",
      "published_ts": 1767961881,
      "source_name": "Julien Hurault ¬∑ Ju Data Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.youtube.com/watch?v=YiT8YejCf6g",
      "title": "Data Governance Is NOT the Foundation for AI",
      "summary": "Data Governance Is NOT the Foundation for AI\nThat statement alone triggers a lot of people.\nAnd there‚Äôs a reason for that.\n\nIn this video, I break down why starting AI initiatives with data governance, definitions, and policies often leads to stalled projects, endless meetings, and zero business impact.\n\nThis is not an argument against data governance.\nIt‚Äôs an argument against treating it as the starting point.\nI‚Äôve seen AI and data teams spend months arguing over definitions, ownership, and frameworks,while leadership waits for results and eventually moves on. When AI fails, governance is blamed. But in reality, the problem usually starts much earlier.\n\nThis video covers:\nWhy data governance is often framed as the foundation for AI\nWhat actually causes AI projects to fail in companies\nThe difference between governance as a prerequisite vs a byproduct\nWhy leadership, goals, and direction matter more than documentation\nHow real AI systems change how governance should be built\nIf you work in data engineering, analytics, AI, or platform teams, this will likely feel uncomfortable and familiar.",
      "published_ts": 1767903443,
      "source_name": "Andreas Kretz (YouTube)",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/securing-grid-practical-guide-cyber-analytics-energy-utilities",
      "title": "Securing the Grid: A Practical Guide to Cyber Analytics for Energy & Utilities",
      "summary": "How Modern Data Platforms Are Transforming Cybersecurity Operations in Critical InfrastructureThe...",
      "published_ts": 1767831300,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/how-iit-bombay-students-code-future-with-gitlab/",
      "title": "How IIT Bombay students are coding the future with GitLab",
      "summary": "The GitLab team recently had the privilege of judging the iHack Hackathon at IIT Bombay's E-Summit . The energy was electric, the coffee was flowing, and the talent was undeniable. But what struck us most wasn't just the code ‚Äî it was the sheer determination of students to solve real-world problems, often overcoming significant logistical and financial hurdles to simply be in the room. Through our GitLab for Education program , we aim to empower the next generation of developers with tools and opportunity. Here is a look at what the students built, and how they used GitLab to bridge the gap between idea and reality. The challenge: Build faster, build securely The premise for the GitLab track of the hackathon was simple: Don't just show us a product; show us how you built it. We wanted to see how students utilized GitLab's platform ‚Äî from Issue Boards to CI/CD pipelines ‚Äî to accelerate the development lifecycle. The results were inspiring. The winners 1st place: Team Decode ‚Äî Democratizing Scientific Research Project: FIRE (Fast Integrated Research Environment) Team Decode took home the top prize with a solution that warms a developer's heart: a local-first, blazing-fast data processing tool built with Rust and Tauri. They identified a massive pain point for data science students: existing tools are fragmented, slow, and expensive. Their solution, FIRE, allows researchers to visualize complex formats (like NetCDF) instantly. What impressed the judges most was their \"hacker\" ethos. They didn't just build a tool; they built it to be open and accessible. How they used GitLab: Since the team lived far apart, asynchronous communication was key. They utilized GitLab Issue Boards and Milestones to track progress and integrated their repo with Telegram to get real-time push notifications. As one team member noted, \"Coordinating all these technologies was really difficult, and what helped us was GitLab... the Issue Board really helped us track who was doing what.\" 2nd place: Team BichdeHueDost ‚Äî Reuniting to Solve Payments Project: SemiPay (RFID Cashless Payment for Schools) The team name, BichdeHueDost, translates to \"Friends who have been set apart.\" It's a fitting name for a group of friends who went to different colleges but reunited to build this project. They tackled a unique problem: handling cash in schools for young children. Their solution used RFID cards backed by a blockchain ledger to ensure secure, cashless transactions for students. How they used GitLab: They utilized GitLab CI/CD to automate the build process for their Flutter application (APK), ensuring that every commit resulted in a testable artifact. This allowed them to iterate quickly despite the \"flaky\" nature of cross-platform mobile development. 3rd place: Team ZenYukti ‚Äî The Eyes of the Campus Project: KSHR (Unified Intelligence Platform) Team ZenYukti impressed us with a heavy-duty enterprise architecture. They built a comprehensive campus monitoring system designed to detect anomalies and ensure student safety using CCTV and biometric data. How they used GitLab: This team showed a sophisticated understanding of DevOps. They used GitLab CI with conditional logic, triggering specific pipelines only when front-end or back-end folders changed. They also utilized private container registries to manage their Docker images securely. Beyond the code: A lesson in inclusion While the code was impressive, the most powerful moment of the event happened away from the keyboard. During the feedback session, we learned about the journey Team ZenYukti took to get to Mumbai. They traveled over 24 hours, covering nearly 1,800 kilometers. Because flights were too expensive and trains were booked, they traveled in the \"General Coach,\" a non-reserved, severely overcrowded carriage. As one student described it: \"You cannot even imagine something like this... there are no seats... people sit on the top of the train. This is what we have endured.\" This hit home. Diversity, Inclusion, and Belonging are core values at GitLab. We realized that for these students, the barrier to entry wasn't intellect or skill, it was access. In that moment, we decided to break that barrier. We committed to reimbursing the travel expenses for the participants who struggled to get there. It's a small step, but it underlines a massive truth: talent is distributed equally, but opportunity is not. The future is bright (and automated) We also saw incredible potential in teams like Prometheus, who attempted to build an autonomous patch remediation tool (DevGuardian), and Team Arrakis, who built a voice-first job portal for blue-collar workers using GitLab Duo to troubleshoot their pipelines. To all the students who participated: You are the future. Through GitLab for Education , we are committed to providing you with the top-tier tools (like GitLab Ultimate) you need to learn, collaborate, and change the world ‚Äî whether you are coding from a dorm room, a lab, or a train carriage. Keep shipping. üí° Learn more about the GitLab for Education program .",
      "published_ts": 1767830400,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/migration-at-scale-moving-marketing-cloud-caching-from-memcached-to-redis-at-1-5m-rps-without-downtime/",
      "title": "Migration at Scale: Moving Marketing Cloud Caching from Memcached to Redis at 1.5M RPS Without Downtime",
      "summary": "By Paladi Sandhya Madhuri, Rakesh Chhabra, Piyush Pruthi, Sumit Sahrawat, Ankit Jain, and Basaveshwar Hiremath. In our Engineering Energizers Q&A series, we highlight the engineering minds driving innovation across Salesforce. Today‚Äôs edition features Paladi Sandhya Madhuri, a Senior Software Engineer on the Marketing Cloud Caching team, whose work involves evolving the platform‚Äôs core caching infrastructure [‚Ä¶] The post Migration at Scale: Moving Marketing Cloud Caching from Memcached to Redis at 1.5M RPS Without Downtime appeared first on Salesforce Engineering Blog .",
      "published_ts": 1767766740,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/rise-of-the-data-platform-engineer",
      "title": "The Rise of the Data Platform Engineer",
      "summary": "The evolution of data engineering demands a platform-first mindset. See how Data Platform Engineers are shaping the future of analytics.",
      "published_ts": 1767731183,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/gemini-3-snowflake-cortex-ai",
      "title": "Announcing Gemini 3 Within Snowflake Cortex AI: A Deeper Google Cloud Collaboration to Help Customers Build Smarter, Faster and More Secure AI",
      "summary": "Snowflake and Google Cloud bring Gemini AI models natively to Snowflake Cortex AI, enabling customers to build and scale generative AI apps with governed data.",
      "published_ts": 1767708000,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/bgp-route-leak-venezuela/",
      "title": "A closer look at a BGP anomaly in Venezuela",
      "summary": "There has been speculation about the cause of a BGP anomaly observed in Venezuela on January 2. We take a look at BGP route leaks, and dive into what the data suggests caused the anomaly in question.",
      "published_ts": 1767686400,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/bcbs-239-compliance-age-ai-turning-regulatory-burden-strategic-advantage",
      "title": "BCBS 239 Compliance in the Age of AI: Turning Regulatory Burden into Strategic Advantage",
      "summary": "The strategic imperative of BCBS 239 complianceBCBS 239 (Risk Data Aggregation and...",
      "published_ts": 1767639660,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-251",
      "title": "Data Engineering Weekly #251",
      "summary": "The Weekly Data Engineering Newsletter",
      "published_ts": 1767591351,
      "source_name": "Data Engineering Weekly",
      "content_type": "technical"
    }
  ],
  "news_general": [],
  "python_analytics": [],
  "warehouses_engines": [
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/data-ai-upskilling-series",
      "title": "Snowflake Offers Free Data and AI Upskilling Event Series",
      "summary": "Fast-track your data and AI skills with insights from Snowflake‚Äôs leading experts",
      "published_ts": 1767920100,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/regulated-workloads-snowflake-backups",
      "title": "Modernizing Regulated Workloads with Snowflake Backups",
      "summary": "Snowflake Backups, now generally available, enable immutable retention for critical data to help meet regulatory and security needs.",
      "published_ts": 1767805140,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://duckdb.org/2026/01/06/duckdb-on-loongarch-morefine.html",
      "title": "DuckDB on LoongArch",
      "summary": "In today's ‚ÄúWhat's on your desk?‚Äù episode, we test a Loongson CPU with the LoongArch architecture.",
      "published_ts": 1767657600,
      "source_name": "DuckDB Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/simplify-multi-warehouse-data-governance-with-amazon-redshift-federated-permissions/",
      "title": "Simplify multi-warehouse data governance with Amazon Redshift federated permissions",
      "summary": "Amazon Redshift federated permissions simplify permissions management across multiple Redshift warehouses. In this post, we show you how to define data permissions one time and automatically enforce them across warehouses in your AWS account, removing the need to re-create security policies in each warehouse.",
      "published_ts": 1767648001,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/nvidia-reachy-mini",
      "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
      "summary": "NVIDIA brings agents to life with DGX Spark and Reachy Mini\nToday at CES 2026, NVIDIA unveiled a world of new open models to enable the future of agents, online and in the real world. From the recently released\nNVIDIA Nemotron\nreasoning LLMs to the new\nNVIDIA Isaac GR00T N1.6\nopen reasoning VLA and\n",
      "published_ts": 1767571200,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    }
  ]
}