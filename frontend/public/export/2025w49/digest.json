{
  "ai_data_engineering": [
    {
      "url": "https://www.dataiku.com/stories/blog/from-ai-wild-west-to-main-street",
      "title": "From AI Wild West to Main Street: Sustainability for Enterprise Agents",
      "summary": "Over the last two years, AI agents have exploded across the enterprise landscape. Marketing teams prototype assistants for campaign analytics . Operations teams build copilots for scheduling and logistics. Finance experiments with reconciliation agents. Product teams embed autonomous workflows into customer-facing features.",
      "published_ts": 1764943380,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/mcp-powered-financial-ai-workflows-databricks",
      "title": "MCP-Powered Financial AI Workflows on Databricks",
      "summary": "To understand the foundations of Model Context Protocol (MCP) and Agent Bricks, see...",
      "published_ts": 1764889200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://medium.com/walmartglobaltech/decoding-brickstorms-garble-strings-b0a60828b3cc?source=rss----905ea2b3d4d1---4",
      "title": "Decoding Brickstorms Garble strings",
      "summary": "By: Jason Reaves From the recent attention to Brickstorm in public reporting, I analyzed the Brickstorm sample listed below that was recently uploaded to Virustotal: 90b760ed1d0dcb3ef0f2b6d6195c9d852bcb65eca293578982a8c4b64f51b035 This sample was obfuscated via an open-source Golang tool called Garble[4] which was mentioned by Mandiant[1]. Mandiant also released a tool for decoding Garble strings[2] which was based on prior work by OALabs[3]. A quick note before diving in: I developed my string decoder independently, without first reviewing the work published by Google Threat Intelligence (GTI). Interestingly, my approach ended up aligning closely with their research on similar samples. I want to spotlight the method detailed in GTI’s blog, which takes a deep dive into Garble’s AST transformation code, a technique that offered valuable insight into how the obfuscation operates. The strings are mostly stack based although some of the larger ones reside elsewhere and are loaded. For harvesting the bytecode sequences I decided to leverage a few possible yara rules which started strict and was relaxed as I continued my work: #good for most #rules = yara.compile(source='rule urls { strings: $a1 = {48 b8 [8] 48 89 [3] 48 b8 [10-200] 48 [2] 48 [3] 7c ??} condition: all of them }') rules = yara.compile(source='rule urls { strings: $a1 = {48 b8 [8] 48 89 [3] [10-400] 48 [2] 48 [3] 7c ??} condition: all of them }') rules2 = yara.compile(source='rule urls { strings: $a1 = {48 8d [1-5] c? 44 [6] [10-400] 48 [2] 48 [3] 7c ??} condition: all of them }') After retrieving all the matches and removing possible substrings I can emulate the code: blobs = uniq_blobs out = b'' if True: STACK=0x9000 code_base = 0x100000 for blob in blobs: mu = Uc(UC_ARCH_X86,UC_MODE_64) mu.mem_map(code_base, 0x100000) mu.mem_map(STACK, 4096*10) #Make sure it ends gracefully blob = blob.matched_data+b'\\x90\\x90\\x90' #Debugging #if binascii.unhexlify('48b8532f2e5e6d03c6b1488944244048') in blob: #    print(\"Found it\") #    print(binascii.hexlify(blob)) mu.mem_write(code_base, b'\\x00'*0x100000) mu.mem_write(STACK, b'\\x00'*(4096*10)) mu.mem_write(code_base,blob) #Debugging: #mu.hook_add(UC_HOOK_BLOCK, hook_block) #mu.hook_add(UC_HOOK_CODE, hook_code) mu.reg_write(UC_X86_REG_RSP,STACK+4096) mu.reg_write(UC_X86_REG_RBP,STACK+4096) mu.reg_write(UC_X86_REG_ESP,STACK+4096) mu.reg_write(UC_X86_REG_EBP,STACK+4096) mu.reg_write(UC_X86_REG_RAX,0) try: mu.emu_start(code_base, code_base+len(blob), timeout=10000) except: continue a = mu.mem_read(STACK,4096*10) a = b''.join(a.split(b'\\x00')) l = mu.reg_read(UC_X86_REG_RAX)+1 if len(a) > 0: print(str(a)[-l:]) out += a I noticed afterwords that OALabs also used a hook but they did so to find the next call instruction: def trace(uc, address, size, user_data): insn = next(cs.disasm(uc.mem_read(address, size), address)) #print(f\"{address:#010x}:\\t{insn.mnemonic}\\t{insn.op_str}\") if insn.mnemonic == 'call': #print(\"Ending on a call!\") uc.emu_stop() I like this approach and will definitely leverage it later, I primarily used hooks for debugging my code in comparison: def hook_block(uc, address, size, user_data): print(\">>> Tracing basic block at 0x%x, block size = 0x%x\" %(address, size)) def hook_code(uc, address, size, user_data): print(user_data) print(uc.reg_read(UC_X86_REG_RCX)) print(\">>> Tracing instruction at 0x%x, instruction size = 0x%x\" %(address, size)) #Debugging: #mu.hook_add(UC_HOOK_BLOCK, hook_block) #mu.hook_add(UC_HOOK_CODE, hook_code) This won’t get every single string as some are passed as offsets to the data residing in rodata section for longer pieces. Decoding: Handshake' undefined' invalid perspective' Handshake' 0-RTT Protected' unknown packet type: %d' QUIC_GO_LOG_LEVEL' initial RTT set after first measurement\" %#x doesn't fit into 62 bits\" invalid varint length\" cannot encode %d in %d bytes\" value doesn't fit into 62 bits: \" chacha20: wrong key size' chacha20: SetCounter attempted to rollback counter\" chacha20: output smaller than input' chacha20: invalid buffer overlap' chacha20: internal error: wrong dst and/or src length' chacha20: wrong HChaCha20 key size' chacha20: wrong HChaCha20 nonce size' GODEBUG sys/cpu: no value specified for \"' GODEBUG sys/cpu: value \"' \" not supported for cpu option \"' GODEBUG sys/cpu: unknown cpu feature \"' GODEBUG sys/cpu: can not enable \"' , missing CPU support\\n' GODEBUG sys/cpu: can not disable \"' , required CPU feature\\n' avx512vbmi' avx512vnniw' avx5124fmaps' avx512vpopcntdq' avx512gfni\" avx512vaes' avx512vbmi2' avx512bitalg' avx512bf16' pclmulqdq' poly1305: write to MAC after Sum or Verify' chacha20poly1305: bad nonce length passed to Seal' chacha20poly1305: plaintext too large' chacha20poly1305: bad nonce length passed to Open\" chacha20poly1305: ciphertext too large' chacha20poly1305: invalid buffer overlap\" chacha20poly1305: invalid buffer overlap\" chacha20poly1305: invalid buffer overlap' chacha20poly1305: invalid buffer overlap\" 20060102150405Z0700' cryptobyte: pending child length %d exceeds %d-byte length prefix\" cryptobyte: BuilderContinuation reallocated a fixed-size buffer\" cryptobyte: attempted write while child is pending\" cryptobyte: length overflow' cryptobyte: Builder is exceeding its fixed-size buffer' bad point length: %d, expected %d' bad input point: low order point' hkdf: entropy limit reached' close notify' unexpected message' bad record MAC' decryption failed' record overflow' decompression failure' handshake failure' bad certificate' unsupported certificate' revoked certificate' expired certificate' unknown certificate' illegal parameter' unknown certificate authority' access denied' error decoding message' error decrypting message' export restriction' protocol version not supported' insufficient security level\" internal error' inappropriate fallback' user canceled' unsupported extension' certificate unobtainable' unrecognized name' bad certificate status response' certificate required' no application protocol' GTLS 1.3, server CertificateVerify\" _TLS 1.3, client CertificateVerify' tls: no certificates configured' CLIENT_RANDOM' CLIENT_EARLY_TRAFFIC_SECRET' CLIENT_HANDSHAKE_TRAFFIC_SECRET\" SERVER_HANDSHAKE_TRAFFIC_SECRET' CLIENT_TRAFFIC_SECRET_0' SERVER_TRAFFIC_SECRET_0' tls: invalid ClientKeyExchange message' tls: invalid ServerKeyExchange message' res binder' c hs traffic' s hs traffic' c ap traffic' s ap traffic' exp master' res master' traffic upd' master secret' key expansion\" client finished\" server finished' tls: alert(' expected an ECDSA public key, got %T' Ed25519 verification failure' expected an RSA public key, got %T\" expected an RSA public key, got %T' internal error: unknown signature type' unsupported signature algorithm: %v\" unsupported signature algorithm: %v' tls: Ed25519 public keys are not supported before TLS 1.2' tls: unsupported public key: %T' tls: unsupported certificate: private key is %T, expected *%T' tls: certificate private key (%T) does not implement crypto.Signer' tls: unsupported certificate curve (%s)' tls: certificate RSA key size too small for supported signature algorithms' tls: unsupported certificate key (%T)\" tls: peer doesn't support the certificate custom signature algorithms\" tls: internal error: unsupported key (%T)' tls: internal error: wrong nonce length' tls: internal error: wrong nonce length' tls: internal error: wrong nonce length\" tls: unable to generate random session ticket key: %v' s %x %x\\n' tls: received unexpected handshake message of type %T when waiting for %T' TLS: sequence number wraparound' unknown cipher type' unknown cipher type' unknown cipher type\" unsupported SSLv2 handshake received' received record with version %x when expecting version %x' remote error' remote error' tls: too many ignored records' local error' local error' unknown cipher type' tls: handshake message of length %d bytes exceeds maximum of %d bytes' tls: internal error: unexpected renegotiation' tls: unknown Renegotiation value\" tls: too many non-advancing records' tls: received unexpected handshake message of type %T' tls: internal error: handshake should have had a result' tls: invalid NextProtos value' tls: NextProtos values too large' tls: no supported versions satisfy MinVersion and MaxVersion' tls: short read from Rand: \" tls: short read from Rand: ' c e traffic\" resumption' tls: server selected unsupported protocol version %x' tls: received unexpected CertificateStatus message' tls: server's identity changed during renegotiation\" tls: failed to write to key log: ' tls: server selected unsupported compression format' tls: initial handshake had non-empty renegotiation extension\" tls: server advertised unrequested ALPN extension' tls: server resumed a session with a different version' tls: server resumed a session with a different cipher suite' tls: server's Finished message was incorrect\" tls: server selected TLS 1.3 using the legacy version field' tls: server selected an invalid version after a HelloRetryRequest' tls: server sent an incorrect legacy version' tls: server sent a ServerHello extension forbidden in TLS 1.3\" tls: server selected unsupported compression format' tls: server changed cipher suite after a HelloRetryRequest\" tls: server sent an unnecessary HelloRetryRequest message\" tls: received malformed key_share extension' tls: server selected unsupported group\" tls: server sent an unnecessary HelloRetryRequest key_share\" tls: server sent a cookie in a normal ServerHello' tls: malformed key_share extension' tls: server did not send a key share' tls: server selected unsupported group' tls: server selected an invalid PSK' tls: server selected an invalid PSK and cipher suite pair' tls: invalid server key share' LPN negotiation failed. Server didn\\'t offer any protocols' ALPN negotiation failed. Server offered: %q' tls: server advertised unrequested ALPN extension' tls: certificate used with invalid signature algorithm' tls: certificate used with invalid signature algorithm\" tls: invalid signature by the server certificate: ' tls: invalid server finished hash' tls: failed to sign handshake: ' tls: received a session ticket with invalid lifetime' invalid value length: expected %d, got %d' tls: internal error: failed to update binders' tls: negotiated TLS < 1.3 when using QUIC' tls: client offered old TLS version %#x' tls: client offered only unsupported versions: %x' tls: client does not support uncompressed connections\" tls: initial handshake had non-empty renegotiation extension' tls: unsupported signing key type (%T)' tls: unsupported decryption key type (%T)\" tls: no cipher suite supported by both client and server' tls: client using inappropriate protocol fallback' tls: client certificate used with invalid signature algorithm' tls: invalid signature by the client certificate: ' ls: client\\'s Finished message is incorrect' tls: failed to parse client certificate: \" ls: client didn\\'t provide a certificate' tls: failed to verify client certificate: ' tls: client certificate contains an unsupported public key of type %T' tls: client used the legacy version field to negotiate TLS 1.3' tls: client using inappropriate protocol fallback' tls: TLS 1.3 client supports illegal compression methods' b'\\x1c9\\xbfZ8\\xf4\\xe1\\xccWH\\x8eUE4\\x19\\xb4\\x85\\x87qS%D\\xe8\\xe6\\tH\\xb8\\xf3' f\\xa5#!\\xef9 handshake had non-empty renegotiation extension' tls: no cipher suite supported by both client and server\" tls: no ECDHE curve supported by both client and server' tls: invalid or missing PSK binders' tls: client sent unexpected early data' resumption' tls: internal error: failed to clone hash\" tls: invalid PSK binder\" c e traffic' tls: client sent invalid key share in second ClientHello' tls: client indicated early data in second ClientHello' tls: client illegally modified second ClientHello' tls: client offered 0-RTT data in second ClientHello' ALPN negotiation failed. Client offered: %q' tls: failed to sign handshake: ' tls: client certificate used with invalid signature algorithm' tls: client certificate used with invalid signature algorithm' tls: invalid signature by the client certificate: ' tls: invalid client finished hash' tls: certificate private key does not implement crypto.Decrypter\" tls: unexpected ServerKeyExchange' tls: no supported elliptic curves offered' tls: certificate cannot be used with the selected cipher suite' tls: failed to sign ECDHE parameters: ' tls: server selected unsupported curve\" tls: server selected unsupported curve' tls: certificate used with invalid signature algorithm' tls: invalid signature by the server certificate: ' tls: missing ServerKeyExchange message' tls: HKDF-Expand-Label invocation failed unexpectedly' tls: internal error: unsupported curve' unknown version' server finished' master secret' key expansion' crypto/tls: reserved ExportKeyingMaterial label: %s' crypto/tls: ExportKeyingMaterial context too long' tls: internal error: session ticket keys unavailable' tls: failed to create cipher while encrypting ticket: ' tls.ConnectionState doesn\\'t match' qtls.ClientSessionState doesn't match\" qtls.CertificateRequestInfo doesn't match\" CONNECTION_REFUSED' FLOW_CONTROL_ERROR' STREAM_LIMIT_ERROR' STREAM_STATE_ERROR' INVALID_TOKEN' APPLICATION_ERROR\" CRYPTO_BUFFER_EXCEEDED' NO_VIABLE_PATH' CRYPTO_ERROR (%#x)' unknown error code: %#x' (frame type: %#x)' Application error %#x\" timeout: handshake did not complete in time' no compatible QUIC version found (we support %s, server offered %s)\" received a stateless reset with token %x' unsupported version' invalid first ACK range' invalid packet number length: %d' invalid connection ID length: %d bytes' invalid connection ID length: %d bytes' invalid packet number length: %d' unknown frame type' %s not allowed at encryption level %s' unknown encryption level' not a QUIC packet' {Largest: %d, Smallest: %d}\" t%s &wire.MaxDataFrame{MaximumData: %d}' t%s &wire.MaxStreamDataFrame{StreamID: %d, MaximumStreamData: %d}' t%s &wire.DataBlockedFrame{MaximumData: %d}' t%s &wire.StreamDataBlockedFrame{StreamID: %d, MaximumStreamData: %d}' t%s &wire.MaxStreamsFrame{Type: uni, MaxStreamNum: %d}' t%s &wire.MaxStreamsFrame{Type: bidi, MaxStreamNum: %d}\" t%s &wire.StreamsBlockedFrame{Type: uni, MaxStreams: %d}' t%s &wire.StreamsBlockedFrame{Type: bidi, MaxStreams: %d}' t%s &wire.NewTokenFrame{Token: %#x}' %d exceeds the maximum stream count\" Retire Prior To value (%d) larger than Sequence Number (%d)' invalid connection ID length: %d' invalid connection ID length: %d' token must not be empty' wire.PutStreamFrame called with packet of wrong size!\" stream data overflows maximum offset' StreamFrame: attempting to write empty frame without FIN' %d exceeds the maximum stream count\" remaining length (%d) smaller than parameter length (%d)' client sent a preferred_address\" wrong length for disable_active_migration: %d (expected empty)\" client sent a stateless_reset_token' wrong length for stateless_reset_token: %d (expected 16)\" client sent an original_destination_connection_id\" client sent a retry_source_connection_id' missing original_destination_connection_id' missing initial_source_connection_id' received duplicate transport parameter %#x' invalid connection ID length: %d\" expected preferred_address to be %d long, read %d bytes' inconsistent transport parameter length for transport parameter %#x' initial_max_streams_bidi too large: %d (maximum %d)' initial_max_streams_uni too large: %d (maximum %d)\" invalid value for max_packet_size: %d (minimum 1200)' invalid value for ack_delay_exponent: %d (maximum %d)' invalid value for max_ack_delay: %dms (maximum %dms)' TransportParameter BUG: transport parameter %d not found' unknown transport parameter marshaling version: %d' RetrySourceConnectionID: %s, ' Version Negotiation packet has empty version list' Version Negotiation packet has a version list with an invalid length\" operation not permitted\" no such process' interrupted system call' input/output error' argument list too long' exec format error' bad file descriptor' no child processes' resource temporarily unavailable\" cannot allocate memory\" permission denied' bad address' block device required' device or resource busy' file exists' no such device' not a directory' is a directory' too many open files in system' too many open files' inappropriate ioctl for device\" text file busy' file too large' no space left on device' illegal seek' read-only file system' too many links' broken pipe' numerical argument out of domain' numerical result out of range' file name too long\" no locks available\" function not implemented' ENOTEMPTY' directory not empty' too many levels of symbolic links' identifier removed' channel number out of range' level 3 halted' level 3 reset\" link number out of range\" protocol driver not attached\" level 2 halted' exchange full' invalid request code\" invalid slot' bad font file format' device not a stream\" no data available' timer expired' out of streams resources' machine is not on the network' package not installed' link has been severed\" advertise error' srmount error' communication error on send\" protocol error' EMULTIHOP' multihop attempted' RFS specific error' bad message' EOVERFLOW\" value too large for defined data type' file descriptor in bad state' remote address changed\" can not access a needed shared library' accessing a corrupted shared library' .lib section in a.out corrupted' invalid or incomplete multibyte or wide character' interrupted system call should be restarted' too many users' EDESTADDRREQ' destination address required' protocol wrong type for socket\" ENOPROTOOPT' protocol not available' EPROTONOSUPPORT' protocol not supported' ESOCKTNOSUPPORT' operation not supported' EPFNOSUPPORT' protocol family not supported' EAFNOSUPPORT' address family not supported by protocol' EADDRINUSE' address already in use' EADDRNOTAVAIL\" cannot assign requested address' ENETUNREACH' network is unreachable' ENETRESET' network dropped connection on reset' ECONNABORTED\" software caused connection abort' ECONNRESET' connection reset by peer\" transport endpoint is already connected' ESHUTDOWN\" cannot send after transport endpoint shutdown' ETOOMANYREFS' too many references: cannot splice' ETIMEDOUT' connection timed out' ECONNREFUSED\" connection refused' EHOSTDOWN\" host is down' EHOSTUNREACH' EINPROGRESS' stale file handle' structure needs cleaning' not a XENIX named type file' no XENIX semaphores available' is a named type file' EREMOTEIO' disk quota exceeded' ENOMEDIUM' no medium found' EMEDIUMTYPE' wrong medium type' ECANCELED' operation canceled' key has expired' EKEYREVOKED' key has been revoked' EKEYREJECTED' key was rejected by service' EOWNERDEAD' owner died' ENOTRECOVERABLE' state not recoverable' operation not possible due to RF-kill\" EHWPOISON' memory page has hardware error' interrupt' illegal instruction' trace/breakpoint trap' bus error' floating point exception' user defined signal 1' segmentation fault\" user defined signal 2' broken pipe' alarm clock' terminated' SIGSTKFLT' stack fault' child exited\" continued' stopped (tty input)' stopped (tty output)' urgent I/O condition' CPU time limit exceeded' file size limit exceeded\" SIGVTALRM' virtual timer expired' profiling timer expired' I/O possible' power failure\" bad system call' not implemented on ' unknown connection type' short message' invalid address' short address' short address' destination unreachable' packet too big\" time exceeded' parameter problem' echo request' echo reply' multicast listener query' router solicitation' router advertisement' neighbor solicitation' neighbor advertisement' icmp node information query' icmp node information response' home agent address discovery request message' home agent address discovery reply message\" certification path solicitation message' certification path advertisement message' multicast router advertisement\" multicast router solicitation' multicast router termination' fmipv6 messages' rpl control message\" ilnpv6 locator update message' mpl control message' extended echo request' extended echo reply' invalid connection' missing address' not implemented on ' echo reply' destination unreachable' router advertisement' router solicitation' time exceeded' parameter problem' timestamp' timestamp reply' extended echo reply' invalid connection' missing address' nil header' not implemented on \" congestion BUG: decreased max datagram size from %d to %d' received packet with unknown encryption level: %s' Cannot drop keys for encryption level %s' unexpected encryption level' tIgnoring all packets below %d.' tQueueing ACK because the first packet should be acknowledged.' tQueueing ACK because packet %d was missing before.\" tSetting ACK timer to max ack delay: %s' tQueuing ACK because there's a new missing packet to report.\" Sending ACK because the ACK timer expired.' pto (Initial)' pto (Handshake)' pto (Application Data)' invalid send mode: %d' negative bytes_in_flight' Cannot drop keys for encryption level %s' invalid packet number space' Peer doesn't await address validation any longer.\" received an ACK for skipped packet number: %d (%s)' tnewly acked packets (%d): %d' Canceling loss detection timer. Amplification limited.' Canceling loss detection timer. No packets in flight.' tlost packet %d (reordering threshold)\" tsetting loss timer for packet %d (%s) to %s (in %s)' Loss detection alarm fired in loss timer mode. Loss time: %s' Loss detection alarm for %s fired in PTO mode. PTO count: %d' PTO timer in unexpected encryption level: %s\" Amplification window limited. Received %d bytes, already sent out %d bytes' Limited by the number of tracked packets: tracking %d packets, maximum %d' Congestion limited: bytes in flight %d, window %d\" Max outstanding limited: tracking %d packets, maximum: %d' no frames' packet %d not found in sent packet history' CryptoSetup: keys at this encryption level not yet available' CryptoSetup: keys were already dropped' decryption failed' ClientHello' ServerHello' Certificate' CertificateRequest' CertificateVerify' Received %s message (%d bytes, encryption level: %s)\" missing quic_transport_parameters extension\" unexpected handshake message: %d' expected handshake message %s to have encryption level %s, has %s\" Restoring of transport parameters from session ticket failed: %s' mismatching version. Got %d, expected %d\" Unmarshalling transport parameters from session ticket failed: %s' Accepting 0-RTT. Restoring RTT from session ticket: %s\" error while handling the handshake message' error while handling the handshake message\" Received 0-RTT read key for the client' Installed 0-RTT Read keys (using %s)' Installed Handshake Read keys (using %s)' Installed 1-RTT Read keys (using %s)' unexpected read encryption level' Received 0-RTT write key for the server' Installed 0-RTT Write keys (using %s)\" Installed Handshake Write keys (using %s)' Installed 1-RTT Write keys (using %s)\" Dropping 0-RTT keys.' unexpected write encryption level' Doing 0-RTT.' Dropping Initial keys.' Dropping Handshake keys.' Dropping 0-RTT keys.' Invalid cipher suite id: %d\" error creating new AES cipher: %s\" invalid sample size' invalid sample size' quic: HKDF-Expand-Label invocation failed unexpectedly' client in' server in\" unexpected Retry integrity tag length: %d\" failed to read session ticket revision' unknown session ticket revision: %d\" failed to read RTT' unmarshaling transport parameters from session ticket failed: %s\" rest when unpacking token: %d\" token too short: %d\" quic-go token source' Dropping key phase %d ahead of scheduled time. Drop time was: %s' Starting key drop timer to drop key phase %d (in %s)' unknown cipher suite %d\" Dropping key phase %d' keys updated too quickly' Peer updated keys to %d' Peer confirmed key update to phase %d' received ACK for key phase %d, but peer didn't update keys\" Initiating key update to key phase %d' received %d bytes for the connection, allowed %d bytes' Increasing receive flow control window for the connection to %d kB' flow controller reset after reading data' received inconsistent final offset for stream %d (old: %d, new: %d bytes)' Increasing receive flow control window for stream %d to %d kB' duplicate stream data' 0-RTT rejected' too many open streams' negative packetBuffer refCount\" packetBuffer refCount not zero' putPacketBuffer called with packet of wrong size!' Received %d packets after sending CONNECTION_CLOSE. Retransmitting.\" Error retransmitting CONNECTION_CLOSE: %s' invalid value for Config.MaxIncomingStreams' retired connection ID %d (highest issued: %d)' received conflicting connection IDs for sequence number %d' received conflicting stateless reset tokens for sequence number %d' expected first connection ID to have sequence number 0' expected first connection ID to have sequence number 0' Activating reading of ECN bits for IPv4 and IPv6.' Activating reading of ECN bits for IPv4.' Activating reading of ECN bits for IPv6.\" activating ECN failed for both IPv4 and IPv6' Activating reading of packet info for IPv4 and IPv6.' activating packet info failed for both IPv4 and IPv6' received invalid offset %d on crypto stream, maximum allowed %d' received crypto data after change of encryption level' encryption level changed, but crypto stream has more data to read\" received CRYPTO frame with unexpected encryption level: %s' Discarding DATAGRAM frame (%d bytes payload)' stream %d canceled with error code %d' too many gaps in received data' no gap found' no gap found\" frame sorter BUG: read position higher than a gap' cannot use different stateless reset keys on the same packet conn' cannot use different tracers on the same packet conn' failed to determine receive buffer size: %w' Conn has receive buffer of %d kiB (wanted: at least %d kiB)' failed to increase receive buffer size: %w' failed to determine receive buffer size: %w' failed to increase receive buffer size (wanted: %d kiB, got %d kiB)' Increased receive buffer size to %d kiB' Not adding connection ID %s, as it already exists.\" Adding connection ID %s.' Not adding connection ID %s for a new session, as it already exists.\" Adding connection IDs %s and %s for a new session.' Removing connection ID %s after it has been retired.' Replacing session for connection ID %s with a closed session.\" Removing connection ID %s for a closed session after it has been retired.' Temporary error reading from conn: %w' error parsing connection ID on packet from %s: %s\" received a packet with an unexpected connection ID %s' Received a stateless reset with token %#x. Closing session.' Sending stateless reset to %s (connection ID: %s). Token: %#x' Error sending Stateless Reset: %s' an\\'t determine encryption level' unknown encryption level' unexpected encryption level: %s' PacketPacker BUG: packet too large (%d bytes, allowed %d bytes)' packetPacker BUG: Peeked and Popped packet numbers do not match' unknown packet type: %s' Packet too small. Expected at least 20 bytes after the header, got %d' BUG: readPosInFrame (%d) > frame.DataLen (%d) in stream.Read\" Read on stream %d canceled with error code %d' STREAM frames are handled with their respective streams.' unexpected encryption level: %s' sendQueue.Send would have blocked' numOutStandingFrames negative' close called for canceled stream %d' %s is not a valid QUIC version' Listening for %s connections on %s\" server closed' Dropping packet from %s (%d bytes). Server receive queue full.' Dropping Version Negotiation packet.' Error parsing packet: %s' misrouted packet: %#v' Dropping a packet that is too small to be a valid Initial (%d bytes)\" Dropping long header packet of type %s (%d bytes)' <- Received Initial packet.' Error occurred handling initial packet: %s' too short connection ID' Error sending INVALID_TOKEN error: %s' Error sending Retry: %s\" Error rejecting connection: %s' Changing connection ID to %s.' Changing connection ID to %s.' Client sent an invalid retry token. Sending INVALID_TOKEN to %s.' Client offered version %s, sending Version Negotiation' Error composing Version Negotiation: %s' Error sending Version Negotiation: %s' closing session in order to recreate it\" Sending a keep-alive PING to keep the connection alive.' Connection %s closed.' error parsing packet: %s' Dropping packet with version %x. Expected %x.' coalesced packet has different destination connection ID: %s, expected %s' Parsed a coalesced packet. Part %d: %d bytes. Remaining: %d bytes.\" Dropping %s packet (%d bytes) because we already dropped the keys.' Dropping %s packet (%d bytes) that could not be unpacked. Error: %s\" <- Reading packet %d (%d bytes) for connection %s, %s\" Dropping (potentially) duplicate packet.' Ignoring Retry.' Ignoring Retry, since we already received a packet.' Ignoring Retry, since a Retry was already received.' gnoring spoofed Retry. Integrity Tag doesn\\'t match.' <- Received Retry:' Switching destination connection ID to: %s' Error parsing Version Negotiation packet: %s' Received a Version Negotiation packet. Supported Versions: %s' No compatible QUIC version found.' Switching to QUIC version %s.' empty packet\" Received first packet. Switching destination connection ID to: %s' unexpected PATH_RESPONSE frame' received a HANDSHAKE_DONE frame' DATAGRAM frame too large\" Destroying session: %s' Destroying session with error: %s' Peer closed session with error: %s' Error sending CONNECTION_CLOSE: %s' Restoring Transport Parameters: %s\" Processed Transport Parameters: %s' expected initial_source_connection_id to equal %s, is %s' expected original_destination_connection_id to equal %s, is %s' missing retry_source_connection_id' expected retry_source_connection_id to equal %s, is %s\" received retry_source_connection_id, although no Retry was performed' session BUG: couldn't pack %s probe packet\" session BUG: unspecified error type (msg: %s)' -> Sending coalesced packet (%d parts, %d bytes) for connection %s' -> Sending packet %d (%d bytes) for connection %s, %s\" -> Sending packet %d (%d bytes) for connection %s, %s' shouldn't queue undecryptable packets after handshake completion\" Dropping undecryptable packet (%d bytes). Undecryptable packet queue full.\" deadline exceeded' peer attempted to open receive stream %d' peer attempted to open send stream %d' tried to delete unknown incoming stream %d' tried to delete incoming stream %d multiple times' tried to delete unknown incoming stream %d' tried to delete incoming stream %d multiple times' peer attempted to open stream %d' tried to delete unknown outgoing stream %d' peer attempted to open stream %d' tried to delete unknown outgoing stream %d' RSA PRIVATE KEY' CERTIFICATE' 1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' 127.0.0.1:0' 127.0.0.2:0' conn closed' conn closed\" endpoint i/o timeout' UpdateConnState\" implement me' 6ba7b810-9dad-11d1-80b4-00c04fd430c8' 6ba7b811-9dad-11d1-80b4-00c04fd430c8\" 6ba7b812-9dad-11d1-80b4-00c04fd430c8' 6ba7b814-9dad-11d1-80b4-00c04fd430c8' invalid UUID (got %d bytes)' invalid UUID length: %d' urn:uuid:' invalid urn prefix: %q' invalid UUID format' invalid UUID format' invalid UUID format\" read timeout' write timeout' unreachable' conn closed' TimeDeadline' already closed' 127.0.0.2:' 127.0.0.1:' not found %s' i/o deadline reached' stream closed' unexpected flag' remote end is not accepting connections' keepalive timeout' backlog must be positive' keep-alive interval must be positive' MaxStreamWindowSize must be larger than %d' both Logger and LogOutput may not be set, select one' one of Logger or LogOutput must be set, select one' unhandled state' [ERR] yamux: unexpected FIN flag in state %d' [ERR] yamux: Failed to read stream data: %v' [ERR] yamux: aborted stream open without inflight syn semaphore' [ERR] yamux: aborted stream open (destination=%s): %v' [ERR] yamux: keepalive failed: %v' [ERR] yamux: Failed to write header: %v' reset by peer' [ERR] yamux: Failed to read header: %v\" [ERR] yamux: Invalid protocol version: %d\" [WARN] yamux: Discarding data for stream: %d' [ERR] yamux: Failed to discard data: %v' [WARN] yamux: frame for missing stream: %v' [WARN] yamux: failed to send go away: %v' [WARN] yamux: failed to send go away: %v' [WARN] yamux: failed to send ping reply: %v' [ERR] yamux: received protocol error go away' yamux protocol error' [ERR] yamux: received internal error go away\" remote yamux internal error' [ERR] yamux: received unexpected go away\" unexpected go away received' [ERR] yamux: duplicate stream declared\" [WARN] yamux: failed to send go away: %v' [WARN] yamux: backlog exceeded, forcing connection reset' [ERR] yamux: SYN tracking out of sync' Vsn:%d Type:%d Flags:%d StreamID:%d Length:%d' stream id overflows, should start a new connection\" operation would block on IO' unsupported protocol version' keep-alive interval must be positive' keep-alive timeout must be larger than keep-alive interval\" max frame size must be positive' max frame size must not be larger than 65535' max receive buffer must be positive' max stream buffer must be positive' max stream buffer must not be larger than max receive buffer' max stream buffer cannot be larger than 2147483647\" allocator Put() incorrect buffer size' both channel are nil' ALL_PROXY' all_proxy' connection forbidden' network unreachable' TTL expired' command not supported' websocket: close sent\" websocket: read limit exceeded' websocket: write timeout' websocket: bad write message type' websocket: write closed\" websocket: invalid control frame' websocket: bad handshake' websocket: invalid compression negotiation' malformed ws or wss URL' websocket: internal error, unexpected bytes at end of flate stream' proxy: unknown scheme: ' proxy: no support for SOCKS5 proxy connections of type ' proxy: failed to parse port number: ' proxy: port number out of range: ' proxy: failed to write greeting to SOCKS5 proxy at ' proxy: failed to read greeting from SOCKS5 proxy at ' proxy: SOCKS5 proxy at ' has unexpected version ' proxy: SOCKS5 proxy at ' requires authentication' proxy: failed to write authentication request to SOCKS5 proxy at ' proxy: failed to read authentication reply from SOCKS5 proxy at ' proxy: SOCKS5 proxy at \" rejected username/password' proxy: destination host name too long: ' proxy: failed to write connect request to SOCKS5 proxy at \" proxy: failed to read connect reply from SOCKS5 proxy at \" unknown error' proxy: SOCKS5 proxy at ' failed to connect: ' proxy: failed to read domain length from SOCKS5 proxy at ' proxy: got unknown address type ' from SOCKS5 proxy at ' proxy: failed to read address from SOCKS5 proxy at ' Sec-Websocket-Extensions' websocket: close ' (normal)' (going away)' (protocol error)' (unsupported data)\" (no status)' (abnormal closure)\" (invalid payload data)' (policy violation)' (message too big)' (mandatory extension missing)' (internal server error)' (TLS handshake error)' websocket: internal error, extra used in client mode' concurrent write to websocket connection\" concurrent write to websocket connection' unexpected reserved bits 0x' message start before final message frame\" continuation after final message frame' unknown opcode ' incorrect mask flag\" invalid close code' invalid utf8 payload in close frame' websocket: \" repeated read on failed websocket connection' websocket: internal error, unexpected text or binary in Reader' websocket' Connection' Sec-WebSocket-Key' Sec-WebSocket-Version' Sec-WebSocket-Protocol' Connection' Sec-Websocket-Key' Sec-Websocket-Version' Sec-Websocket-Extensions' Sec-Websocket-Protocol' websocket: duplicate header not allowed: \" Sec-Websocket-Protocol\" Sec-WebSocket-Protocol' Sec-WebSocket-Extensions' permessage-deflate; server_no_context_takeover; client_no_context_takeover\" websocket' Connection' Sec-Websocket-Accept\" permessage-deflate' Proxy-Authorization' socks connect' socks bind' succeeded' general SOCKS server failure' connection not allowed by ruleset' network unreachable' TTL expired' command not supported' nil context' nil context' network not implemented' command not implemented' username/password authentication failed' unsupported authentication method ' too many authentication methods' unexpected protocol version ' no acceptable authentication methods\" unknown address type' FQDN too long' unexpected protocol version ' unknown error ' non-zero reserved field' unknown address type ' ALL_PROXY' all_proxy' failed to dial: %s' User-Agent' User-Agent' failed to dial %s:%s, Err:%s\" method is not allowed' no matching route was found' mux: duplicated route variable %q' mux: path must start with a slash, got %q' mux: missing name or pattern in %q' %s(?P<%s>%s)' mux: unbalanced braces in %q' mux: unbalanced braces in %q' nil pointer to error encoder' /WindowsDriver/' invalid range: failed to overlap\" 127.0.0.1:0\" Content-Type' text/html; charset=utf-8' ailed: %s\\n\" uploadFile\" ailed: %s\\n' rror: %s\\n' re>up over\\n%s sha256sum: %x\\n</pre>' <a style=\\'text-decoration: none;\\' href=\"%s/\">%s</a></p>\\n' Error reading directory' Content-Type\" text/html; charset=utf-8' style=\\'text-decoration: none;\\'  href=\"%s\"><h5>..</h5></a>\\n' a style='text-decoration: none;'><h5>now at: %s</h5></a>\\n\" otal %d\\n' Content-Type' seeker can't seek\" Content-Type' Content-Range' bytes */%d' Content-Range' Content-Type' multipart/byteranges; boundary=\" Accept-Ranges' If-Unmodified-Since\" If-None-Match' If-Modified-Since' Last-Modified' Content-Type\" Content-Length' Last-Modified\" /index.html' Last-Modified' 404 page not found' 403 Forbidden' bytes %d-%d/%d' Content-Range' Content-Type' invalid range\" invalid range' invalid range' invalid range\" invalid range' unsupported' /dev/ptmx' /dev/pts/' wss://natsupport[.]net/api' /opt/vmware/vpostgres/current/bin/pg_update' /usr/sbin' rpclistener' /usr/sbin/rpclistener' exit status 121\" f\\xff' (empty)' %x' Initial' 0' 1' Server' Client' Initial' ' ' ' cpu.\" \\n' on' \\n' \\n' avx512' avx512f\" bmi1' bmi2' erms' popcnt' rdrand' rdseed\" sse3' )' *' .' derived' derived' .' derived' derived' tls13 ' : ' ->' , ' t%s %#v\" ENOENT' ENOEXEC' ECHILD' EAGAIN' ENOMEM' EACCES\" EFAULT' ENOTBLK' EEXIST' ENOTDIR\" EISDIR' EINVAL' EMFILE' ENOTTY' ETXTBSY' ENOSPC' ESPIPE' EMLINK' EDOM' ERANGE' EDEADLK' ENOLCK' ENOSYS' ENOMSG' ECHRNG' EL3HLT' EL3RST' ELNRNG' EUNATCH' ENOCSI' ENOANO\" EBADSLT' EBFONT' ENOSTR' ENODATA' ENONET' ENOPKG' EREMOTE' EADV' ESRMNT\" EPROTO' EDOTDOT' EBADMSG' EREMCHG' ELIBACC' ELIBBAD' ELIBSCN' ELIBMAX' EILSEQ' EUSERS' ENOBUFS' EUCLEAN' ENOTNAM' ENAVAIL' EISNAM' ENOKEY' ERFKILL' SIGHUP' hangup' SIGINT' SIGQUIT' quit' SIGILL' SIGTRAP' SIGABRT\" aborted' SIGBUS' SIGFPE' SIGKILL' killed' SIGUSR1' SIGSEGV\" SIGUSR2' SIGPIPE' SIGALRM\" SIGTERM' SIGCHLD' SIGCONT' SIGSTOP' stopped' SIGTTIN' SIGTTOU' SIGURG' SIGXCPU' SIGXFSZ' SIGPROF' SIGPWR' SIGSYS' solaris' netbsd' openbsd' /' recvmsg' android' illumos' windows' tcp6' udp6' /' /' read' none' quic hp' quic hp' tls13 ' quic ku' ' ' server' init' suspend' none' null' server' cc|)' exit' Remove' closed' timeout' \\xff' socks5' tcp4' : ' : ' : ' : ' : ' : ' : ' : ' \"' ;' =' : ' :' ]' http' Upgrade' 13' , ' Host' Upgrade' upgrade' http' :' Basic ' CONNECT\" ' socks ' tcp4' Origin' smux' http' socks5' /' /' /' /' :' [/]?' =' :' =' =' &;' v' /' /' /' &' &lt;' >' &gt;' \"' %v' %v' /' Post' /api' windows' /' /' pre>\\n' /' /pre>\\n' %.2fMB' %.2fGB' %.2fTB' %.2fEB' HEAD' W/' W/' W/' Etag' Etag' Etag' Etag' ./' /' /' /' /' bytes=' ,' -' nil EOF' /' # ' ' exit' cd' n' nil EOF' /bin/sh' -c' nil EOF' ICMP' n' ' ' /' :' SETENV1' SETENV2' true' TERM' USER' PATH' TERM' USER' true' PATH' IOCs wss://natsupport[.]net/api References 1: https://cloud.google.com/blog/topics/threat-intelligence/brickstorm-espionage-campaign 2: https://cloud.google.com/blog/topics/threat-intelligence/gostringungarbler-deobfuscating-strings-in-garbled-binaries 3: https://research.openanalysis.net/garble/go/obfuscation/strings/2023/08/03/garble.html 4: https://github.com/burrowers/garble Decoding Brickstorms Garble strings was originally published in Walmart Global Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "published_ts": 1764880743,
      "source_name": "Walmart Global Tech",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/building-ai-ready-enterprise-leaders-share-real-world-ai-solutions-and-practices",
      "title": "Building the AI-Ready Enterprise: Leaders Share Real-World AI Solutions and Practices",
      "summary": "AI adoption is progressing at a faster pace than any previous technology cycle, and...",
      "published_ts": 1764878400,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://medium.com/pinterest-engineering/on-the-re-prioritization-of-open-source-ai-86f7279481e3?source=rss----4c5a5f6279b6---4",
      "title": "On the (re)-prioritization of open-source AI",
      "summary": "Dmitry Kislyuk | Director, Machine Learning; Ryan Galgon | Director, Product Management; Chuck Rosenberg | Vice President, Engineering; Matt Madrigal | Chief Technology Officer Foreword from Bill Ready, CEO The AI landscape is undergoing a fundamental shift, and it’s not the one you think. The competitive frontier isn’t only about building the largest proprietary models. There are two other major trends emerging that haven’t had enough discussion: Open-source models have made tremendous strides, especially on cost relative to performance. Compact, fit-for-purpose models can meaningfully out-perform general purpose LLMs on specific tasks and do so at dramatically lower cost. Our Chief Technology Officer and AI team share how we are using open-source AI models at Pinterest to achieve similar performance at less than 10% of the cost of leading, proprietary AI models. They also share how Pinterest has built in-house, fit-for-purpose models that are able to significantly outperform leading, proprietary general purpose models. The race to build the largest, most powerful models is profound and meaningful. If you want to see a thriving ecosystem of innovation in an AI-driven world, you should also want to see a thriving open-source AI community that creates democratization and transparency. It’s a good thing for us all that open source is in the race. For our part, we’ll continue to share our findings in leveraging open-source AI so that more companies and builders can benefit from the democratizing effect of open-source AI. Pinterest helps users worldwide to search, save, and shop for the best ideas powered by our visual AI capabilities. These are powered by a mix of models operating across different modalities; a recent development is that for applications requiring LLMs and VLMs¹, we have found significant advantages in adapting open-source models with Pinterest’s unique data and existing technologies. As a result, Pinterest has been shifting more of our AI investments towards fine-tuned open-source models, achieving similar quality at a fraction of the cost, particularly for visual and multimodal tasks. This shift reflects a broader industry trend: core LLM architectures are commoditizing, while differentiation increasingly comes from domain-specific data, personalization, and product integration. It is worth taking a closer look at the technical strategy behind foundation models at Pinterest. Just because it can be built in-house does not mean every capability should or must be. The build, buy, adapt set of tradeoffs are a well understood concept in the industry, and AI models are no different. At Pinterest, we structure our thinking about this question by looking at the primary modality over which the foundation model is optimized for: Users (recommendation systems). User modeling systems are typically deeply coupled with the specific product that they are optimized for and are thus nearly always built internally for large search and social platforms. Given its scale, Pinterest has published extensive work on utilizing long-term sequences of user actions and universally compatible user representations for recommendation systems, relying on an image-board-user graph consisting of hundreds of billions of nodes to build these capabilities. These systems include both representation learning approaches (e.g. PinFM ) and generative recommendation models (e.g. PinRec ). Visual (encoders and diffusion models)². As a visual platform, Pinterest has consistently invested in building frontier image understanding models. Although strong open-source models are available, we have found that the rich visual datasets curated through our visual search product and visual board collections enable the large-scale weakly-supervised pretraining that modern visual AI systems require. Thus, we largely default to training these models internally from scratch as well. Text (LLMs)³. The remarkable model progress on text modeling and more abstract capabilities (e.g. reasoning) has been empirically connected to training with enormous amounts of compute and internet-scale text data, and Pinterest has largely relied on both open-source and third-party proprietary LLMs to build the best possible products for our users in recent years. What we are observing now is that the capabilities of open-source multimodal LLM architectures have begun to level the playing field of model capabilities. Critically, across many product categories at Pinterest, the core differentiation in capabilities is shifting to the ability to fine-tune models with domain-specific data, and investing in end-to-end optimization and integration. The trend toward domain-specific data and deep product integration as a core differentiator can be seen as a reversion to a common trend in the ML industry. In the first decade of the AlexNet era, core architectures were routinely commoditized, and either fine-tuning open-source models or training models on specific web-scale datasets was the most common form of development. We saw this first-hand with our development of various visual encoders (e.g. UVE , PinCLIP ), where training embedding models from scratch on Pinterest image and visual search data has yielded meaningful retrieval gains over off-the-shelf embedding models⁴. Recently, we’ve also seen this with Pinterest Canvas , our image generation foundation model, where tuning an internally-trained diffusion model for specific image editing and enhancement use-cases with Pinterest data has thus far yielded better results than using larger but more general-purpose visual generative models. Our most recent data point in this trend comes from the beta launch of Pinterest Assistant in October of this year. We can think of the Pinterest Assistant as being broken down into two sets of ML technologies. First, there is an underlying set of multimodal retrieval systems, recommendation services, and specialized generative models (including other LLMs) that serve as tools for an agentic LLM to invoke. These tools are predominantly Pinterest-native and rely on our user and visual foundation models. And second, there is the core multimodal LLM itself, which oversees the agentic loop and is responsible for query understanding, query planning, and effective tool calling. The key factor with this LLM is that it acts as an intelligent router that recursively delegates much of the recommendation and agentic capabilities to the aforementioned Pinterest-native tools. In this design, the biggest lever we have for product improvements is scaling the quality of the tools, and scaling test-time-compute (e.g. breaking down the call into more advanced steps), as opposed to focusing solely on using the largest core LLM possible. Indeed, as comparisons of LLMs start showing small or negligible differences, we have observed that open-source solutions meet our product needs; we’re getting more value by focusing on building out more domain-specific tools, fine-tuning for product-specific use-cases, optimizing for latency, etc. There are some benefits we are particularly excited about as we adopt more open-source multimodal LLMs at Pinterest: Cost . Pinterest’s visual-first AI systems have heavy image understanding requirements, often dealing with 10s of images in each conversation turn. In these situations, we are currently observing that self-hosted, fine-tuned open-source models allow for an order of magnitude reduction in inference costs. With further engineering investment in inference optimizations like disaggregated serving and smarter cache routing, we expect that the cost and throughput via internal model hosting will become even more favorable. Personalization . Personalization in a proprietary LLM can typically occur by passing in relevant context about a user as text, and there are many situations where this is sufficient. However, the visual nature of Pinterest means that user representations encode many more stylistic and visual preferences that do not translate well to text, and the ability to fine-tune an LLM to natively interpolate with internal embeddings, session context, and other user signals is highly desirable to produce relevant results. Capabilities . Beyond personalization, leveraging precomputed internal content embeddings (e.g. PinCLIP) projected to internal LLMs allows for more efficient computation of long visual contexts. For Pinterest product development, this is an important consideration, since board and collage objects are a natural target for conversations, and they may consist of dozens or hundreds of Pins. On the output side, another capability that we have already seen progress on is the ability to fine-tune for specific tool calling (e.g. refining multimodal queries), especially in ways that enable future extensibility to novel tools without needing end-to-end fine-tuning of all capabilities jointly. Brand Values . There are significant advantages in being able to tune and update a model directly to align with Pinterest’s investments in Inclusive AI commitments and our Community Guidelines, as opposed to layering this on as an additional module on top of a third party API call. Looking ahead, ML and AI capabilities at Pinterest will continue to be powered by a mix of internally-developed foundation models, fine-tuned open-source models, and licensed third party models. In addition, third party AI platforms are widely used by Pinterest engineering teams for coding tools, internal productivity, and rapid prototyping. However, the scalability advantages and capability gains from all forms of internally hosted models, whether they are trained from scratch or fine-tuned, are leading to a change in technology defaults at Pinterest. Furthermore, the development of model families that provide generative capabilities across a variety of latency and throughput requirements have allowed for a development pattern where product teams can prototype and iterate with third party models, while the ML teams develop more scalable and personalized internal models for the relevant capability. How long will this open-source trend hold? We can only make an educated guess. The large-scale buildout of AI data centers may result in more step-function jumps in quality and emergent capabilities for proprietary models. In parallel, the supply-side growth in chip production may drive down fine-tuned open-source inference costs even further. Either way, our strategy at Pinterest to bring inspiration to all of our users will remain the same: leverage our visual, graph, and recommendation data to build the best and most efficient models we can, and address any capability gaps by partnering with third-party providers, alongside regular research & development from Pinterest Labs . ¹We use “LLM” to refer to both text-only models, and multimodal visual LLMs, which contain an image encoder, which are sometimes referred to as Visual Language Models (VLMs). Most applications of generative models at Pinterest require visual inputs, so internally we assume multimodal capabilities as a default. ²Visual models are commonly trained with text supervision via contrastive learning or other forms of conditioning. But the dominant training signal for the model remains the raw visual input. ³Most LLMs benefit from a mix of modalities, with VLMs designed explicitly for this purpose. However, pre-training remains focused on an autoregressive text token prediction task, which is why we characterize them as text-dominant models. ⁴For example, we have seen our PinCLIP system outperform state of the art open-source multimodal embeddings by more than 30% on core retrieval tasks. On the (re)-prioritization of open-source AI was originally published in Pinterest Engineering Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "published_ts": 1764867735,
      "source_name": "Pinterest Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/european-air-transport",
      "title": "How European Air Transport Built AI Agents for 80x Operational Efficiency",
      "summary": "With AI agents in Dataiku, European Air Transport automated document processing to achieve near real-time reporting that informs planning and critical decision-making. 80x more productive; 40–50 hours of manual work reduced to 30 minutes 1000s of documents processed automatically Reduced data latency for daily reports from days to just minutes",
      "published_ts": 1764853202,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/intel-deepmath",
      "title": "DeepMath: A lightweight math reasoning Agent with smolagents",
      "summary": "DeepMath: A lightweight math reasoning Agent with smolagents\nBy Intel AI Software Group\nDeepMath\nis an aligned math reasoning agent built on\nQwen3-4B Thinking\nand fine-tuned with\nGRPO (Group Relative Policy Optimization)\n. Instead of verbose text, the model emits\ntiny Python snippets\nfor intermediat",
      "published_ts": 1764806400,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/hf-skills-training",
      "title": "We Got Claude to Fine-Tune an Open Source LLM",
      "summary": "We Got Claude to Fine-Tune an Open Source LLM\nWe gave Claude the ability to fine-tune language models using a new tool called\nHugging Face Skills\n. Not just write training scripts, but to actually submit jobs to cloud GPUs, monitor progress, and push finished models to the Hugging Face Hub. This tut",
      "published_ts": 1764806400,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.langchain.com/evaluating-deep-agents-our-learnings/",
      "title": "Evaluating Deep Agents: Our Learnings",
      "summary": "Over the past month at LangChain, we shipped four applications on top of the Deep Agents harness: DeepAgents CLI : a coding agent LangSmith Assist: an in-app agent to help with various things in LangSmith Personal Email Assistant: an email assistant that learns from interactions with each user Agent Builder : a",
      "published_ts": 1764783857,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/improve-model-accuracy-with-reinforcement-fine-tuning-in-amazon-bedrock/",
      "title": "Amazon Bedrock adds reinforcement ﬁne-tuning simplifying how developers build smarter, more accurate AI models",
      "summary": "Amazon Bedrock now supports reinforcement fine-tuning delivering 66% accuracy gains on average over base models.",
      "published_ts": 1764778094,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/a-modern-approach-to-analytics-powered-by-dataiku-and-databricks",
      "title": "A Modern Approach to Analytics, Powered by Dataiku and Databricks",
      "summary": "Data and IT leaders are under increasing pressure to accelerate the delivery of AI results while  maintaining airtight governance . Data volumes are exploding, models are more complex, and business teams expect real-time, self-service capabilities. Traditional, siloed analytics systems built for batch processing and static reporting can no longer support the speed and scale of today’s enterprise.",
      "published_ts": 1764768023,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/how-agentforce-achieved-3-5x-faster-response-times-while-solving-enterprise-scale-architectural-complexity/",
      "title": "How Agentforce Achieved 3–5x Faster Response Times While Solving Enterprise-Scale Architectural Complexity",
      "summary": "By Kunal Pal and Krista Hardebeck. In our Engineering Energizers Q&A series, we showcase the engineering minds driving innovation across Salesforce. Today, we introduce Krista Hardebeck, Regional Vice President of Forward Deployed Engineering (FDE), whose team collaborated with a large multi-brand specialty retailer to launch their initial production Agentforce service experience on an accelerated timeline. […] The post How Agentforce Achieved 3–5x Faster Response Times While Solving Enterprise-Scale Architectural Complexity appeared first on Salesforce Engineering Blog .",
      "published_ts": 1764735320,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/nvidia/custom-policy-reasoning-nemotron-content-safety",
      "title": "Custom Policy Enforcement with Reasoning: Faster, Safer AI Applications",
      "summary": "Custom Policy Enforcement with Reasoning: Faster, Safer AI Applications\nMost safety models enforce a single, generalized policy that blocks obviously harmful content,  toxicity, and jailbreak attempts. That works for broad categories, but real-world applications demand more. Generic content safety m",
      "published_ts": 1764701428,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.langchain.com/langsmith-agent-builder-now-in-public-beta/",
      "title": "LangSmith Agent Builder now in Public Beta",
      "summary": "Now anyone can create production ready agents without writing code, just chat.\nAgent Builder guides you from initial idea to deployed agent, creating detailed prompts, selecting required tools, and even creating subagents.",
      "published_ts": 1764693039,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-bedrock-agentcore-adds-quality-evaluations-and-policy-controls-for-deploying-trusted-ai-agents/",
      "title": "Amazon Bedrock AgentCore adds quality evaluations and policy controls for deploying trusted AI agents",
      "summary": "Deploy AI agents with confidence using new quality evaluations and policy controls—enabling precise boundaries on agent actions, continuous quality monitoring, and experience-based learning while maintaining natural conversation flows.",
      "published_ts": 1764692076,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-bedrock-adds-fully-managed-open-weight-models/",
      "title": "Amazon Bedrock adds 18 fully managed open weight models, including the new Mistral Large 3 and Ministral 3 models",
      "summary": "Access fully managed foundation models from leading providers like Google, Kimi AI, MiniMax AI, Mistral AI, NVIDIA, OpenAI, and Qwen, including the new Mistral Large 3 and Ministral 3 3B, 8B, and 14B models through Amazon Bedrock.",
      "published_ts": 1764691557,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/accelerate-ai-development-using-amazon-sagemaker-ai-with-serverless-mlflow/",
      "title": "Accelerate AI development using Amazon SageMaker AI with serverless MLflow",
      "summary": "Simplify AI experimentation with zero-infrastructure MLflow that launches in minutes, scales automatically, and seamlessly integrates with SageMaker's model customization and pipeline capabilities.",
      "published_ts": 1764691376,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-amazon-nova-2-lite-a-fast-cost-effective-reasoning-model/",
      "title": "Introducing Amazon Nova 2 Lite, a fast, cost-effective reasoning model",
      "summary": "New fast, cost-effective model supports extended thinking with adjustable reasoning depth, letting you control the balance between speed, intelligence, and cost while building AI applications for everyday workloads.",
      "published_ts": 1764691180,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-amazon-nova-forge-build-your-own-frontier-models-using-nova/",
      "title": "Introducing Amazon Nova Forge: Build your own frontier models using Nova",
      "summary": "New program gives organizations unprecedented access to Nova model training, enabling them to build custom frontier models that deeply embed domain expertise without the traditional barriers of cost, compute, and time.",
      "published_ts": 1764690831,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.octo.com/la-school-of-product-2025-talk-d'olivier-girardot-construire-un-compagnon-ia-pour-les-hopitaux",
      "title": "La School of Product 2025 - Talk d'Olivier Girardot - Construire un compagnon IA pour les hôpitaux",
      "summary": "Dans ce contexte où l’IA est poussé partout et tout le temps, la conférence d’Olivier Girardot  rappelle que l’IA en santé doit rester utile, responsable et digne de confiance. Elle doit libérer du temps aux médecins tout en garantissant la souveraineté des données, malgré une maturité produit encore fragile et un marché en pleine effervescence.",
      "published_ts": 1764672430,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/databricks-and-nvidia-powering-next-generation-industry-ai",
      "title": "Databricks and NVIDIA: Powering the Next Generation of Industry AI",
      "summary": "Industry Use Case Transformation with AIAs we head to Las Vegas for Amazon Web Services (AWS) re:Invent...",
      "published_ts": 1764628200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.rudderstack.com/blog/assisted-ecommerce-ai-personal-shopping",
      "title": "Assisted eCommerce: Is this the future of retail?",
      "summary": "Discover how assisted eCommerce, human stylists, and AI agents use customer data infrastructure to boost conversion, LTV, and personalized shopping experiences.",
      "published_ts": 1764627801,
      "source_name": "Rudderstack Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/genesis-workbench-blueprint-life-sciences-applications-databricks",
      "title": "Genesis Workbench: A Blueprint for Life Sciences Applications on Databricks",
      "summary": "AI is Accelerating Target Discovery and Transforming Drug DesignThe life sciences...",
      "published_ts": 1764610476,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://slack.engineering/streamlining-security-investigations-with-agents/",
      "title": "Streamlining Security Investigations with Agents",
      "summary": "Slack’s Security Engineering team is responsible for protecting Slack’s core infrastructure and services. Our security event ingestion pipeline handles billions of events per day from a diverse array of data sources. Reviewing alerts produced by our security detection system is our primary responsibility during on-call shifts. We’re going to show you how we’re using AI…",
      "published_ts": 1764604842,
      "source_name": "Slack Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/how-ai-driven-refactoring-cut-a-2-year-legacy-code-migration-to-4-months/",
      "title": "How AI-Driven Refactoring Cut a 2-Year Legacy Code Migration to 4 Months",
      "summary": "By Ishay Dahan, Lilach Nachmias, and Adi Vaknin. In our Engineering Energizers Q&A series, we highlight the engineering minds driving innovation across Salesforce. Today, we meet Lilach Nachmias, Senior Manager of Software Engineering, who helped migrate the Own Archive managed package, a seven-year-old third-party application, into Salesforce’s Core infrastructure. Own is a company Salesforce acquired […] The post How AI-Driven Refactoring Cut a 2-Year Legacy Code Migration to 4 Months appeared first on Salesforce Engineering Blog .",
      "published_ts": 1764601922,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/hugging-science/sarlo-80-sar-optic-language-dataset",
      "title": "SARLO-80: Worldwide Slant SAR Language Optic Dataset at 80 cm Resolution",
      "summary": "SARLO-80: Worldwide Slant SAR Language Optic Dataset at 80 cm Resolution\nAuthors: Solène Debuysère\n1\n, Nicolas Trouvé\n1\n, Nathan Letheule\n1\n, Elise Colin\n1\n, Georgia Channing\n2\nAffiliations:\n1\nONERA – The French Aerospace Lab\n2\nHugging Face\nSatellite imagery has transformed the way we observe our pl",
      "published_ts": 1764583838,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/transformers-v5",
      "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
      "summary": "Transformers v5: Simple model definitions powering the AI ecosystem\nTransformers' version v4.0.0rc-1, the initial release candidate for version 4, was released on November 19th, 2020. Five years later, we now release v5.0.0rc-0.\nToday, as we launch v5, Transformers is installed more than\n3 million t",
      "published_ts": 1764547200,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    }
  ],
  "cloud_infra_observability": [
    {
      "url": "https://blog.cloudflare.com/5-december-2025-outage/",
      "title": "Cloudflare outage on December 5, 2025",
      "summary": "Cloudflare experienced a significant traffic outage on  December 5, 2025, starting approximately at 8:47 UTC. The incident lasted approximately 25 minutes before resolution. We are sorry for the impact that it caused to our customers and the Internet. The incident was not caused by an attack and was due to configuration changes being applied to attempt to mitigate a recent industry-wide vulnerability impacting React Server Components.",
      "published_ts": 1764892800,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://medium.com/pinterest-engineering/autonomous-observability-at-pinterest-part-1-of-2-eb0adae830ba?source=rss----4c5a5f6279b6---4",
      "title": "Autonomous Observability at Pinterest (Part 1 of 2)",
      "summary": "Marcel Mateos Salles | Software Engineer Intern; Jorge Chavez | Sr. Software Engineer; Khashayar Kamran | Software Engineer II; Andres Almeida | Software Engineer; Peter Kim | Manager II; Ajay Jha | Sr. Manager At Pinterest, inspiration isn’t just for our users — it shapes how we build and care for our platform. Until recently, our own observability (o11y) tools told a fragmented story: logs over here, traces over there, and metrics somewhere else. We’ve always excelled at collecting signals: time-series metrics, traces, logs, and change related events. But without the seamless context and unity now promised by open standards like OpenTelemetry (OTel), we were missing out on the “big picture”: the full narrative behind every anomaly and alert. While modern observability standards like OpenTelemetry (OTel) promise a unified world of correlated data, the reality for many mature, large-scale infrastructures is far more fragmented. These systems, often predating the widespread adoption of such standards, are composed of powerful but disconnected data silos. We solved the problem with a pragmatic solution to this common challenge by leveraging AI agents through a centralized Model Context Protocol (MCP) server to bridge these gaps without mandating a complete infrastructure overhaul. The Pinterest Observability team is charting a new course that meets the moment. We’re working both left and right: “shift-left” practices to bake better logging and instrumentation into the heart of our code, and “shift-right” strategies to keep production observability robust and responsive. Still, we know that tools alone aren’t enough. The real breakthrough comes with bringing more intelligence and context into the mix. We are embracing the new era of AI, and at its core is the Model Context Protocol (MCP), Agent2Agent(A2A) and context engineering, a new way to bring all our observability signals together and feed them into intelligent agents. Beginning with the MCP server, we attempt to make every major pillar of observability data available in a unified, contextual stream. Observability analysis systems can dig deep, asking the right questions. Following clues across logs, metrics, traces, and change events, and iteratively building insight much like a Pinterest board comes together, piece by piece. The result? Faster, clearer root-cause analysis, and actionable guidance for our engineers, right where they need it. This isn’t just about connecting yesterday’s silos, it’s about creating new frontiers for discovery and problem-solving, empowering every Pinterest team to build their own context-aware tools and shape observability that grows with us. A Fragmented State The field of observability (o11y) faces major turning points every couple of years, with a major shift a couple of years back when OpenTelemetry (OTel) and similar services came into the picture. These tools facilitate the o11y process by enabling context propagation across the different pillars of o11y data while remaining vendor and language agnostic. For example, under a single SDK, you have the ability to generate metrics, logs, and traces with an ID that allows for correlation and connections between those unique data pillars. However, our o11y infrastructure was set up before conventions and tools like OTel were available, and it is not feasible to overturn our entire o11y infrastructure in order to incorporate them into our stack. This means that we suffer from a lack of the virtues they provide. We had to individually implement separate tools and pipelines for ingesting logs, metrics, and traces from our services. This resulted in a strong, yet fragmented system where each individual pillar is constrained to its own domain with no clear matching across datapoints. As a result, an on-call engineer must jump around multiple unique interfaces when root causing an issue, leading to the potential loss of valuable time. A steep learning curve for the current tools unique to each pillar further extends this loss of time for newer engineers. Consequently, advanced o11y analysis by leveraging machine learning or other techniques that can holistically understand the health of our systems creates non-trivial problems for the o11y team. Figure 1: Fragmented Signals for Observability Sidestepping the Problem Knowing these limitations, the o11y team here at Pinterest is committed to overcoming these gaps by what we call “shifting-left” and “shifting-right.” When “shifting-left,” we have prioritized the integration and standardization of o11y practices and tools, which facilitates the proactive identification and resolution of issues. Meanwhile, when “shifting-right,” we focus on maintaining system visibility in production through the use of our alerting and health inferencing systems. This means that we have to continue to innovate and connect the dots across our pillars while ensuring teams can continue to monitor the health of their services and quickly solve problems when they arise. Enter the era of AI and Agents. What if our limitations didn’t truly matter? We could just provide our data to Large Language Models (LLMs) acting as agents and have them connect the dots for us, find correlations, return meaningful information to our users in a single interface, facilitate the root-causing process, and in the future lead to a system where we can autonomously solve issues as they arise. We are working towards that future and are excited to share work we have taken up in that regard. Context Engineering An AI agent is only as good as the information that it has access to, so we knew that we had to build a system that would be able to provide our o11y agents with as much relevant data as possible. LLMs are impressive on their own, but with some real context engineering behind them, they become so impressive that you begin to feel like you are living in the advanced future from your favorite Sci-Fi movies and shows. Different techniques have sprung up recently to facilitate the sharing of context with an agent. However, the most prominent and widely accepted is that of the Model Context Protocol (MCP), which was released by Anthropic in late 2024. This protocol has become the new standard and a staple of agentic projects for companies and enthusiasts alike. In short, it provides an agent different tools that it can utilize when working to resolve a request, allowing it the flexibility to choose what to use (if it wants to call anything at all) as it organically works through a task with its reasoning and newfound information. MCP was the perfect fit to help us sidestep our limitations and begin to drive Pinterest o11y into a new era as it grants the following: Unity of Disparate Signals: By building an MCP server, we can empower an agent to simultaneously interact with time-series metrics, logs, traces, changefeed events (deployments, experiments, etc.), alerts, and more. This allows it to find connections and build hypotheses from patterns it sees within and across our data despite the lack of a thread connecting it together. Fine-Grained Context Control: As the developers of the MCP server, we get to decide what information and ability agents interacting with our team’s data actually get, so developing the MCP server ensures that we maintain full control of our services and data. If not, other teams could have independently developed them on their own, incorrectly accessing our data or giving their agents the ability to alter data that should not be changed. By providing the MCP server as an interface to our data, we prevent the agents access to everything, allowing us to maintain tighter safety and privacy controls. Furthermore, we can also guide agents in the right direction, providing relevant subsets of data by combining what we know with what the agent has learned. Plug-and-Play Extensibility: In a practical sense, an MCP server is a service that provides an AI agent with a toolbox to better fulfill its job. The tools within can easily be removed, replaced, and expanded upon without changing the overall system. The agent will connect and interact with it the same, only changing what it can achieve and discover with the tools provided. This means that our server can easily grow and change with our team, becoming more advanced as we provide more tools over time. Hub for Agentic o11y Experience: We plan for this to only be the beginning of our team’s GenAI tooling. It creates the perfect infrastructure to allow for advanced agents and creates a hub for engineering teams at Pinterest to be able to access our data for their own agentic needs (recently, we hosted our company-wide hackathon where multiple teams developed projects that depended on our MCP server, including the team that bagged the first place!). Figure 2: Before and After experience with Tricorder Agent with MCP Server tools MCP Server For Observability And so, the o11y team’s very own MCP server was born. It is now available internally for Pinterest engineers to use and is a central part of our move towards autonomous o11y. Currently, it provides models with tooling for accessing the following data: ChangeFeed Events: Finds events related to the service of interest; for instance: deploys, feature flag activations, or experiment rollouts Metrics: Queries metrics from our time series database Post Mortem Documents : After having previously ingested incident Post-Mortem documents into a database, fetches them for analysis when relevant Logs : Fetches logs related to the service of interest for a relevant time range Traces : Fetches traces related to the service of interest for a relevant time range Alert Information: From a triggered alert, fetches information such as relevant metrics, service identifiers, and time range of interest Dependency Graphs : Finds dependencies for a service of interest, both downstream and upstream Its development was a great experience and allowed us to learn a very important lesson about applied AI. It is partially a consequence of our data but something that anyone who wants to do something with agents should consider as a limitation: the model context size. Going in, we overestimated the amount of information that a model could take while also underestimating the amount of data that we own as a team. The o11y team processes around 3 billion data points per minute, 12 billion keys (tag key/value combinations) per minute, 7 TB of logs per day, and 7 TB of traces per day — no small amount of data! If we allow an agent to organically look through this data, it would end up querying for too much at a time (even if it was only querying for a 15 min window), breaking its context window and causing itself to crash. We came up with two main solutions to prevent this from happening, the first being short-term while we test the other: Link Generation: The first use case we planned to test our agents on was the collection of relevant data for an on-call engineer. We just wanted it to collect the relevant information related to an alert to facilitate the engineers job, streamlining resolution and reducing mean time to resolution (MTTR). For this use case, the agent does not need to parse the raw data. Instead, it only needs to know of relevant time periods and services. This allowed us to have the agent generate links to the dashboards containing that information (already filtered to the correct time periods and relevant services), saving an on-call engineer the time spent jumping around all our interfaces and filtering. All their time can now be spent on the act of resolving conflicts. More Specific Tool Documentation: Knowing the previous solution would not work for all use cases, especially the most advanced ones where we would want agents to be able to find connections between the data, we kept looking for solutions. We noticed that we were overcomplicating the situation trying to come up with complex solutions. The tools a MCP server gives an agent come with a lot of metadata explaining their functionality so that an agent can reason about whether to use it. This means that in this metadata, we could include instructions to only query for a very small period and to call it again until the wanted time period is covered. We are also currently working on and testing another solution with the Spark team within Pinterest. They are looking at building a similar agent, where we leverage an additional LLM within the server (with a fresh context) to summarize the data. This allows us to only return a summary to the agent connected to the MCP server which, in theory, would conserve a lot of context space. We just need to verify that these summaries don’t drastically decrease the agent’s performance. Our MCP Server agent is called Tricorder Agent. It is designed to assist engineers in quickly analyzing problems and resolving conflicts. The agent is part of a broader suite of new tools under development by the o11y team, collectively known as the Tricorder. The engineer can provide the Tricorder with their alert link/number and sit back while it gathers the relevant information for their investigation. Before, this would have been extremely time consuming as engineers would’ve had to switch between all our interfaces and apply filters to find relevant data. Additionally, Tricorder queries our services directly to understand what is going on and hypothesize a cause, providing suggestions and next steps as it gains more information. Throughout this process, there have been many times where we have been pleasantly surprised by the Tricorder. For example, a lot of information is unlocked when a dependency graph becomes available. The agents use tools on multiple parts of the graph, exploring all the incoming and outgoing dependencies to check for the overall health of connections with no specific prompting to do so. Additionally, when generating links and narrowing down to relevant services, they include the services in the dependency graph, knowing the problems could be stemming from them. Autonomous Observability at Pinterest (Part 1 of 2) was originally published in Pinterest Engineering Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "published_ts": 1764781335,
      "source_name": "Pinterest Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/new-options-for-ai-powered-innovation-resiliency-and-control-with-microsoft-azure/",
      "title": "New options for AI-powered innovation, resiliency, and control with Microsoft Azure",
      "summary": "We are extending Azure public regions with options that adapt to our customers’ evolving business requirements without forcing trade-offs. The post New options for AI-powered innovation, resiliency, and control with Microsoft Azure appeared first on Microsoft Azure Blog .",
      "published_ts": 1764781200,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/new-serverless-customization-in-amazon-sagemaker-ai-accelerates-model-fine-tuning/",
      "title": "New serverless customization in Amazon SageMaker AI accelerates model fine-tuning",
      "summary": "Accelerate AI model development with new training features that enable rapid recovery from failures and automatic scaling based on resource availability.",
      "published_ts": 1764778083,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/",
      "title": "Introducing checkpointless and elastic training on Amazon SageMaker HyperPod",
      "summary": "Accelerate AI model development with new training features that enable instant recovery from failures and automatic scaling based on resource availability.",
      "published_ts": 1764778072,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/ddos-threat-report-2025-q3/",
      "title": "Cloudflare's 2025 Q3 DDoS threat report -- including Aisuru, the apex of botnets",
      "summary": "Welcome to the 23rd edition of Cloudflare’s Quarterly DDoS Threat Report. This report offers a comprehensive analysis of the evolving threat landscape of Distributed Denial of Service (DDoS) attacks based on data from the Cloudflare network. In this edition, we focus on the third quarter of 2025.",
      "published_ts": 1764770400,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/waf-rules-react-vulnerability/",
      "title": "Cloudflare WAF proactively protects against React vulnerability",
      "summary": "Cloudflare offers protection against a new high profile vulnerability for React Server Components: CVE-2025-55182. All WAF customers are automatically protected as long as the WAF is deployed.",
      "published_ts": 1764720000,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/introducing-mistral-large-3-in-microsoft-foundry-open-capable-and-ready-for-production-workloads/",
      "title": "Introducing Mistral Large 3 in Microsoft Foundry: Open, capable, and ready for production workloads",
      "summary": "Explore Mistral Large 3 in Azure—open-source, long-context, multimodal AI built for reliable enterprise workloads. The post Introducing Mistral Large 3 in Microsoft Foundry: Open, capable, and ready for production workloads appeared first on Microsoft Azure Blog .",
      "published_ts": 1764698550,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/announcing-replication-support-and-intelligent-tiering-for-amazon-s3-tables/",
      "title": "Announcing replication support and Intelligent-Tiering for Amazon S3 Tables",
      "summary": "New features enable automatic cost optimization through intelligent storage tiering and simplified table replication across AWS Regions and accounts.",
      "published_ts": 1764692354,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-s3-storage-lens-adds-performance-metrics-support-for-billions-of-prefixes-and-export-to-s3-tables/",
      "title": "Amazon S3 Storage Lens adds performance metrics, support for billions of prefixes, and export to S3 Tables",
      "summary": "New capabilities help optimize application performance, analyze unlimited prefixes, and simplify metrics analysis through S3 Tables integration.",
      "published_ts": 1764692112,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/build-multi-step-applications-and-ai-workflows-with-aws-lambda-durable-functions/",
      "title": "Build multi-step applications and AI workflows with AWS Lambda durable functions",
      "summary": "New Lambda capability lets you build applications that coordinate multiple steps reliably over extended periods—from seconds to up to one year—without paying for idle compute time when waiting for external events or human decisions.",
      "published_ts": 1764691939,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-rds-for-oracle-and-rds-for-sql-server-add-new-capabilities-to-enhance-performance-and-optimize-costs/",
      "title": "New capabilities to optimize costs and improve scalability on Amazon RDS for SQL Server and Oracle",
      "summary": "Manage development, testing, and production database workloads more efficiently with new features including Developer Edition support for SQL Server, M7i/R7i instance support with optimize CPU, and expanded storage options up to 256 TiB.",
      "published_ts": 1764691769,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-database-savings-plans-for-aws-databases/",
      "title": "Introducing Database Savings Plans for AWS Databases",
      "summary": "New pricing model helps maintain cost efficiency while providing flexibility with database services and deployment options.",
      "published_ts": 1764691766,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-cloudwatch-introduces-unified-data-management-and-analytics-for-operations-security-and-compliance/",
      "title": "Amazon CloudWatch introduces unified data management and analytics for operations, security, and compliance",
      "summary": "Reduce data management complexity and costs with automatic normalization across sources, native analytics integration, and built-in support for industry-standard formats like OCSF and Apache Iceberg.",
      "published_ts": 1764691631,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/new-and-enhanced-aws-support-plans-add-ai-capabilities-to-expert-guidance/",
      "title": "New and enhanced AWS Support plans add AI capabilities to expert guidance",
      "summary": "Prevent cloud infrastructure issues before they impact your business with AWS Support plans that combine AI-powered insights with expert guidance, offering faster response times and proactive monitoring across performance, security, and cost dimensions.",
      "published_ts": 1764691623,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-opensearch-service-improves-vector-database-performance-and-cost-with-gpu-acceleration-and-auto-optimization/",
      "title": "Amazon OpenSearch Service improves vector database performance and cost with GPU acceleration and auto-optimization",
      "summary": "Build and optimize large-scale vector databases up to 10 times faster and at a quarter of the cost with new GPU acceleration and auto-optimization capabilities that automatically balance search quality, speed, and resource usage.",
      "published_ts": 1764691601,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-s3-vectors-now-generally-available-with-increased-scale-and-performance/",
      "title": "Amazon S3 Vectors now generally available with increased scale and performance",
      "summary": "Scale vector storage and querying to new heights with S3 Vectors' general availability—now supporting up to 1 billion vectors per index, 100ms query latencies, and expanded regional availability, while reducing costs up to 90% compared to specialized databases.",
      "published_ts": 1764691571,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/introducing-amazon-ec2-x8aedz-instances-powered-by-5th-gen-amd-epyc-processors-for-memory-intensive-workloads/",
      "title": "Introducing Amazon EC2 X8aedz instances powered by 5th Gen AMD EPYC processors for memory-intensive workloads",
      "summary": "New memory-optimized instances deliver up to 5 GHz processor speeds and 3 TiB of memory—ideal for electronic design automation workloads and memory-intensive databases requiring high single-threaded performance.",
      "published_ts": 1764691544,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/aws-devops-agent-helps-you-accelerate-incident-response-and-improve-system-reliability-preview/",
      "title": "AWS DevOps Agent helps you accelerate incident response and improve system reliability (preview)",
      "summary": "New service acts as an always-on DevOps engineer, helping you respond to incidents, identify root causes, and prevent future issues through systematic analysis of incidents and operational patterns.",
      "published_ts": 1764691542,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-fsx-for-netapp-ontap-now-integrates-with-amazon-s3-for-seamless-data-access/",
      "title": "Amazon FSx for NetApp ONTAP now integrates with Amazon S3 for seamless data access",
      "summary": "Access FSx for NetApp ONTAP file data through S3 to enable AI/ML workloads and analytics—letting you use enterprise file data with Bedrock, SageMaker, and analytics services while it remains in your file system.",
      "published_ts": 1764691194,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/new-aws-security-agent-secures-applications-proactively-from-design-to-deployment-preview/",
      "title": "New AWS Security Agent secures applications proactively from design to deployment (preview)",
      "summary": "Scale your AppSec expertise with AI-powered design reviews, code analysis, and contextual penetration testing that understand your unique security requirements and application architecture.",
      "published_ts": 1764691121,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/aws-security-hub-now-generally-available-with-near-real-time-analytics-and-risk-prioritization/",
      "title": "AWS Security Hub now generally available with near real-time analytics and risk prioritization",
      "summary": "Today, AWS Security Hub is generally available, transforming how security teams identify and respond to critical security risks across their AWS environments. These new capabilities were first announced in preview at AWS re:Inforce 2025. Security Hub prioritizes your critical security issues and unifies your security operations to help you respond at scale by correlating and […]",
      "published_ts": 1764691091,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/aws/amazon-guardduty-adds-extended-threat-detection-for-amazon-ec2-and-amazon-ecs/",
      "title": "Amazon GuardDuty adds Extended Threat Detection for Amazon EC2 and Amazon ECS",
      "summary": "Today, we’re announcing new enhancements to Amazon GuardDuty Extended Threat Detection with the addition of two attack sequence findings for Amazon Elastic Compute Cloud (Amazon EC2) instances and Amazon Elastic Container Service (Amazon ECS) tasks. These new findings build on the existing Extended Threat Detection capabilities, which already combine sequences involving AWS Identity and Access […]",
      "published_ts": 1764691074,
      "source_name": "AWS Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.uber.com/blog/improving-mysql-cluster-uptime-part1/",
      "title": "Improving MySQL® Cluster Uptime: Designing Advanced Detection, Mitigation, and Consensus with Group Replication",
      "summary": "At Uber, high availability is non-negotiable. Learn how we’ve adopted MySQL® Group Replication in single-primary mode to achieve a less than 10 second failover time  and massively improve reliability and write availability during failures.",
      "published_ts": 1764684000,
      "source_name": "Uber Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://stripe.com/blog/new-features-to-help-saas-platforms-manage-risk-and-stay-compliant",
      "title": "New features to help SaaS platforms manage risk and stay compliant",
      "summary": "We recently launched three new features that give you more control to fine-tune your risk and compliance strategy, allowing you to use more of Stripe’s data to inform your approach. Here’s what’s new.",
      "published_ts": 1764633600,
      "source_name": "Stripe Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/automate-embedded-systems-compliance-with-gitlab-and-codesonar/",
      "title": "Automate embedded systems compliance with GitLab and CodeSonar",
      "summary": "Embedded systems development teams face a persistent challenge: maintaining development velocity while meeting stringent functional safety and code quality requirements. Standards like ISO 26262, IEC 62304, DO-178C, and IEC 61508 demand rigorous verification processes that are often manual and time-consuming. Compliance reviews against coding standards like MISRA C/C++, isolated scanning workflows, and post-development verification create bottlenecks. Teams are forced to choose between speed and safety. GitLab's integration with CodeSonar (from AdaCore) addresses this challenge by automating compliance workflows and enabling continuous verification throughout the development lifecycle. Specialized scanning for safety-critical systems Safety-critical systems require deep analysis of C/C++ code compiled with specialized embedded tools. These systems must demonstrate compliance with coding standards (MISRA C/C++, CERT C/C++, AUTOSAR C++) and functional safety frameworks (ISO 26262, DO-178C, IEC 61508) that require detailed evidence trails. Beyond aligning with coding standards, teams also need to address security concerns. This means testing for memory problems as well as a host of other problems like uninitialized variables and command injection. CodeSonar performs whole program analysis with specialized scanning capabilities for these standards. Pairing CodeSonar with GitLab enables teams to automate compliance workflows and maintain comprehensive audit trails throughout the development lifecycle. Automating compliance from commit to merge The GitLab and CodeSonar integration provides a compliance-as-code approach that automates policy enforcement from the earliest stages of development. CodeSonar functions as an additional scanner within GitLab CI/CD pipelines , analyzing code in every commit and merge request. Because CodeSonar was purpose-built for embedded systems, it performs deep control flow and data flow analysis across entire programs, identifying vulnerabilities like buffer overruns, data taint, uninitialized variables, use-after-free conditions, and command injection — the root causes of most security incidents in embedded systems. The integration works through GitLab's CI/CD configuration. When developers push code changes, the pipeline triggers CodeSonar scanning. For C and C++ firmware, CodeSonar observes compiler invocations during the actual build process, creating an internal representation of the code that enables sophisticated analysis. Results are converted from SARIF format to GitLab's Static Application Security Testing ( SAST ) format and surfaced directly in merge requests, where they feed into GitLab Ultimate's Security Dashboard, Vulnerability Management, and Compliance Frameworks . Example workflow: ISO 26262 ASIL-D compliance The demo video below shows the complete workflow for an embedded system subject to ISO 26262 ASIL-D requirements. The scenario illustrates how embedded development teams can implement continuous compliance without compromising development velocity. <div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1139086924?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"Automated Compliance for Embedded Systems using GitLab and CodeSonar\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>` The workflow begins with a developer submitting a merge request for firmware changes. GitLab's CI/CD pipeline automatically triggers CodeSonar scanning, which performs deep C/C++ analysis against custom ISO 26262 policies configured in the pipeline. When CodeSonar identifies an ASIL-D relevant vulnerability, the pipeline halts automatically per the compliance policy, with clear documentation explaining the issue. The complete scan results, issue tracking, and approval workflow are maintained in GitLab as a single source of truth for audit trails. Developers can use both the CodeSonar hub interface and GitLab Duo AI to understand the vulnerability. CodeSonar provides detailed information about the path through the source code that leads to the problem, along with code navigation features to isolate the root cause. GitLab Duo explains the vulnerability and provides specific remediation recommendations . After the developer implements the fix and validates the resolution, the code merges successfully with full compliance evidence automatically collected throughout the process. Benefits of the integration Organizations implementing this integrated compliance with GitLab and CodeSonar will see significant improvements in both development velocity and compliance confidence. Efficiency gains: Development teams reduce time-to-market by catching coding standard compliance issues early when they're less expensive to fix. Automated security policy enforcement decreases manual security review overhead, freeing specialists to focus on complex problems rather than routine checks. Audit readiness improves through automated evidence collection. Compliance artifacts are generated as a by-product of normal development rather than through separate documentation efforts. Compliance maturity: This integrated approach helps organizations maintain continuous compliance with industry standards and regulations. By embedding verification into every code change, teams build comprehensive audit trails that demonstrate adherence to ISO 26262, DO-178C, MISRA C/C++, and other requirements. The automated workflow transforms compliance from a periodic checkpoint into an ongoing verification process. Implementation considerations Implementing the GitLab and CodeSonar integration requires access to GitLab Ultimate, a CodeSonar hub, GitLab runners where code can be compiled and analyzed, and appropriate mechanisms for managing analysis data files. Both GitLab and CodeSonar fully support on-premises and air-gapped environments and can be deployed to auto-scalable cloud environments as well. Teams should configure Custom Compliance Frameworks in GitLab to define specific policies for their relevant standards: ISO 26262 for automotive, DO-178C for aerospace, IEC 62304 for medical devices, and others. These frameworks enable automated enforcement of compliance requirements through merge request approval rules, vulnerability thresholds, and scan policy gates. Get started The CodeSonar GitLab CI component is available through GitLab's CI/CD Catalog. Detailed integration documentation provides platform-specific setup instructions for Linux, Docker, and Windows environments. For organizations evaluating this solution, the implementation demonstrates how specialized embedded systems tools can integrate with a modern DevSecOps platform to deliver both development velocity and compliance rigor. For more information about implementing GitLab with CodeSonar for your embedded systems development, visit the CodeSonar integration documentation . You can also request a trial of CodeSonar .",
      "published_ts": 1764633600,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/azure-networking-updates-on-security-reliability-and-high-availability/",
      "title": "Azure networking updates on security, reliability, and high availability",
      "summary": "The cloud landscape is evolving at an unprecedented pace, driven by the exponential growth of AI workloads. The post Azure networking updates on security, reliability, and high availability appeared first on Microsoft Azure Blog .",
      "published_ts": 1764608400,
      "source_name": "Azure Blog",
      "content_type": "technical"
    }
  ],
  "data_modeling_governance": [
    {
      "url": "https://medium.com/pinterest-engineering/improving-quality-of-recommended-content-through-pinner-surveys-eebca8a52652?source=rss----4c5a5f6279b6---4",
      "title": "Improving Quality of Recommended Content through Pinner Surveys",
      "summary": "Rudraksh Kapil | Machine Learning Engineer I; Michal Giemza | Senior Machine Learning Engineer; Devan Srinivasan | Machine Learning Engineering Intern; Leif Sigerson | Senior Data Scientist; Stephanie Chen | Staff Quantitative Product Researcher; Wendy Matheny | Senior Lead Public Policy Manager; Jianjin Dong | Engineering Manager II; Qinglong Zeng | Senior Engineering Manager Introductory Summary In 2023 Pinterest became the Founding Signatory of the Inspired Internet Pledge — publicly stating our vision to adhere to three principles: (1) tuning for wellbeing, (2) listening to and acting on what we hear from users, and (3) sharing what we learn about making the internet a safer and healthier place for all, especially teens. Now, at the end of the second full year of our work with the Inspired Internet Pledge, we’re pleased to share more about what we’re learning from Pinner (a.k.a. user) surveys and how we incorporate their feedback to improve content quality on our platform. While much of this blog will get into the weeds of how we design surveys, interpret the data we get from them, and technical details on how we train a machine learning model to understand generic Pinner perception, at the heart of all of this is Pinterest’s commitment to “Put Pinners First.” Our work demonstrates a win/win for both Pinners and the business, and allows us to do good while doing well. Background At Pinterest, we want people to discover high-quality content — content that makes them feel good when they see it, inspires them to keep exploring, and ultimately drives fulfilling, long-term engagement. But it’s challenging to construct guidelines on what exactly “high quality” content is without understanding the average Pinner’s notion of quality. We know that when we only show content that’s engaging, it tends to promote low-quality “clickbait” with limited long-term engagement. Unsurprisingly, an effective way to understand Pinners’ perception of quality is to ask them directly. Surveys are a way for Pinners to tell us exactly what they think and for us to build that intentionality directly into our platform . If we use Pinner surveys to teach our recommendation systems to promote highly-rated content, we expect people will react positively — with both good feelings and as a result, with action, repins, and saves. Experts in the industry have also called for using surveys more for training recommendation systems [ Stray et. al., 2022 , Cunningham et. al., 2024 ], rather than optimizing purely for engagement. Not all engagement is good , so the latter approach can mislead the system into promoting low-quality or even harmful content. Surveys provide an excellent avenue for de-biasing the system, allowing us to understand content quality and ensure that the engagement we reward comes from high quality content. In this blog post we’ll discuss the success we’ve found with incorporating a machine learning model trained on survey data into all three of our major surfaces: Homefeed, Related Pins, and Search. This aligns with one of our company’s core values, to Put Pinners First, by optimizing our recommendations according to their feedback. Survey Data Collection We launched an in-app survey campaign where we asked Pinners to rate images on a scale of 1–5 for visual appeal. The exact question wording was, “How visually pleasing or displeasing is this Pin?” [Figure 1]. We intentionally left the wording somewhat vague to encourage Pinners to respond based on their own notion of quality. Although it would be great to get survey responses for each of the billions of images in our corpus, we have a policy of not bombarding Pinners with constant survey requests. For this survey, we limited ourselves to collecting responses for just 5k Pins. We sampled 1k Pins (weighted by impressions) from each of our top five L1 interest verticals: Art , Beauty , DIY & Crafts , Home Decor , and Women’s Fashion . “ L1” here refers to Pinterest’s top level taxonomy of interests . We only asked Pinners to rate mid-to-high quality images rather than exposing them to low quality images just for the purposes of this survey. It’s important to remember that the question of visual quality is in the “sweet spot” for subjectivity. For things that are relatively objective (e.g., policy violating content), a company should pay reliable human reviewers and not burden users or decrease their experience. For things that are extremely subjective (e.g., personal relevance), we wouldn’t be able to get a reliable Pin-level signal because the score would be a function of the Pin, the context, and the individual rater. Since our goal is to understand visual quality for the average Pinner, a survey is well suited for data collection. Figure 1. In-app survey UI. “Very visually displeasing” is assigned a score of 1, while “Very visually pleasing” is assigned a score of 5. Importantly, we collected at least 10 responses for each image to reduce subjectivity and alleviate noise. By computing each image’s average rating, we can get an idea of what the average Pinner would rate the image, which acts as a proxy for the “objective” rating of the image. Asking multiple Pinners to rate each image also leaves a buffer for misclicks; if each image was rated just once, we would have run the risk of ending up with a noisy and unreliable dataset. The data from this survey allowed us to get thoughtful reflections directly from Pinners on what they consider to be quality content, which enables us to build more “good” into the product and weed out the “bad.” For example, the highest average ratings were received by generally appealing images across wide-ranging topics like makeup, grooming styles, maximalist home decor, landscapes, sunsets, and baby animals. There were also subtle differences in ratings between the different L1 interests in our survey. Home decor Pins were in general rated higher than the rest [Figure 2]. The highest variance in responses was for Art Pins, which isn’t too surprising considering the subjective nature of art, and they make up the majority of both the top and bottom 200 images [Figure 3]. Figure 2. Distribution of Pinner ratings in each interest vertical. Figure 3. Proportion of images from each L1 interest vertical in the top and bottom 100 images. Art and Home Decor dominate the top 100, whereas the bottom 100 is more evenly distributed with Art images having the highest representation once again. Machine Learning Modelling We leveraged the survey data to train a machine learning model for learning the average Pinner’s perception of visual quality. Given embedding features for an image, the model’s task is to map to a single score between 0 and 1, with higher scores indicating higher visual quality as perceived by the average Pinner. These in-house embedding features each represent some aspect of the Pin image, such as the relationships between the image and boards it’s saved to along with its visual and textual information. We opted for a simple fully-connected neural network with 92k parameters to learn this mapping. Not only does this help prevent the model from overfitting to the relatively small dataset of 5k Pins, but it also makes inference at scale quicker and cheaper. The model architecture is depicted in Figure 4. Figure 4. Machine learning model architecture. We utilized a fully-connected network to map content embedding features to a single score representing visual quality. The problem was formulated as a pairwise ranking rather than a classification or regression problem — a well established machine learning technique to find the relative ranking of items instead of a composite score for each independent item. The idea can be simplified as: Given two images, we ask the model to predict which one the average Pinner would agree is “better,” rather than trying to predict the actual mean response score for a single image. To define what’s “better” quality, we take the mean of the 10 responses per image as its ground truth quality score. So the task is formulated as a two-item ranking problem, where the model needs to determine which image has the higher mean response. We only compare images belonging to the same L1 when training the model. This is done to force the model to focus on visual quality differences between images, rather than semantic ones. During training, the model outputs a score for each image. Then, to account for differences between L1s, we separate the images into their L1 interest verticals. To introduce stochasticity for more effective training, we further randomly split the images into smaller groups or “sub-group.” We compute the pairwise margin ranking loss within each sub-group and optimize the model weights to reduce this loss. As the name indicates, the loss function is computed between all pairs within a group and summed. The training process is summarized in Figure 5 below. Figure 5. Grouped pairwise ranking approach. Within each group we compute pairwise margin ranking loss. Although we collected multiple responses for each image, our dataset may still be somewhat noisy. We incorporate the variance in responses for images by using a variable margin for the loss function. Briefly, the margin is the minimum difference we want the model to enforce between the scores of the two images. If the better image’s score isn’t higher by at least this margin, the model is penalized. The margin in this loss function is typically fixed. We instead vary the margin, using higher values for images whose response mean is more certain, and lower values otherwise. Our ranking-based approach can help tackle the data sparsity issue. Although we only collected responses for 5k images due to limited survey bandwidth, the pairwise ranking approach effectively expands the dataset size to 5 * 1000C2 = ~2.5M pairs, and the model is able to learn nuances between images to understand why one may be perceived as higher quality than another. Moreover, this ranking approach also makes it more suitable for adoption in Pinterest’s downstream surface recommendation systems, where the absolute score matters much less for these ranking models than the relative differences between different Pins. Offline Results Offline evaluation on a holdout test set showed that our model can correctly distinguish between higher and lower content quality, hinting early on that it could serve as an informative feature if incorporated into the surface recommenders. As expected, images that were assigned high scores by our model looked similar to those that received high ratings in our survey. From Figure 6, we can see that the model-predicted scores align well with the Pinner ratings for our holdout test set, with over 90% of predictions being within one standard deviation. The kernel density estimate plots in Figure 7 further show that our model can distinguish between high and low quality images. Quantitatively, we looked at two key metrics. The first is pairwise ranking accuracy, which measures whether the model can correctly predict which of two given images is higher quality. The second is NDCG@20, or Normalized Discounted Cumulative Gain, which measures how correctly a list of 20 images are ranked by the model scores, with more importance on the top of the list. The results are summarized in Table 1. Overall the model performs well, with better performance on some verticals ( Art ) than others ( Women’s Fashion ). Table 1. Offline evaluation results on test set. The model performs better for some verticals like Art and Beauty than others. Figure 6 . Distribution of model predicted scores (blue) and the “true” Pinner mean ratings (green). Outliers whose predictions fall outside one standard deviation of the mean responses are marked in red. Figure 7. Kernel density estimate (KDE) plot of model predicted scores for the top and bottom 100 Pins by true Pinner ratings. The peaks for the true and predicted distributions for both sets are roughly aligned, with our model predicted scores being more spread out due to the varying nature of responses. Impact The recommender systems for major surfaces at Pinterest such as Homefeed determine the content that makes it to a Pinner’s feed, using a combination of objective content features (e.g., the L1 interest, the Pin title and description, etc.) and subjective user features (e.g., what other Pins the Pinner has interacted with, search queries, etc.). Online A/B experiment results showed that the visual quality signal we built is a win/win for both Pinners and the business across all three major user-facing surfaces at Pinterest : Homefeed, Search, and Related Pins. We saw significant reductions in “low quality” sessions (i.e., sessions where Pinners encounter low quality content) and increases in “successful” sessions (i.e., sessions where Pinners are able to find what they’re looking for) showing that the overall Pinner experience is improved. We also see an increase in individual engagement metrics like repins of organic content and long click-throughs on shopping content to name a few. Taken together, these suggest we’re delivering better content that Pinners want to interact with. Table 2. Examples of some of our numerous online metrics wins across all three major ranking surfaces. Although we split the table into two sets of wins for clarity, we strongly feel that any wins for Pinners are also a win for our business, and vice versa. Building on this Success At Pinterest, we strive to incorporate Pinner feedback directly into our product to keep improving and Putting Pinners First. Surveys are an excellent way to give Pinners a voice and learn what they think is high quality, and for them to tell us what kind of content they’d like to see more of on their feeds. Our work has shown that incorporating survey feedback into our ranking systems is a win-win for both Pinners and our business! At the end of the day, Pinterest is about personalization. Pinners choose what ultimately makes it to their boards. Improving the quality of recommendations for the general audience helps us refine the “best” images that will eventually make it to their boards. We plan to continue building on the success we’ve found in this work. We have conducted additional similar surveys and will iterate on our model. We’ve explored leveraging state-of-the-art Visual Language Models (VLMs) that can learn more intrinsic information from the survey responses, and we are keen to productionize these in the next iteration of this signal in 2026. Moreover, we also monitor Pinner perception of content quality via ongoing tracking surveys, and we are working on expanding our survey-based signals to include other types of content besides images. Acknowledgements This was a cross-functional effort that would not have been possible without excellent support from our partner teams. The Content Quality Team would sincerely like to thank Survey Support Team Anket Team Experience Framework Team Related Pins Ranking Team Search Ranking Team Homefeed Team Improving Quality of Recommended Content through Pinner Surveys was originally published in Pinterest Engineering Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "published_ts": 1764964933,
      "source_name": "Pinterest Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/maximizing-enterprise-data-products-distribution",
      "title": "Maximizing Enterprise Data Products Distribution",
      "summary": "What’s the difference between traditional data outputs and data products? Jean-Guillaume Appert , senior director of product management at Dataiku, and Marko Stojsavljevic , business transformation expert at Dataiku, answered this question (and more!) in a recent Dataiku Product Days session.",
      "published_ts": 1764964171,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.getdbt.com/blog/bring-structured-context-to-conversational-analytics-with-dbt",
      "title": "Bring structured context to conversational analytics with dbt",
      "summary": "Make conversational analytics trustworthy with dbt as your foundation.",
      "published_ts": 1764802740,
      "source_name": "dbt Blog",
      "content_type": "technical"
    },
    {
      "url": "https://seattledataguy.substack.com/p/translating-data-buzzwords-into-real",
      "title": "Translating Data Buzzwords Into Real Requirements",
      "summary": "Bridging the Communication Gap Between Data and the Business",
      "published_ts": 1764688343,
      "source_name": "Seattle Data Guy",
      "content_type": "rex"
    },
    {
      "url": "https://www.databricks.com/blog/completing-lakehouse-vision-open-storage-open-access-unified-governance",
      "title": "Completing the Lakehouse Vision: Open Storage, Open Access, Unified Governance",
      "summary": "Until now, organizations have had no way to unify attribute-based access control...",
      "published_ts": 1764683680,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    }
  ],
  "etl_orchestration": [
    {
      "url": "https://dagster.io/blog/orchestrating-nanochat-building-the-tokenizer",
      "title": "From NanoChat to Dagster: Building Your Ingestion Pipeline and Custom Tokenizer",
      "summary": "Explore how to transform NanoChat into a Dagster-powered LLM system by designing ingestion flows, cleaning datasets, and building a tokenizer tailored to your model’s needs.",
      "published_ts": 1764792629,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/migration-from-azure-devops-to-gitlab/",
      "title": "Guide: Migrate from Azure DevOps to GitLab",
      "summary": "Migrating from Azure DevOps to GitLab can seem like a daunting task, but with the right approach and tools, it can be a smooth and efficient process. This guide will walk you through the steps needed to successfully migrate your projects, repositories, and pipelines from Azure DevOps to GitLab. Overview GitLab provides both Congregate (maintained by GitLab Professional Services organization) and a built-in Git repository import for migrating projects from Azure DevOps (ADO). These options support repository-by-repository or bulk migration and preserve git commit history, branches, and tags. With Congregate and professional services tools, we support additional assets such as wikis, work items, CI/CD variables, container images, packages, pipelines, and more (see this feature matrix ). Use this guide to plan and execute your migration and complete post-migration follow-up tasks. Enterprises migrating from ADO to GitLab commonly follow a multi-phase approach: Migrate repositories from ADO to GitLab using Congregate or GitLab's built-in repository migration. Migrate pipelines from Azure Pipelines to GitLab CI/CD. Migrate remaining assets such as boards, work items, and artifacts to GitLab Issues, Epics, and the Package and Container Registries. High-level migration phases: graph LR\n    subgraph Prerequisites\n        direction TB\n        A[\"Set up identity provider (IdP) and<br/>provision users\"]\n        A --> B[\"Set up runners and<br/>third-party integrations\"]\n        B --> I[\"Users enablement and<br/>change management\"]\n    end\n    \n    subgraph MigrationPhase[\"Migration phase\"]\n        direction TB\n        C[\"Migrate source code\"]\n        C --> D[\"Preserve contributions and<br/> format history\"]\n        D --> E[\"Migrate work items and<br/>map to <a href=\"https://docs.gitlab.com/topics/plan_and_track/\">GitLab Plan <br/>and track work\"]\n    end\n    \n    subgraph PostMigration[\"Post-migration steps\"]\n        direction TB\n        F[\"Create or translate <br/>ADO pipelines to GitLab CI\"]\n        F --> G[\"Migrate other assets<br/>packages and container images\"]\n        G --> H[\"Introduce <a href=\"https://docs.gitlab.com/user/application_security/secure_your_application/\">security</a> and<br/>SDLC improvements\"]\n    end\n    \n    Prerequisites --> MigrationPhase\n    MigrationPhase --> PostMigration\n\n    style A fill:#FC6D26\n    style B fill:#FC6D26\n    style I fill:#FC6D26\n    style C fill:#8C929D\n    style D fill:#8C929D\n    style E fill:#8C929D\n    style F fill:#FFA500\n    style G fill:#FFA500\n    style H fill:#FFA500 Planning your migration To plan your migration, ask these questions: How soon do we need to complete the migration? Do we understand what will be migrated? Who will run the migration? What organizational structure do we want in GitLab? Are there any constraints, limitations, or pitfalls that need to be taken into account? Determine your timeline, as it will largely dictate your migration approach. Identify champions or groups familiar with both ADO and GitLab platforms (such as early adopters) to help drive adoption and provide guidance. Inventory what you need to migrate: The number of repositories, pull requests, and contributors The number and complexity of work items and pipelines Repository sizes and dependency relationships Critical integrations and runner requirements (agent pools with specific capabilities) Use GitLab Professional Services's Evaluate tool to produce a complete inventory of your entire Azure DevOps organization, including repositories, PR counts, contributor lists, number of pipelines, work items, CI/CD variables and more. If you're working with the GitLab Professional Services team, share this report with your engagement manager or technical architect to help plan the migration. Migration timing is primarily driven by pull request count, repository size, and amount of contributions (e.g. comments in PR, work items, etc). For example, 1,000 small repositories with few PRs and limited contributors can migrate much faster than a smaller set of repositories containing tens of thousands of PRs and thousands of contributors. Use your inventory data to estimate effort and plan test runs before proceeding with production migrations. Compare inventory against your desired timeline and decide whether to migrate all repositories at once or in batches. If teams cannot migrate simultaneously, batch and stagger migrations to align with team schedules. For example, in Professional Services engagements, we organize migrations into waves of 200-300 projects to manage complexity and respect API rate limits, both in GitLab and ADO . GitLab's built-in repository importer migrates Git repositories (commits, branches, and tags) one-by-one. Congregate is designed to preserve pull requests (known in GitLab as merge requests), comments, and related metadata where possible; the simple built-in repository import focuses only on the Git data (history, branches, and tags). Items that typically require separate migration or manual recreation: Azure Pipelines - create equivalent GitLab CI/CD pipelines (consult with CI/CD YAML and/or with CI/CD components ). Alternatively, consider using AI-based pipeline conversion available in Congregate. Work items and boards - map to GitLab Issues, Epics, and Issue Boards. Artifacts, container images (ACR) - migrate to GitLab Package Registry or Container Registry. Service hooks and external integrations - recreate in GitLab. Permissions models differ between ADO and GitLab; review and plan permissions mapping rather than assuming exact preservation. Review what each tool (Congregate vs. built-in import) will migrate and choose the one that fits your needs. Make a list of any data or integrations that must be migrated or recreated manually. Who will run the migration? Migrations are typically run by a GitLab group owner or instance administrator, or by a designated migrator who has been granted the necessary permissions on the destination group/project. Congregate and the GitLab import APIs require valid authentication tokens for both Azure DevOps and GitLab. Decide whether a group owner/admin will perform the migrations or whether you will grant a specific team/person delegated access. Ensure the migrator has correctly configured personal access tokens (Azure DevOps and GitLab) with the scopes required by your chosen migration tool (for example, api/read_repository scopes and any tool-specific requirements). Test tokens and permissions with a small pilot migration. Note: Congregate leverages file-based import functionality for ADO migrations and requires instance administrator permissions to run ( see our documentation ). If you are migrating to GitLab.com, consider engaging Professional Services. For more information, see the Professional Services Full Catalog . Non-admin account cannot preserve contribution attribution! What organizational structure do we want in GitLab? While it's possible to map ADO structure directly to GitLab structure, it's recommended to rationalize and simplify the structure during migration. Consider how teams will work in GitLab and design the structure to facilitate collaboration and access management. Here is a way to think about mapping ADO structure to GitLab structure: graph TD\n    subgraph GitLab\n        direction TB\n        A[\"Top-level Group\"]\n        B[\"Subgroup (optional)\"]\n        C[\"Projects\"]\n        A --> B\n        A --> C\n        B --> C\n    end\n\n    subgraph AzureDevOps[\"Azure DevOps\"]\n        direction TB\n        F[\"Organizations\"]\n        G[\"Projects\"]\n        H[\"Repositories\"]\n        F --> G\n        G --> H\n    end\n\n    style A fill:#FC6D26\n    style B fill:#FC6D26\n    style C fill:#FC6D26\n    style F fill:#8C929D\n    style G fill:#8C929D\n    style H fill:#8C929D Recommended approach: Map each ADO organization to a GitLab group (or a small set of groups), not to many small groups. Avoid creating a GitLab group for every ADO team project. Use migration as an opportunity to rationalize your GitLab structure. Use subgroups and project-level permissions to group related repositories. Manage access to sets of projects by using GitLab groups and group membership (groups and subgroups) rather than one group per team project. Review GitLab permissions and consider SAML Group Links to implement an enterprise RBAC model for your GitLab instance (or a GitLab.com namespace). ADO Boards and work items: State of migration It's important to understand how work items migrate from ADO into GitLab Plan (issues, epics, and boards). ADO Boards and work items map to GitLab Issues, Epics, and Issue Boards. Plan how your workflows and board configurations will translate. ADO Epics and Features become GitLab Epics. Other work item types (e.g., user stories, tasks, bugs) become project-scoped issues. Most standard fields are preserved; selected custom fields can be migrated when supported. Parent-child relationships are retained so Epics reference all related issues. Links to pull requests are converted to merge request links to maintain development traceability. Example: Migration of an individual work item to a GitLab Issue, including field accuracy and relationships: Batching guidance: If you need to run migrations in batches, use your new group/subgroup structure to define batches (for example, by ADO organization or by product area). Use inventory reports to drive batch selection and test each batch with a pilot migration before scaling. Pipelines migration Congregate recently introduced AI-powered conversion for multi-stage YAML pipelines from Azure DevOps to GitLab CI/CD. This automated conversion works best for simple, single-file pipelines and is designed to provide a working starting point rather than a production-ready .gitlab-ci.yml file. The tool generates a functionally equivalent GitLab pipeline that you can then refine and optimize for your specific needs. Converts Azure Pipelines YAML to .gitlab-ci.yml format automatically. Best suited for straightforward, single-file pipeline configurations. Provides a boilerplate to accelerate migration, not a final production artifact. Requires review and adjustment for complex scenarios, custom tasks, or enterprise requirements. Does not support Azure DevOps classic release pipelines — convert these to multi-stage YAML first. Repository owners should review the GitLab CI/CD documentation to further optimize and enhance their pipelines after the initial conversion. Example of converted pipelines: # azure-pipelines.yml\n\ntrigger:\n  - main\n\nvariables:\n  imageName: myapp\n\nstages:\n  - stage: Build\n    jobs:\n      - job: Build\n        pool:\n          vmImage: 'ubuntu-latest'\n        steps:\n          - checkout: self\n\n          - task: Docker@2\n            displayName: Build Docker image\n            inputs:\n              command: build\n              repository: $(imageName)\n              Dockerfile: '**/Dockerfile'\n              tags: |\n                $(Build.BuildId)\n\n  - stage: Test\n    jobs:\n      - job: Test\n        pool:\n          vmImage: 'ubuntu-latest'\n        steps:\n          - checkout: self\n\n          # Example: run tests inside the container\n          - script: |\n              docker run --rm $(imageName):$(Build.BuildId) npm test\n            displayName: Run tests\n\n  - stage: Push\n    jobs:\n      - job: Push\n        pool:\n          vmImage: 'ubuntu-latest'\n        steps:\n          - checkout: self\n\n          - task: Docker@2\n            displayName: Login to ACR\n            inputs:\n              command: login\n              containerRegistry: '<your-acr-service-connection>'\n\n          - task: Docker@2\n            displayName: Push image to ACR\n            inputs:\n              command: push\n              repository: $(imageName)\n              tags: |\n                $(Build.BuildId) # .gitlab-ci.yml\n\nvariables:\n  imageName: myapp\n\nstages:\n  - build\n  - test\n  - push\n\nbuild:\n  stage: build\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - docker build -t $imageName:$CI_PIPELINE_ID -f $(find . -name Dockerfile) .\n  only:\n    - main\n\ntest:\n  stage: test\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - docker run --rm $imageName:$CI_PIPELINE_ID npm test\n  only:\n    - main\n\npush:\n  stage: push\n  image: docker:latest\n  services:\n    - docker:dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker tag $imageName:$CI_PIPELINE_ID $CI_REGISTRY/$CI_PROJECT_PATH/$imageName:$CI_PIPELINE_ID\n    - docker push $CI_REGISTRY/$CI_PROJECT_PATH/$imageName:$CI_PIPELINE_ID\n  only:\n    - main Final checklist: Decide timeline and batch strategy. Produce a full inventory of repositories, PRs, and contributors. Choose Congregate or the built-in import based on scope (PRs and metadata vs. Git data only). Decide who will run migrations and ensure tokens/permissions are configured. Identify assets that must be migrated separately (pipelines, work items, artifacts, and hooks) and plan those efforts. Run pilot migrations, validate results, then scale according to your plan. Running your migrations After planning, execute migrations in stages, starting with trial runs. Trial migrations help surface org-specific issues early and let you measure duration, validate outcomes, and fine-tune your approach before production. What trial migrations validate: Whether a given repository and related assets migrate successfully (history, branches, tags; plus MRs/comments if using Congregate) Whether the destination is usable immediately (permissions, runners, CI/CD variables, integrations) How long each batch takes, to set schedules and stakeholder expectations Downtime guidance: GitLab's built-in Git import and Congregate do not inherently require downtime. For production waves, freeze changes in ADO (branch protections or read-only) to avoid missed commits, PR updates, or work items created mid-migration. Trial runs do not require freezes and can be run anytime. Batching guidance: Run trial batches back-to-back to shorten elapsed time; let teams validate results asynchronously. Use your planned group/subgroup structure to define batches and respect API rate limits. Recommended steps: Create a test destination in GitLab for trials: GitLab.com: create a dedicated group/namespace (for example, my-org-sandbox) Self-managed: create a top-level group or a separate test instance if needed Prepare authentication: Azure DevOps PAT with required scopes. GitLab Personal Access Token with api and read_repository (plus admin access for file-based imports used by Congregate). Run trial migrations: Repos only: use GitLab's built-in import (Repo by URL) Repos + PRs/MRs and additional assets: use Congregate Post-trial follow-up: Verify repo history, branches, tags; merge requests (if migrated), issues/epics (if migrated), labels, and relationships. Check permissions/roles, protected branches, required approvals, runners/tags, variables/secrets, integrations/webhooks. Validate pipelines ( .gitlab-ci.yml ) or converted pipelines where applicable. Ask users to validate functionality and data fidelity. Resolve issues uncovered during trials and update your runbooks. Network and security: If your destination uses IP allow lists, add the IPs of your migration host and any required runners/integrations so imports can succeed. Run production migrations in waves: Enforce change freezes in ADO during each wave. Monitor progress and logs; retry or adjust batch sizes if you hit rate limits. Optional: remove the sandbox group or archive it after you finish. <figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/ibIXGfrVbi4?si=ZxOVnXjCF-h4Ne0N\" frameborder=\"0\" allowfullscreen=\"true\"></iframe>\n</figure> Terminology reference for GitLab and Azure DevOps GitLab Azure DevOps Similarities & Key Differences Group Organization Top-level namespace, membership, policies. ADO org contains Projects; GitLab Group contains Subgroups and Projects. Group or Subgroup Project Logical container, permissions boundary. ADO Project holds many repos; GitLab Groups/Subgroups organize many Projects. Project (includes a Git repo) Repository (inside a Project) Git history, branches, tags. In GitLab, a \"Project\" is the repo plus issues, CI/CD, wiki, etc. One repo per Project. Merge Request (MR) Pull Request (PR) Code review, discussions, approvals. MR rules include approvals, required pipelines, code owners. Protected Branches, MR Approval Rules, Status Checks Branch Policies Enforce reviews and checks. GitLab combines protections + approval rules + required status checks. GitLab CI/CD Azure Pipelines YAML pipelines, stages/jobs, logs. ADO also has classic UI pipelines; GitLab centers on .gitlab-ci.yml. .gitlab-ci.yml azure-pipelines.yml Defines stages/jobs/triggers. Syntax/features differ; map jobs, variables, artifacts, and triggers. Runners (shared/specific) Agents / Agent Pools Execute jobs on machines/containers. Target via demands (ADO) vs tags (GitLab). Registration/scoping differs. CI/CD Variables (project/group/instance), Protected/Masked Pipeline Variables, Variable Groups, Library Pass config/secrets to jobs. GitLab supports group inheritance and masking/protection flags. Integrations, CI/CD Variables, Deploy Keys Service Connections External auth to services/clouds. Map to integrations or variables; cloud-specific helpers available. Environments & Deployments (protected envs) Environments (with approvals) Track deploy targets/history. Approvals via protected envs and manual jobs in GitLab. Releases (tag + notes) Releases (classic or pipelines) Versioned notes/artifacts. GitLab Release ties to tags; deployments tracked separately. Job Artifacts Pipeline Artifacts Persist job outputs. Retention/expiry configured per job or project. Package Registry (NuGet/npm/Maven/PyPI/Composer, etc.) Azure Artifacts (NuGet/npm/Maven, etc.) Package hosting. Auth/namespace differ; migrate per package type. GitLab Container Registry Azure Container Registry (ACR) or others OCI images. GitLab provides per-project/group registries. Issue Boards Boards Visualize work by columns. GitLab boards are label-driven; multiple boards per project/group. Issues (types/labels), Epics Work Items (User Story/Bug/Task) Track units of work. Map ADO types/fields to labels/custom fields; epics at group level. Epics, Parent/Child Issues Epics/Features Hierarchy of work. Schema differs; use epics + issue relationships. Milestones and Iterations Iteration Paths Time-boxing. GitLab Iterations (group feature) or Milestones per project/group. Labels (scoped labels) Area Paths Categorization/ownership. Replace hierarchical areas with scoped labels. Project/Group Wiki Project Wiki Markdown wiki. Backed by repos in both; layout/auth differ slightly. Test reports via CI, Requirements/Test Management, integrations Test Plans/Cases/Runs QA evidence/traceability. No 1:1 with ADO Test Plans; often use CI reports + issues/requirements. Roles (Owner/Maintainer/Developer/Reporter/Guest) + custom roles Access levels + granular permissions Control read/write/admin. Models differ; leverage group inheritance and protected resources. Webhooks Service Hooks Event-driven integrations. Event names/payloads differ; reconfigure endpoints. Advanced Search Code Search Full-text repo search. Self-managed GitLab may need Elasticsearch/OpenSearch for advanced features.",
      "published_ts": 1764720000,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://airbyte.com/blog/agentic-data-infrastructure",
      "title": "The Rise of Agentic Data Infrastructure | Airbyte",
      "summary": "Discover how agentic data infrastructure is transforming modern data systems by combining automation, intelligence, and adaptability.",
      "published_ts": 1764720000,
      "source_name": "Airbyte Blog",
      "content_type": "technical"
    },
    {
      "url": "https://vutr.substack.com/p/my-framework-to-build-a-data-pipeline",
      "title": "A framework I use to build a data pipeline.",
      "summary": "You cannot simply say, \"I'll use Spark, Kafka, and so on\"; you need to ask clarifying questions to gather information for proposing a robust data pipeline.",
      "published_ts": 1764645335,
      "source_name": "VuTrinh · Data Engineering",
      "content_type": "rex"
    },
    {
      "url": "https://www.databricks.com/blog/events-insights-complex-state-processing-schema-evolution-transformwithstate",
      "title": "From Events to Insights: Complex State Processing with Schema Evolution in transformWithState",
      "summary": "Across industries, one of the most persistent challenges in data engineering is schema...",
      "published_ts": 1764608444,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://airbyte.com/blog/introducing-agent-blueprint",
      "title": "Introducing Agent Blueprint | Airbyte",
      "summary": "Agent Blueprint helps teams connect real-time data to AI agents with production-ready infrastructure, faster integration, and reliable context management.",
      "published_ts": 1764547200,
      "source_name": "Airbyte Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/continuously-deploying-the-largest-gitlab-instance/",
      "title": "How we deploy the largest GitLab instance 12 times daily",
      "summary": "Every day, GitLab deploys code changes to the world's largest GitLab instance — GitLab.com  — up to 12 times without any downtime. We use GitLab's own CI/CD platform to manage these deployments, which impact millions of developers worldwide. This deployment frequency serves as our primary quality gate and stress test. It also means our customers get access to new features within hours of development rather than waiting weeks or months. When organizations depend on GitLab for their DevOps workflows, they're using a platform that's proven at scale on our own infrastructure. In this article, you'll learn how we built an automated deployment pipeline using core GitLab CI/CD functionality to handle this deployment complexity. The business case for deployment velocity For GitLab: Our deployment frequency isn't just an engineering metric — it's a business imperative. Rapid deployment cycles mean we can respond to customer feedback within hours, ship security patches immediately, and validate new features in production before scaling them. For our customers: Every deployment to GitLab.com validates the deployment practices we recommend to our users. When you use GitLab's deployment features, you're using the same battle-tested approach that handles millions of git operations, CI/CD pipelines, and user interactions daily. You benefit from: Latest features available immediately: New capabilities reach you within hours of completion, not in quarterly release cycles Proven reliability at scale: If a feature works on GitLab.com, you can trust it in your environment Full value of GitLab: Zero-downtime deployments mean you never lose access to your DevOps platform, even during updates Real-world tested practices: Our deployment documentation isn't theory — it's exactly how we run the largest GitLab instance in existence Code flow architecture Our deployment pipeline follows a structured progression through multiple stages, each acting as a checkpoint on the journey from code proposal to production deployment. graph TD\n      A[Code Proposed] --> B[Merge Request Created]\n      B --> C[Pipeline Triggered]\n      C --> D[Build & Test]\n      D --> E{Spec/Integration/QA Tests Pass?}\n      E -->|No| F[Feedback Loop]\n      F --> B\n      E -->|Yes| G[Merge to default branch]\n      G -->|Periodically| H[Auto-Deploy Branch]\n\n      subgraph \"Deployment Pipeline\"\n          H --> I[Package Creation]\n          I --> K[Canary Environment]\n          K --> L[QA Validation]\n          L --> M[Main Environment]\n\n      end Deployment pipeline makeup Our deployment approach uses GitLab's native CI/CD capabilities to orchestrate complex deployments across hybrid infrastructure.\nHere's how we do it. Build Building GitLab is a complex topic in and of itself, so I'll go over the details at a high level. We build both our Omnibus package and our Cloud Native GitLab (CNG) images. The Omnibus packages deploy to our Gitaly fleet (our Git storage layer), while CNG images run all other components as containerized workloads. Other stateful services like Postgres and Redis have grown so large we have dedicated teams managing them separately. For GitLab.com, those systems are not deployed during our Auto-Deploy procedures. We have a scheduled pipeline that will regularly look at gitlab-org/gitlab and search for the most recent commit on the default branch with a successful (“green”) pipeline. Green pipelines signal that every component of GitLab has passed its comprehensive test suite. We then create an auto-deploy branch from that commit. This triggers a sequence of events: primarily, the need to build this package and all components that are a part of our monolith.\nAnother scheduled pipeline selects the latest built package and initiates the deployment pipeline. Procedurally, it looks this simple: graph LR\n      A[Create branch] --> B[Build]\n      B --> C[Choose Built package]\n      C --> D[Start Deploy Pipeline] Building takes some time, and since deployments can vary due to various circumstances, we choose the latest build to deploy. We technically build more versions of GitLab for .com than will ever be deployed. This enables us to always have a package lined up ready to go, and this brings us the closest we can be to having a full continuously delivered product for .com. Environment-based validation and Canary strategy Quality assurance (QA) isn't just an afterthought here — it's baked into every layer from development through deployment. Our QA process leverages automated test suites that include unit tests, integration tests, and end-to-end tests that simulate real user interactions with GitLab's features. But more importantly for our deployment pipeline, our QA process works hand-in-hand with our Canary strategy through environment-based validation. As part of our validation approach, we leverage GitLab's native Canary deployments , enabling controlled validation of changes with limited traffic exposure before full production deployment. We send roughly 5% of all traffic through our Canary stage . This approach increases the complexity of database migrations, but successfully navigating Canary deployments ensures we deploy a reliable product seamlessly. The Canary deployment features you use in GitLab were refined through managing one of the most complex deployment scenarios in production. When you implement Canary deployments for your applications, you're using patterns proven at massive scale. Our deployment process follows a progressive rollout strategy: Staging Canary: Initial validation environment Production Canary: Limited production traffic Staging Main: Full staging environment deployment Production Main: Full production rollout graph TD\n      C[Staging Canary Deploy]\n      C --> D[QA Smoke Main Stage Tests]\n      C --> E[QA Smoke Canary Stage Tests]\n      D --> F\n      E --> F{Tests Pass?}\n      F -->|Yes| G[Production Canary Deploy]\n      G --> S[QA Smoke Main Stage Tests]\n      G --> T[QA Smoke Canary Stage Tests]\n      F -->|No| H[Issue Creation]\n      H --> K[Fix & Backport]\n      K --> C\n\n      S --> M[Canary Traffic Monitoring]\n      T --> M[Canary Traffic Monitoring baking period]\n      M --> U[Production Safety Checks]\n      U --> N[Staging Main]\n      N --> V[Production Main] Our QA validation occurs at multiple checkpoints throughout this progressive deployment process: after each Canary deployment, and again after post-deploy migrations. This multilayered approach ensures that each phase of our deployment strategy has its own safety net. You can learn more about GitLab's comprehensive testing approach in our handbook. Deployment pipeline Here are the challenges we address across our deployment pipeline. Technical architecture considerations GitLab.com represents real-world deployment complexity at scale. As the largest known GitLab instance, deployments use our official GitLab Helm chart and the official Linux package — the same artifacts our customers use. You can learn more about the GitLab.com architecture in our handbook. This hybrid approach means our deployment pipeline must intelligently handle both containerized services and traditional Linux services in the same deployment cycle. Dogfooding at scale: We deploy using the same procedures we document for zero-downtime upgrades . If something doesn't work smoothly for us, we don't recommend it to customers. This self-imposed constraint drives continuous improvement in our deployment tooling. The following stages are run for all environment and stage upgrades: graph LR\n      a[prep] --> c[Regular Migrations - Canary stage only]\n      a --> f[Assets - Canary stage only]\n      c --> d[Gitaly]\n      d --> k8s\n\n      subgraph subGraph0[\"VM workloads\"]\n        d[\"Gitaly\"]\n      end\n\n      subgraph subGraph1[\"Kubernetes workloads\"]\n        k8s[\"k8s\"]\n      end\n\n      subgraph fleet[\"fleet\"]\n        subGraph0\n        subGraph1\n      end Stage details: Prep: Validates deployment readiness and performs pre-deployment checks Migrations: Executes database regular migrations. This only happens during the Canary stage. Because both Canary and Main stages share the same database, these changes are already available when the Main stage deploys, eliminating the need to repeat these tasks. Assets: We leverage a GCS bucket for all static assets. If any new assets are created, we upload these to our bucket such that they are immediately available to our Canary stage. As we leverage WebPack for assets, and properly leverage SHAs in the naming of our assets, we can confidently not worry that we override an older asset. Therefore, old assets continue to be available for older deployments and new assets are imemdiately made available when Canary begins its deploy. This only happens during the Canary stage deployment. Because Canary and Main stages share the same asset storage, these changes are already available when the Main stage deploys. Gitaly: Updates Gitaly Virtual Machine storage layer via our Omnibus Linux package on each Gitaly node. This service is unique as we bundle it with git . Therefore, we need to ensure that this service is capable of atomic upgrades. We leverage a wrapper around Gitaly , which enables us to install a newer version of Gitaly and make use of the library tableflip to cleanly rotate the running Gitaly, ensuring high availability of this service on each of our instances. Kubernetes: Deploys containerized GitLab components via our Helm chart. Note that we deploy to numerous clusters spread across Zones for redundancy, so these are usually broken into their own stages to minimize harm and sometimes allows us to stop mid-deploy if critical issues are detected. Multi-version compatibility: The hidden challenge As you read our process, you will notice that there's a period of time where our database schema is ahead of the code that the Main stage knows about. This happens because the Canary stage has already deployed new code and runs regular database migrations, but the Main stage is still running the previous version of the code that doesn't know about these new database changes. Real-world example: Imagine we're adding a new merge_readiness field to merge requests. During deployment, some servers are running code that expects this field. while others don't know it exists yet. If we handle this poorly, we break GitLab.com for millions of users. If we handle it well, nobody notices anything happened. This occurs with most other services, as well. For example, if a client sends multiple requests, there's a chance one of them might land in our Canary stage; other requests might be directed to the Main stage. This is not too different from a deploy as it does take a decent amount of time to roll through the few thousand Pods that run our services. With a few exceptions, the vast majority of our services will run a slightly newer version of that component in Canary for a period of time. In a sense, these scenarios are all transient states. But they can often persist for several hours or days in a live, production environment. Therefore, we must treat them with the same care as permanent states. During any deployment, we have multiple versions of GitLab running simultaneously and they all need to play nicely together. Database operations Database migrations present a unique challenge in our Canary deployment model. We need schema changes to support new features while maintaining our ability to roll back if issues arise. Our solution involves careful separation of concerns: Regular migrations: Run during the Canary stage, designed to be backward-compatible, consists of only reversible changes Post-deploy migrations: The \"point of no return\" migrations that happen only after multiple successful deployments Database changes are handled with precision and extensive validation procedures: graph LR\n      A[Regular Migrations] --> B[Canary Stage Deploy]\n      B --> C[Main Stage Deploy]\n      C --> D[Post Deploy Migrations] Post-deploy migrations GitLab deployments involve many components. Updating GitLab is not atomic, so many components must be backward-compatible. Post-deploy migrations often contain changes that can't be easily rolled back — think data transformations, column drops, or structural changes that would break older code versions. By running them after we've gained confidence through multiple successful deployments, we ensure: The new code is stable and we're unlikely to need a rollback Performance characteristics are well understood in production Any edge cases have been discovered and addressed The blast radius is minimized if something does go wrong This approach provides the optimal balance: enabling rapid feature deployment through Canary releases while maintaining rollback capabilities until we have high confidence in deployment stability. The expand-migrate-contract pattern: Our database, frontend, and application compatibility changes follow a carefully orchestrated three-phase approach. Expand: Add new structures (columns, indexes) while keeping old ones functional Migrate: Deploy new application code that uses the new structures Contract: Remove old structures in post-deploy migrations after everything is stable Real-world example: When adding a new merge_readiness column to merge requests: Expand: Add the new column with a default value; existing code ignores it Migrate: Deploy code that reads and writes to the new column while still supporting the old approach 3 Contract: After several successful deployments, remove the old column in a post-deploy migration All database operations, application code, frontend code, and more, are subject to a set of guidelines that Engineering must adhere to, which can be found in our Multi-Version Compatibility documentation . Results and impact Our deployment infrastructure delivers measurable benefits: For GitLab Up to 12 deployments daily to GitLab.com Zero-downtime deployments serving millions of developers Security patches can reach production within hours, not days New features validated in production at massive scale before general availability For customers Proven deployment patterns you can adopt for your own applications Features battle-tested on the world's largest GitLab instance before reaching your environment Documentation that reflects actual production practices, not theoretical best practices Confidence that GitLab's recommended upgrade procedures work at any scale Key takeaways for engineering teams GitLab's deployment pipeline represents a sophisticated system that balances deployment velocity with operational reliability. The progressive deployment model, comprehensive testing integration, and robust rollback capabilities provide a foundation for reliable software delivery at scale. For engineering teams implementing similar systems, key considerations include: Automated testing: Comprehensive test coverage throughout the deployment pipeline Progressive rollout: Staged deployments to minimize risk and enable rapid recovery Monitoring integration: Comprehensive observability across all deployment stages Incident response: Rapid detection and resolution capabilities for deployment issues GitLab's architecture demonstrates how modern CI/CD systems can manage the complexity of large-scale deployments while maintaining the velocity required for competitive software development. Important note on scope This article specifically covers the deployment pipeline for services that are part of the GitLab Omnibus package and Helm chart — essentially the core GitLab monolith and its tightly integrated components. However, GitLab's infrastructure landscape extends beyond what's described here. Other services, notably our AI services and services that might be in a proof of concept state , follow a different deployment approach using our internal platform called Runway. If you're working with or curious about these other services, you can find more information in the Runway documentation . Other offerings, such as GitLab Dedicated are deployed more in alignment with what we expect customers to be capable of performing themselves by way of the GitLab Environment Toolkit . If you'd like to learn more, check out the GitLab Environment Toolkit project . The deployment strategies, architectural considerations, and pipeline complexities outlined in this article represent the battle-tested approach we use for our core platform — but like any large engineering organization, we have multiple deployment strategies tailored to different service types and maturity levels. Further documentation about Auto-Deploy and our procedures can be found at the below links: Engineering Deployments Release Procedural Documentation More resources How we decreased GitLab repo backup times from 48 hours to 41 minutes How we supercharged GitLab CI statuses with WebSockets How we reduced MR review time with Value Stream Management",
      "published_ts": 1764547200,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    }
  ],
  "hors_sujet": [],
  "lake_storage_formats": [
    {
      "url": "https://www.databricks.com/blog/expensive-delta-lake-s3-storage-mistakes-and-how-fix-them",
      "title": "Expensive Delta Lake S3 Storage Mistakes (And How to Fix Them)",
      "summary": "1. Introduction: The FoundationCloud object storage, such as S3, is the foundation...",
      "published_ts": 1764967200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    }
  ],
  "news": [
    {
      "url": "https://blog.octo.com/sopo-2025-from-0-to-ai--product-marketing-lessons-from-launching-create-with-ai-dominique-rolink",
      "title": "[SoPo 2025] From 0 to AI : Product Marketing lessons from launching Create with AI - Dominique Rolink",
      "summary": "Comment positionner l’IA comme un outil accessible plutôt qu’un saut dans l’inconnu ?Dominique Rolink PMM chez Miro nous donne quelques clés pour encourager l'adoption d'un produit IA.",
      "published_ts": 1765135390,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://joereis.substack.com/p/the-hangover-40-avoided-aws-reinvent",
      "title": "The Hangover 4.0 (Avoided): AWS re:Invent, The \"Year of the Agent,\" Mixed Model Arts, and the Ensh*tification of Everything",
      "summary": "The Weekend Windup #13 - Cool Reads, Events, and More",
      "published_ts": 1764986504,
      "source_name": "Joe Reis · Data Engineering",
      "content_type": "rex"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/ai-augmenting-decision-making",
      "title": "AI Isn't Taking Over, It's Augmenting Decision-Making",
      "summary": "It's a Saturday afternoon, and I'm at a kid's birthday party. The obligatory small talk is in full swing, and after discussing whose kid belongs to whom and comparing notes on cupcake flavors, I hear the inevitable question, \"So, what do you do?\"",
      "published_ts": 1764952290,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/ai-literacy-chro",
      "title": "AI Literacy: CHRO’s Strategic Lever for Talent Transformation",
      "summary": "AI (including Generative AI) is reshaping the workplace, and the speed of these changes demands that companies equip their workforce to navigate and leverage this new technology. For organizations to stay competitive and agile, employees need to be more than just users of AI — they need to understand how it works and how it can amplify their roles.",
      "published_ts": 1764900393,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/databricks-wins-seven-2025-aws-partner-year-awards",
      "title": "Databricks Wins Seven 2025 AWS Partner of the Year Awards",
      "summary": "We’re excited to announce that Databricks has received a record number of honors...",
      "published_ts": 1764897000,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://joereis.substack.com/p/help-shape-the-practical-data-2026",
      "title": "Help shape the Practical Data 2026 State of Data Engineering Report.",
      "summary": "And it only takes 2-3 minutes of your time!",
      "published_ts": 1764869376,
      "source_name": "Joe Reis · Data Engineering",
      "content_type": "rex"
    },
    {
      "url": "https://dataengineerthings.substack.com/p/data-engineer-things-newsletter-26",
      "title": "Data Engineer Things Newsletter #26 (Dec 2025)",
      "summary": "Thinking like a Data engineer, Real-time distributed graph, Secret behind super fast databases, Multimodal data workloads, and more.",
      "published_ts": 1764864163,
      "source_name": "Data Engineer Things",
      "content_type": "rex"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/data-ai-predictions-2026",
      "title": "Snowflake Data + AI Predictions 2026: AI Agents Take the Lead",
      "summary": "From longer context windows to better memory and human-AI collaboration, here’s how Data and AI will reshape work and decision-making in 2026.",
      "published_ts": 1764712320,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/a-decade-of-open-innovation-celebrating-10-years-of-microsoft-and-red-hat-partnership/",
      "title": "A decade of open innovation: Celebrating 10 years of Microsoft and Red Hat partnership",
      "summary": "A decade-long partnership between Microsoft and Red Hat has redefined what open innovation means, empowering customers with flexible, secure, and scalable solutions across the cloud. The post A decade of open innovation: Celebrating 10 years of Microsoft and Red Hat partnership appeared first on Microsoft Azure Blog .",
      "published_ts": 1764691200,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/databricks-neurips-2025",
      "title": "Databricks at NeurIPS 2025",
      "summary": "Databricks is proud to be a platinum sponsor of NeurIPS 2025. The conference runs...",
      "published_ts": 1764601200,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/why-replicate-joining-cloudflare/",
      "title": "Why Replicate is joining Cloudflare",
      "summary": "Today, we’re excited to announce that Replicate is officially part of Cloudflare. We wanted to share a bit about our journey and why we made this decision.",
      "published_ts": 1764568800,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    }
  ],
  "news_general": [],
  "python_analytics": [],
  "warehouses_engines": [
    {
      "url": "https://www.databricks.com/blog/bps-geospatial-ai-engine-transforming-safety-and-operations-databricks",
      "title": "BP’s Geospatial AI Engine: Transforming Safety and Operations with Databricks",
      "summary": "The integration of DATABRICKS capabilities with geospatial technology marks a significant...",
      "published_ts": 1764889500,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.uber.com/blog/improving-mysql-cluster-uptime-part2/",
      "title": "Improving MySQL® Cluster Uptime: Making MGR Viable at Scale",
      "summary": "Dive into the implementation, automation and failover logic that made MySQL® Group Replication viable at Uber scale.",
      "published_ts": 1764856800,
      "source_name": "Uber Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://stripe.com/blog/analyzing-how-saas-platforms-are-shipping-payments-and-finance-products-in-days",
      "title": "Analyzing how SaaS platforms are shipping payments and finance products in days",
      "summary": "Leading SaaS platforms across industries—from Squarespace to Jobber—are using embedded components to ship payments and finance features with minimal code. We analyzed the data to see which types of platforms are getting the most value.",
      "published_ts": 1764806400,
      "source_name": "Stripe Engineering Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/snowflake-aws-enterprise-data-ai-adoption",
      "title": "Snowflake and AWS: Accelerating Enterprise Data and AI Adoption",
      "summary": "Together with AWS, we’re excited to build an open, connected and secure foundation that turns data into intelligence and intelligence into action.",
      "published_ts": 1764753000,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/startup-spotlight-euno",
      "title": "Startup Spotlight: Euno on Scalable Self-Service Analytics",
      "summary": "Discover how Euno helps data teams scale self-service analytics and governance in this Snowflake Startup Spotlight with Co-founder and CEO Sarah Levy.",
      "published_ts": 1764611217,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    }
  ]
}