# Architecture Technique - Veille Tech Crawling

*Document gÃ©nÃ©rÃ© automatiquement par analyse du code - Date : 2025-12-20*

## 1. Vue d'Ensemble

**Type de projet :** Full-Stack Web Application
**Stack principale :** Python (Backend) + React + TypeScript (Frontend)

SystÃ¨me automatisÃ© de veille technologique pour Data Engineers qui crawle 60+ sources RSS, classifie les articles via LLM, score la pertinence par embeddings sÃ©mantiques, et gÃ©nÃ¨re des rÃ©sumÃ©s hebdomadaires publiÃ©s sur une interface web moderne.

**Architecture Globale :**
```
GitHub Actions (Lundi 06:00 UTC)
  â†“
Backend Python Pipeline (4 Ã©tapes sÃ©quentielles)
  â”œâ”€â”€ 1. Crawling RSS + extraction contenu
  â”œâ”€â”€ 2. Classification LLM (Groq)
  â”œâ”€â”€ 3. Scoring pertinence (embeddings)
  â””â”€â”€ 4. GÃ©nÃ©ration rÃ©sumÃ© LLM
  â†“
Export JSON/Markdown â†’ Git commit + push
  â†“
Frontend React Build (Vite)
  â†“
GitHub Pages Deploy
```

---

## 2. Stack Technique DÃ©taillÃ©e

### Backend Python

#### **Runtime & Core**
- **Python** : 3.11+ (asyncio natif)
- **Package Manager** : pip + venv
- **Virtual Env** : `.venv/` (gitignored)

#### **Crawling & HTTP**
- **aiohttp** : Client HTTP asynchrone
- **aiolimiter** : Rate limiting per-host (1.0 req/sec)
- **feedparser** : Parsing RSS/Atom
- **BeautifulSoup4** : HTML parsing
- **html5lib** : HTML5 parser
- **lxml** : XML/HTML parser (backend BeautifulSoup)
- **readability-lxml** : Extraction contenu article

#### **Intelligence Artificielle**
- **openai** : Client OpenAI-compatible (utilisÃ© avec Groq)
  - Base URL: `https://api.groq.com/openai/v1`
  - Model: `llama-3.1-8b-instant`
- **sentence-transformers** : Embeddings sÃ©mantiques
  - Model local: `all-MiniLM-L6-v2` (384 dimensions)
- **scikit-learn** : Calcul similaritÃ© cosine

#### **Data & Storage**
- **SQLite 3** : Base de donnÃ©es locale (veille.db)
- **Pydantic** : Validation configuration YAML

#### **Utils & CLI**
- **python-dotenv** : Chargement variables .env
- **pyyaml** : Parsing config.yaml
- **tqdm** : Barres de progression CLI

#### **API (Optionnelle)**
- **FastAPI** : REST API development (api_server.py)
- **uvicorn** : ASGI server

#### **Testing**
- **pytest** : Framework de tests
- **pytest-asyncio** : Support tests async
- **pytest-cov** : Coverage reporting

---

### Frontend React

#### **Framework & Build**
- **React** : 19.2.0 (avec Strict Mode)
- **react-dom** : 19.2.0
- **Vite** : 7.2.2 (build tool ultra-rapide)
- **@vitejs/plugin-react** : 5.1.0 (HMR + Fast Refresh)
- **TypeScript** : 5.9.3 (strict mode activÃ©)

#### **Styling**
- **Tailwind CSS** : 3.4.13 (utility-first CSS)
- **@tailwindcss/typography** : 0.5.19 (prose styling pour markdown)
- **autoprefixer** : 10.4.20
- **postcss** : 8.4.33

#### **Features**
- **Fuse.js** : 7.1.0 (fuzzy search)
- **marked** : 17.0.0 (markdown parser + renderer)

#### **Development Tools**
- **ESLint** : 9.39.1
  - **@eslint/js** : 9.39.1
  - **eslint-plugin-react-hooks** : 5.2.0 (rules React hooks)
  - **eslint-plugin-react-refresh** : 0.4.24
  - **typescript-eslint** : 8.46.3
- **globals** : 16.5.0

#### **Types**
- **@types/react** : 19.2.2
- **@types/react-dom** : 19.2.2
- **@types/node** : 24.10.0

---

### Infrastructure & DevOps

#### **DÃ©ploiement**
- **GitHub Actions** : CI/CD automatique
  - Workflow backend: `.github/workflows/backend-weekly.yml` (Lundi 06:00 UTC)
  - Workflow frontend: `.github/workflows/deploy-frontend.yml` (on push main)
- **GitHub Pages** : Hosting statique frontend
  - Base path: `/veille/`
  - Source: GitHub Actions

#### **Monitoring & Observability**
- **Logging** : Python logging + logger.py custom (fichier `logs/veille_tech.log`)
- **MÃ©triques** : MetricsCollector custom (feeds_processed, articles_crawled, errors)
- âš ï¸ **Monitoring externe** : Aucun (Sentry, Datadog, etc. - manquant)

#### **CI/CD**
- âœ… GitHub Actions backend (weekly cron)
- âœ… GitHub Actions frontend (deploy on push)
- âŒ Tests automatiques en CI (non configurÃ©)
- âŒ Staging environment (non existant)

---

## 3. Structure du Projet

```
veille_tech_crawling/
â”œâ”€â”€ backend/                      # Backend Python
â”‚   â”œâ”€â”€ .venv/                    # Virtual environment (gitignored)
â”‚   â”œâ”€â”€ config.yaml               # Configuration centrale (60+ sources)
â”‚   â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”‚   â”œâ”€â”€ pytest.ini               # Configuration pytest
â”‚   â”œâ”€â”€ .env                     # Variables d'environnement (gitignored)
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py                  # ğŸ¯ Orchestrateur pipeline (37 lignes)
â”‚   â”œâ”€â”€ veille_tech.py           # ğŸ“¡ Crawling + extraction (668 lignes)
â”‚   â”œâ”€â”€ classify_llm.py          # ğŸ¤– Classification LLM (248 lignes)
â”‚   â”œâ”€â”€ analyze_relevance.py     # ğŸ“Š Scoring pertinence (581 lignes)
â”‚   â”œâ”€â”€ summarize_week_llm.py    # ğŸ“ RÃ©sumÃ© hebdomadaire (374 lignes)
â”‚   â”œâ”€â”€ content_classifier.py    # ğŸ¯ Type contenu + filtrage (378 lignes)
â”‚   â”œâ”€â”€ logger.py                # ğŸ“‹ Logging structurÃ© (112 lignes)
â”‚   â”œâ”€â”€ api_server.py            # ğŸŒ API REST optionnelle (179 lignes)
â”‚   â”‚
â”‚   â”œâ”€â”€ test_veille_tech.py      # âœ… Tests veille_tech (312 lignes)
â”‚   â”œâ”€â”€ test_content_classifier.py # âœ… Tests content_classifier (228 lignes)
â”‚   â”‚
â”‚   â”œâ”€â”€ regenerate_weeks.py      # ğŸ”„ Utilitaire regÃ©nÃ©ration
â”‚   â”œâ”€â”€ write_week_selection.py  # ğŸ“„ Utilitaire Ã©criture sÃ©lection
â”‚   â”œâ”€â”€ generate_summary_from_selection.py
â”‚   â”œâ”€â”€ convert_to_summary_json.py
â”‚   â”œâ”€â”€ broken_sources_cleanup.py
â”‚   â”œâ”€â”€ check_alternate_urls.py
â”‚   â”œâ”€â”€ fix_sources.py
â”‚   â”œâ”€â”€ migrate_add_level_fields.py
â”‚   â”œâ”€â”€ reclassify_tech_levels.py
â”‚   â”‚
â”‚   â”œâ”€â”€ veille.db                # ğŸ—„ï¸ Base SQLite (gitignored)
â”‚   â”œâ”€â”€ logs/                    # ğŸ“‹ Logs d'exÃ©cution
â”‚   â””â”€â”€ scripts/                 # ğŸ› ï¸ Scripts shell
â”‚
â”œâ”€â”€ frontend/                    # Frontend React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/          # Composants React
â”‚   â”‚   â”‚   â”œâ”€â”€ App.tsx          # Composant principal (221 lignes)
â”‚   â”‚   â”‚   â”œâ”€â”€ Hero.tsx         # Header + sÃ©lecteur semaine
â”‚   â”‚   â”‚   â”œâ”€â”€ Overview.tsx     # AperÃ§u markdown
â”‚   â”‚   â”‚   â”œâ”€â”€ ContentTypeTabs.tsx # Onglets Technical/REX
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchBar.tsx    # Barre de recherche
â”‚   â”‚   â”‚   â”œâ”€â”€ CategoryFilter.tsx # Filtres catÃ©gories
â”‚   â”‚   â”‚   â”œâ”€â”€ SectionCard.tsx  # Conteneur catÃ©gorie
â”‚   â”‚   â”‚   â”œâ”€â”€ ArticleCard.tsx  # Carte article
â”‚   â”‚   â”‚   â”œâ”€â”€ Top3.tsx        # Top 3 articles
â”‚   â”‚   â”‚   â””â”€â”€ Chip.tsx        # Badge rÃ©utilisable
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â”œâ”€â”€ parse.ts         # Parsing exports JSON/MD
â”‚   â”‚   â”‚   â””â”€â”€ search.ts        # Logique recherche Fuse.js
â”‚   â”‚   â”œâ”€â”€ main.tsx            # Entry point React
â”‚   â”‚   â””â”€â”€ index.css           # Styles Tailwind globaux
â”‚   â”‚
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ export/             # DonnÃ©es copiÃ©es (build)
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ copy-export.js      # Copie backend/export â†’ public/export
â”‚   â”‚
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ vite.config.ts          # Config Vite (base: /veille/)
â”‚   â”œâ”€â”€ tsconfig.json           # TypeScript config
â”‚   â”œâ”€â”€ tsconfig.app.json
â”‚   â”œâ”€â”€ tsconfig.node.json
â”‚   â”œâ”€â”€ tailwind.config.js      # Tailwind config
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ eslint.config.js        # ESLint config
â”‚
â”œâ”€â”€ export/                     # ğŸ“¤ Exports hebdomadaires
â”‚   â”œâ”€â”€ categories.json         # Mapping category_key â†’ title
â”‚   â”œâ”€â”€ weeks.json             # Index semaines + mÃ©tadonnÃ©es
â”‚   â”œâ”€â”€ search.json            # Index recherche plat
â”‚   â”œâ”€â”€ latest â†’ 2025w51       # Symlink semaine courante
â”‚   â””â”€â”€ 2025w51/               # Semaine ISO
â”‚       â”œâ”€â”€ digest.json        # Format frontend complet
â”‚       â”œâ”€â”€ ai_selection.json  # Articles filtrÃ©s
â”‚       â”œâ”€â”€ ai_selection.md
â”‚       â”œâ”€â”€ ai_summary.md
â”‚       â”œâ”€â”€ top3.json
â”‚       â”œâ”€â”€ top3.md
â”‚       â””â”€â”€ range.txt          # Plage dates
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ backend-weekly.yml  # Crawl Monday 06:00 UTC
â”‚       â””â”€â”€ deploy-frontend.yml # Deploy GitHub Pages
â”‚
â”œâ”€â”€ .claude/
â”‚   â””â”€â”€ commands/              # Commandes slash Claude Code
â”‚       â”œâ”€â”€ epct.md
â”‚       â””â”€â”€ reverse-engineer.md
â”‚
â”œâ”€â”€ README.md                  # Documentation utilisateur
â”œâ”€â”€ CLAUDE.md                  # Guide Claude Code
â””â”€â”€ .gitignore
```

---

## 4. Architecture Applicative

### Pattern Architectural : **Pipeline SÃ©quentiel**

Le backend suit un pattern de pipeline ETL avec 4 Ã©tapes indÃ©pendantes orchestrÃ©es par `main.py` :

```
main.py (subprocess.run orchestration)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: CRAWLING & EXTRACTION              â”‚
â”‚ veille_tech.py                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Fetch 60+ RSS/Atom feeds (async)          â”‚
â”‚ â€¢ Auto-dÃ©couverte feeds (HTML parsing)      â”‚
â”‚ â€¢ Respect robots.txt + rate limiting        â”‚
â”‚ â€¢ Extract article content (readability)     â”‚
â”‚ â€¢ Filter par path regex + min length        â”‚
â”‚ â€¢ Classify par keywords (initial)           â”‚
â”‚ â€¢ Detect content_type (technical/rex)       â”‚
â”‚ â€¢ Calculate tech_level + marketing_score    â”‚
â”‚ â€¢ Deduplication (hash URL+title)            â”‚
â”‚ â€¢ Store in SQLite (upsert)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: LLM CLASSIFICATION                 â”‚
â”‚ classify_llm.py                             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Read uncategorized articles                â”‚
â”‚ â€¢ Call Groq API (llama-3.1-8b-instant)      â”‚
â”‚ â€¢ Prompt with 8 categories description      â”‚
â”‚ â€¢ Parse JSON response {category, confidence}â”‚
â”‚ â€¢ Update DB (category_key, llm_classified)  â”‚
â”‚ â€¢ Rate limit: 30 req/min                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: RELEVANCE SCORING & SELECTION      â”‚
â”‚ analyze_relevance.py                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Load all articles of week                  â”‚
â”‚ â€¢ Calculate components:                      â”‚
â”‚   - semantic_score (55%): embeddings cosine  â”‚
â”‚   - source_weight (20%): reputation          â”‚
â”‚   - quality_score (15%): length + code       â”‚
â”‚   - tech_score (10%): keywords               â”‚
â”‚ â€¢ Combine â†’ final_score [0-100]             â”‚
â”‚ â€¢ Filter by threshold (per category)         â”‚
â”‚ â€¢ Diversity filter (max 2 per source/cat)   â”‚
â”‚ â€¢ Export: ai_selection.json + top3.json     â”‚
â”‚ â€¢ Create indexes: categories, weeks, search â”‚
â”‚ â€¢ Symlink export/latest                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: WEEKLY SUMMARY GENERATION          â”‚
â”‚ summarize_week_llm.py                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Read ai_selection.json                    â”‚
â”‚ â€¢ Build markdown context (grouped by cat)   â”‚
â”‚ â€¢ Call Groq LLM with structured prompt      â”‚
â”‚ â€¢ Parse response (overview + sections)       â”‚
â”‚ â€¢ Post-process markdown (normalize)         â”‚
â”‚ â€¢ Export: digest.json + digest.md           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Git commit + push â†’ Trigger frontend deploy
```

### Flux de DonnÃ©es Complet

```
RSS Feeds (60+ sources config.yaml)
  â†“
veille_tech.py â†’ SQLite items table
  â†“
classify_llm.py â†’ Update category_key
  â†“
analyze_relevance.py â†’ Calculate scores + Filter
  â†“
summarize_week_llm.py â†’ Generate digest
  â†“
export/<YYYYwWW>/digest.json
  â†“
Frontend React â†’ Load + Display
  â†“
User (GitHub Pages)
```

---

## 5. Base de DonnÃ©es

### SQLite Schema (`veille.db`)

#### **Table `items`**

**Colonnes principales (crÃ©ation initiale) :**
```sql
CREATE TABLE items (
  id TEXT PRIMARY KEY,              -- sha256(url + "||" + title)
  url TEXT NOT NULL,                -- URL article
  title TEXT NOT NULL,              -- Titre article
  summary TEXT,                     -- RÃ©sumÃ© court (feed RSS)
  content TEXT,                     -- Contenu complet extrait
  published_ts INTEGER,             -- Timestamp publication (UTC)
  source_name TEXT,                 -- Nom flux RSS
  category_key TEXT,                -- ClÃ© catÃ©gorie (ex: "warehouses_engines")
  created_ts INTEGER NOT NULL       -- Timestamp crawl (UTC)
);

CREATE INDEX idx_items_cat_pub
  ON items(category_key, published_ts DESC);
```

**Colonnes ajoutÃ©es par migrations :**

```sql
-- classify_llm.py:
llm_classified INTEGER DEFAULT 0,   -- 0=non traitÃ©, 1=classÃ© LLM
original_category_key TEXT,         -- CatÃ©gorie avant LLM

-- analyze_relevance.py:
semantic_score REAL,                -- Score embeddings (0-100)
source_weight REAL,                 -- Poids source (0-100)
quality_score REAL,                 -- Score qualitÃ© (0-100)
tech_score REAL,                    -- Score tech keywords (0-100)
final_score INTEGER,                -- Score final (0-100)

-- veille_tech.py + content_classifier.py (Phase 1):
content_type TEXT DEFAULT 'technical', -- "technical" | "rex"
tech_level TEXT DEFAULT 'intermediate', -- "beginner" | "intermediate" | "advanced"
marketing_score INTEGER DEFAULT 0,  -- Score marketing (0-100)
is_excluded INTEGER DEFAULT 0,      -- Flag exclusion anti-bruit
exclusion_reason TEXT,              -- "beginner_content" | "promotional_content" | ...

-- Legacy/debug:
llm_score INTEGER,                  -- Score LLM (deprecated)
llm_notes TEXT                      -- Notes LLM debug
```

**Index supplÃ©mentaires :**
```sql
CREATE INDEX idx_items_content_type ON items(content_type);
CREATE INDEX idx_items_final_score ON items(final_score DESC);
```

#### **Statistiques Typiques**

- **Taille DB** : ~50-100 MB (aprÃ¨s plusieurs mois)
- **Articles par semaine** : ~500-1000 crawlÃ©s, ~50-100 sÃ©lectionnÃ©s
- **Retention** : IndÃ©finie (pas de purge automatique)

---

## 6. API & Endpoints

### API FastAPI (Optionnelle - Development)

**Fichier** : `api_server.py` (179 lignes)

**Endpoints** :

| MÃ©thode | Path | Description |
|---------|------|-------------|
| GET | `/api/weeks` | Liste toutes les semaines disponibles |
| GET | `/api/week/{week}/sections` | Articles par catÃ©gorie pour une semaine |
| GET | `/api/week/{week}/top3` | Top 3 articles de la semaine |
| GET | `/api/week/latest` | Semaine courante (via symlink) |

**CORS** :
- Autorise : `http://localhost:5173` (dev)
- Autorise : `https://*.github.io` (GitHub Pages)

**Note Importante** :
L'API n'est **pas utilisÃ©e en production**. Le frontend consomme directement les fichiers statiques JSON depuis `export/`.

---

## 7. Standards de Code

### Backend Python

**Conventions :**
- **Modules** : `snake_case.py`
- **Fonctions** : `snake_case()`
- **Classes** : `PascalCase`
- **Constantes** : `UPPER_CASE`
- **Type hints** : UtilisÃ©s partout (Python 3.11+)
- **Docstrings** : FranÃ§ais, format simple
- **Imports** : GroupÃ©s (stdlib â†’ tiers â†’ local)

**Exemple :**
```python
from typing import List, Optional
import asyncio

def week_bounds(
    tz_name: str = "Europe/Paris",
    week_offset: int = 0
) -> tuple[int, int, str, str, str]:
    """Calcule les bornes de la semaine ISO.

    Returns:
        (start_ts_utc, end_ts_utc, week_label, start_str, end_str)
    """
    ...
```

**Logging :**
```python
logger.info("Processing feed", feed=feed_name, articles=count)
logger.error("Failed to fetch", url=url, error=str(e))
```

**Error Handling :**
- Try/except dans Fetcher et async calls
- Graceful degradation (errors logged, pas de crash)

---

### Frontend React/TypeScript

**Conventions :**
- **Composants** : `PascalCase.tsx`
- **Utils** : `camelCase.ts`
- **Types** : `PascalCase` (interfaces/types)
- **Props** : Interface avec suffix `Props` ou inline
- **Hooks** : Prefix `use` (React convention)

**Style :**
- Tailwind utility-first
- Responsive : mobile-first (`sm:`, `md:` breakpoints)
- Semantic HTML (`<header>`, `<main>`, `<section>`, `<article>`)
- Accessibility : `aria-*`, `alt`, `htmlFor`, focus rings

**TypeScript** :
```typescript
interface ArticleCardProps {
  title: string;
  url?: string;
  source?: string;
  date?: string;
  score?: number | string;
  tech_level?: 'beginner' | 'intermediate' | 'advanced';
  marketing_score?: number;
  className?: string;
}

export function ArticleCard({ title, url, source, ... }: ArticleCardProps) {
  ...
}
```

**React Patterns :**
- Functional components (pas de class)
- Hooks : `useState`, `useEffect`, `useMemo`
- Props drilling (pas de Context/Redux pour ce projet)
- Conditional rendering : `{condition && <Component />}`

---

## 8. SÃ©curitÃ©

### ImplÃ©mentÃ© âœ…

- [x] **Variables d'environnement** : API keys via `.env` (gitignored)
- [x] **robots.txt respect** : RobotsCache avec urllib.robotparser
- [x] **Rate limiting** : AsyncLimiter per-host (1.0 req/sec)
- [x] **User-Agent** : Custom UA avec URL projet
- [x] **Deduplication** : Hash (URL + titre) pour Ã©viter duplicates
- [x] **HTTPS upgrade** : Auto-upgrade HTTP â†’ HTTPS
- [x] **Sanitization HTML** : readability + BeautifulSoup
- [x] **TypeScript strict** : Typage strict frontend
- [x] **CORS** : ConfigurÃ© pour localhost + GitHub Pages

### Manquant âš ï¸

- [ ] **Secrets hardcodÃ©s** : âŒ Aucun secret dÃ©tectÃ© en dur (bonne pratique)
- [ ] **Input validation** : âš ï¸ Validation Pydantic config.yaml uniquement
- [ ] **Rate limiting API** : âŒ Pas de rate limit sur api_server.py (mais non prod)
- [ ] **HTTPS enforcement** : âŒ Pas de redirection forcÃ©e
- [ ] **Content Security Policy** : âŒ Aucun header CSP
- [ ] **Dependency scanning** : âŒ Pas de Dependabot/Snyk
- [ ] **Secrets scanning** : âŒ Pas de GitHub Secret Scanning

**Recommandations P1 :**
1. Activer Dependabot (GitHub) pour scans CVE
2. Ajouter CSP headers si API dÃ©ployÃ©e en prod
3. Audit sÃ©curitÃ© OWASP Top 10

---

## 9. Performance

### Optimisations ImplÃ©mentÃ©es âœ…

**Backend :**
- [x] **Asyncio** : Crawling async (8 feeds parallÃ¨les)
- [x] **Connection pooling** : aiohttp TCPConnector
- [x] **Rate limiting intelligent** : Per-host (Ã©vite ban)
- [x] **Embeddings caching** : Global `_model_semantic`, `_profile_embedding`
- [x] **SQLite indexing** : Index sur (category_key, published_ts)
- [x] **Batch processing** : Classif LLM par batch async

**Frontend :**
- [x] **Vite build** : Rollup optimisÃ© + code splitting
- [x] **useMemo** : Filtrage + parsing memoized
- [x] **Lazy loading** : Images favicons `loading="lazy"`
- [x] **Static files** : Pas d'API calls (JSON statiques)
- [x] **Tailwind purge** : CSS unused purgÃ© au build

### ProblÃ¨mes IdentifiÃ©s âš ï¸

**Backend :**
- âš ï¸ **Embeddings recalcul** : Pas de cache Redis (recalculÃ© chaque run)
- âš ï¸ **LLM calls sÃ©quentiels** : 30 req/min max (Groq limite)
- âš ï¸ **SQLite contention** : Pas de WAL mode (single-writer)

**Frontend :**
- âš ï¸ **Large JSON files** : digest.json peut Ãªtre > 500 KB
- âš ï¸ **No virtualization** : Toutes les cartes rendues (pas de react-window)
- âš ï¸ **No pagination** : Toutes les semaines chargÃ©es d'un coup

**Recommandations P2 :**
1. Redis cache pour embeddings (Ã©vite recalcul)
2. SQLite WAL mode pour meilleure concurrence
3. Pagination frontend (react-window pour long lists)
4. Compress JSON exports (gzip)

---

## 10. Tests

### Configuration Pytest

**Fichier** : `pytest.ini`

```ini
[pytest]
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts = -v --tb=short --strict-markers --disable-warnings

markers =
    slow: tests lents
    integration: tests d'intÃ©gration
    unit: tests unitaires

asyncio_mode = auto
```

### Tests Existants

**Fichiers de tests :**
- `test_veille_tech.py` (312 lignes) - 20+ tests
- `test_content_classifier.py` (228 lignes) - 17 tests

**Total : 37 tests**

**Coverage :**
```bash
pytest --cov=. --cov-report=html
```
- âš ï¸ Coverage actuel : **Non mesurÃ© rÃ©guliÃ¨rement**
- ğŸ¯ Coverage cible : **> 80%**

**Marqueurs utilisÃ©s :**
- `@pytest.mark.unit` : Tests rapides (< 1s)
- `@pytest.mark.integration` : Tests avec I/O externe
- `@pytest.mark.slow` : Tests long-running (> 10s)

**Ce qui est testÃ© :**
- âœ… `classify()` : Classification par keywords
- âœ… `week_bounds()` : Calcul semaines ISO
- âœ… `normalize_ts()` : Normalisation timestamps
- âœ… `detect_content_type()` : DÃ©tection technical/rex
- âœ… `calculate_marketing_score()` : Score marketing
- âœ… `calculate_technical_level()` : Niveau technique

**Ce qui manque :**
- âŒ Tests LLM calls (mocking Groq API)
- âŒ Tests embeddings (mocking sentence-transformers)
- âŒ Tests end-to-end pipeline complet
- âŒ Tests frontend (Vitest/Jest)
- âŒ Tests E2E (Playwright)

---

## 11. Dette Technique IdentifiÃ©e

### P0 - Critique (Ã€ corriger immÃ©diatement)

**Aucune dette critique dÃ©tectÃ©e** âœ…

Le code est globalement de bonne qualitÃ©, bien structurÃ© et documentÃ©.

### P1 - Haute (Ã€ corriger sous 1-2 sprints)

#### [DEBT-001] Absence de tests frontend
**Impact :** Risque de rÃ©gressions UI non dÃ©tectÃ©es
**Estimation :** 13 SP
**Action :**
- Setup Vitest
- Tests composants critiques (App, SearchBar, CategoryFilter)
- Tests E2E Playwright (search + filter flows)

#### [DEBT-002] Pas de CI/CD pour tests
**Impact :** Tests manuels uniquement
**Estimation :** 5 SP
**Action :**
- Ajouter step pytest dans `.github/workflows/backend-weekly.yml`
- Fail si tests Ã©chouent
- Coverage report upload (Codecov)

#### [DEBT-003] Embeddings non cachÃ©s (Redis)
**Impact :** Performance (recalcul chaque run)
**Estimation :** 8 SP
**Action :**
- Setup Redis (Docker ou cloud)
- Cache embeddings par hash(content)
- TTL 30 jours

#### [DEBT-004] Monitoring/Observability manquant
**Impact :** Bugs production non dÃ©tectÃ©s
**Estimation :** 8 SP
**Action :**
- IntÃ©grer Sentry (backend + frontend)
- Alertes Slack si erreurs
- Dashboard mÃ©triques (articles/semaine, sources down, etc.)

### P2 - Moyenne (Backlog)

#### [DEBT-005] Pas de staging environment
**Impact :** Test en prod uniquement
**Estimation :** 5 SP

#### [DEBT-006] SQLite single-writer (pas de WAL)
**Impact :** Performance DB limitÃ©e
**Estimation :** 2 SP

#### [DEBT-007] Pas de Dependabot (CVE scanning)
**Impact :** DÃ©pendances vulnÃ©rables non dÃ©tectÃ©es
**Estimation :** 1 SP

#### [DEBT-008] Frontend JSON non paginÃ©
**Impact :** Performance si > 100 semaines
**Estimation :** 5 SP

---

## 12. Ce qui Manque (Gaps)

### Infrastructure

- [ ] **Monitoring** : Sentry, Datadog, New Relic
- [ ] **Staging** : Environnement de test prÃ©-prod
- [ ] **Backup strategy** : Backup SQLite automatique
- [ ] **Rollback strategy** : Version antÃ©rieure si deploy fail
- [ ] **Health checks** : Endpoint `/health` pour monitoring

### Code

- [ ] **Tests frontend** : Coverage 0% actuellement
- [ ] **Tests E2E** : Playwright flows complets
- [ ] **CI tests** : pytest + vitest automatiques
- [ ] **API documentation** : Swagger/OpenAPI (si API prod)
- [ ] **Error tracking** : Sentry integration

### SÃ©curitÃ©

- [ ] **Dependabot** : Scans CVE automatiques
- [ ] **Secret scanning** : GitHub Secret Scanning
- [ ] **CSP headers** : Content Security Policy
- [ ] **Rate limiting** : Sur API si prod
- [ ] **HTTPS enforcement** : Redirection forcÃ©e

### Performance

- [ ] **Redis cache** : Embeddings + LLM responses
- [ ] **CDN** : CloudFront/Cloudflare pour assets
- [ ] **Image optimization** : WebP + lazy loading avancÃ©
- [ ] **Bundle analysis** : Webpack Bundle Analyzer

### Features

- [ ] **Mode sombre** : Dark mode toggle
- [ ] **Export PDF** : Digest en PDF
- [ ] **Notifications** : Slack/email hebdomadaire
- [ ] **Personnalisation** : Filtres sauvegardÃ©s par user
- [ ] **Analytics** : Tendances, stats, graphiques

---

## 13. Score SantÃ© Globale : 73/100

| CritÃ¨re | Score | Commentaire |
|---------|-------|-------------|
| **Architecture** | 18/20 | Pipeline clair, modulaire, bien sÃ©parÃ© |
| **Tests** | 10/20 | Backend partiellement testÃ©, frontend 0% |
| **Documentation** | 18/20 | README excellent, CLAUDE.md complet |
| **SÃ©curitÃ©** | 14/20 | Bonnes pratiques, mais monitoring manquant |
| **Performance** | 13/20 | Asyncio bien utilisÃ©, manque caching |
| **TOTAL** | **73/100** | **Bon - Quelques amÃ©liorations nÃ©cessaires** |

---

## 14. Points d'Extension Future

### ModularitÃ©

Le code est conÃ§u pour Ãªtre facilement extensible :

1. **Nouveaux LLM providers** : Abstraction OpenAI-compatible (Groq, OpenAI, Ollama)
2. **Nouveaux modÃ¨les embeddings** : Configurable (actuellement hardcodÃ©)
3. **Nouveaux scoring components** : Ajouter freshness_score, engagement_score, etc.
4. **Nouveaux storage backends** : PostgreSQL au lieu de SQLite
5. **Nouveaux export formats** : PDF, HTML email, Notion, etc.
6. **Nouveaux workflows** : Dbt, Airflow pour orchestration avancÃ©e

### IntÃ©grations Possibles

- **Slack bot** : Commande `/veille` pour digest
- **Email newsletter** : Envoi automatique hebdomadaire
- **API publique** : Exposer l'API pour tiers
- **Webhook** : Notifier services externes (Zapier, etc.)
- **RSS feed** : GÃ©nÃ©rer RSS du digest

---

## 15. Conclusion

L'architecture de **Veille Tech Crawling** est **solide, moderne et bien conÃ§ue**. Le pipeline backend est **modulaire et asynchrone**, le frontend est **type-safe et performant**, et le dÃ©ploiement est **automatisÃ© via GitHub Actions**.

**Forces principales :**
- Pipeline ETL clair en 4 phases
- Scoring multi-critÃ¨res sophistiquÃ©
- Interface utilisateur moderne et responsive
- DÃ©ploiement automatique et fiable
- Code bien documentÃ© et structurÃ©

**Axes d'amÃ©lioration prioritaires :**
1. Tests frontend (Vitest + Playwright)
2. CI/CD pour tests automatiques
3. Monitoring avec Sentry
4. Cache Redis pour embeddings

Le projet est **prÃªt pour la production** et dÃ©jÃ  dÃ©ployÃ© avec succÃ¨s. Les amÃ©liorations identifiÃ©es sont des **optimisations incrÃ©mentales** plutÃ´t que des problÃ¨mes bloquants.
