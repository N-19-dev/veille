{
  "ai_data_engineering": [
    {
      "url": "https://vutr.substack.com/p/i-spent-8-hours-learning-about-vector",
      "title": "I spent 8 hours learning about vector databases",
      "summary": "From their typical workload, how it stores to how it serves the data",
      "published_ts": 1762830936,
      "source_name": "VuTrinh · Data Engineering",
      "score": 62.79399165511131
    },
    {
      "url": "https://www.databricks.com/blog/unlocking-future-automotive-industry-part-2-implementing-scalable-geospatial-analytics-ai",
      "title": "Unlocking the future of the Automotive Industry (Part 2): Implementing Scalable Geospatial Analytics & AI",
      "summary": "In Part 1, we explored core concepts and datasets driving geospatial analytics in...",
      "published_ts": 1762882200,
      "source_name": "Databricks Blog",
      "score": 60.593836814165115
    },
    {
      "url": "https://www.databricks.com/blog/native-openai-models-now-generally-available-databricks",
      "title": "Native OpenAI Models Now Generally Available on Databricks",
      "summary": "OpenAI models, including GPT-5.1, are now generally available on the Databricks Data...",
      "published_ts": 1763056800,
      "source_name": "Databricks Blog",
      "score": 60.150169879198074
    },
    {
      "url": "https://dagster.io/blog/compass-now-available",
      "title": "Compass: AI Data Analysis for Your Warehouse, in Slack",
      "summary": "Connect your Snowflake, BigQuery, Redshift, or Databricks warehouse to Compass and get AI-powered answers in Slack. Your data stays where it is. Your governance stays intact. Available now.",
      "published_ts": 1763054436,
      "source_name": "Dagster Blog",
      "score": 58.24791884422302
    }
  ],
  "cloud": [],
  "cloud_infra_observability": [
    {
      "url": "https://aws.amazon.com/blogs/big-data/how-yelp-modernized-its-data-infrastructure-with-a-streaming-lakehouse-on-aws/",
      "title": "How Yelp modernized its data infrastructure with a streaming lakehouse on AWS",
      "summary": "This is a guest post by Umesh Dangat, Senior Principal Engineer for Distributed Services and Systems at Yelp, and Toby Cole, Principle Engineer for Data Processing at Yelp, in partnership with AWS. Yelp processes massive amounts of user data daily—over 300 million business reviews, 100,000 photo uploads, and countless check-ins. Maintaining sub-minute data freshness with […]",
      "published_ts": 1763057242,
      "source_name": "Redshift / AWS Big Data",
      "score": 57.524667263031006
    },
    {
      "url": "https://aws.amazon.com/blogs/big-data/your-guide-to-aws-analytics-at-aws-reinvent-2025/",
      "title": "Your guide to AWS Analytics at AWS re:Invent 2025",
      "summary": "It’s that time of year again — AWS re:Invent is here! At re:Invent, bold ideas come to life. Get a front-row seat to hear inspiring stories from AWS experts, customers, and leaders as they explore today’s most impactful topics, from data analytics to AI. For all the data enthusiasts and professionals, we’ve curated a comprehensive […]",
      "published_ts": 1763064379,
      "source_name": "Redshift / AWS Big Data",
      "score": 55.04175837337971
    }
  ],
  "data_modeling_governance": [
    {
      "url": "https://www.databricks.com/blog/accelerating-secure-interoperable-identity-collaboration-trade-desk-and-databricks-partnership",
      "title": "Accelerating Secure, Interoperable Identity Collaboration: The Trade Desk and Databricks Partnership",
      "summary": "Databricks is pleased to announce a new technology integration with The Trade Desk...",
      "published_ts": 1763047800,
      "source_name": "Databricks Blog",
      "score": 56.917933747172356
    },
    {
      "url": "https://blog.octo.com/du-dataset-jetable-au-data-product-perenne--comment-le-data-mesh-transforme-notre-rapport-a-la-donnee",
      "title": "Du dataset jetable au data product pérenne : comment le data mesh transforme notre rapport à la donnée",
      "summary": "Dans les organisations où les données sont nombreuses, hétérogènes et stratégiques, leur mise à disposition efficace devient un levier clé de performance. L’approche data mesh, en décentralisant la gouvernance et en traitant la donnée comme un produit, promet une meilleure agilité et une appropriation renforcée par les équipes métier.",
      "published_ts": 1763126418,
      "source_name": "OCTO Talks!",
      "score": 56.24236725270748
    },
    {
      "url": "https://www.databricks.com/blog/how-scale-data-governance-attribute-based-access-control-unity-catalog",
      "title": "How to scale data governance with Attribute-Based Access Control in Unity Catalog",
      "summary": "As organizations democratize access to data to accelerate analytics and AI, maintaining...",
      "published_ts": 1763042400,
      "source_name": "Databricks Blog",
      "score": 54.532224997878075
    }
  ],
  "dataprep_orchestration_etl": [],
  "db_sql_olap": [],
  "etl_orchestration": [
    {
      "url": "https://dagster.io/blog/what-dagster-believes-about-data-platforms",
      "title": "What Dagster Believes About Data Platforms",
      "summary": "The beliefs that organizations adopt about the way their data platforms should function influence their outcomes. Here are ours.",
      "published_ts": 1762799248,
      "source_name": "Dagster Blog",
      "score": 65.23372974991798
    }
  ],
  "hors_sujet": [],
  "lake_storage_formats": [],
  "ml_ai": [],
  "news": [
    {
      "url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-245",
      "title": "Data Engineering Weekly #245",
      "summary": "The Weekly Data Engineering Newsletter",
      "published_ts": 1762742511,
      "source_name": "Data Engineering Weekly",
      "score": 71.37995100021362
    }
  ],
  "news_general": [],
  "python_analytics": [],
  "python_polars_duckdb": [],
  "viz_bi": [],
  "warehouses_engines": [
    {
      "url": "https://blog.dataiku.com/modernizing-analytics-with-dataiku-and-snowflake",
      "title": "Modernizing Analytics With Dataiku and Snowflake",
      "summary": "Today’s enterprises move at a speed legacy systems can’t match. Many still rely on outdated analytics tools built for a slower, simpler era. These systems fragment workflows, duplicate data, and slow the path from insight to action. To stay competitive, organizations need connected, scalable, and governed analytics foundations that bring together people, processes, and data in real time.",
      "published_ts": 1762871367,
      "source_name": "Dataiku Blog",
      "score": 61.30188262462616
    },
    {
      "url": "http://www.dataiku.com/stories/blog/modernizing-analytics-with-dataiku-and-snowflake",
      "title": "Modernizing Analytics With Dataiku and Snowflake",
      "summary": "Today’s enterprises move at a speed legacy systems can’t match. Many still rely on outdated analytics tools built for a slower, simpler era. These systems fragment workflows, duplicate data, and slow the path from insight to action. To stay competitive, organizations need connected, scalable, and governed analytics foundations that bring together people, processes, and data in real time.",
      "published_ts": 1762871367,
      "source_name": "Dataiku Blog",
      "score": 61.30188262462616
    },
    {
      "url": "https://www.databricks.com/blog/accelerating-data-and-ai-google-axion-c4a-vms-databricks",
      "title": "Accelerating Data and AI with Google Axion C4A VMs on Databricks",
      "summary": "Databricks customers on Google Cloud running in Classic compute environments can...",
      "published_ts": 1762900800,
      "source_name": "Databricks Blog",
      "score": 58.52611815929413
    },
    {
      "url": "https://www.lemagit.fr/actualites/366634267/Snowflake-veut-lui-aussi-faire-de-PostgreSQL-un-lakehouse-ouvert",
      "title": "Snowflake veut lui aussi faire de PostgreSQL un lakehouse ouvert",
      "summary": "<p>Dans sa course contre Databricks pour offrir la plateforme de gestion de données la plus complète du marché, l’éditeur lancera «&nbsp;bientôt&nbsp;» sa version managée de PostgreSQL en préversion publique. Pour mémoire, il avait annoncé en juin dernier <a href=\"https://www.lemagit.fr/actualites/366625313/Comme-Databricks-Snowflake-acquiert-un-specialiste-de-PostgreSQL\">l’acquisition de Crunchy Data.</a> Databricks, lui, a racheté, peu de temps avant<a href=\"https://www.lemagit.fr/actualites/366623965/PostgreSQL-serverless-Databricks-sapprete-a-acquerir-Neon\">, Neon, un autre spécialiste</a> de la base de données relationnelle open source.</p> \n<p>Snowflake a surtout profité de son événement Build de la semaine dernière pour libérer pg_lake, une suite d’extensions ouverte pour le SGBDR le plus populaire du marché. Elles permettent de gérer des tables Apache Iceberg dans <a href=\"https://www.lemagit.fr/definition/PostgreSQL\">PostgreSQL</a>. Il est également possible d’interroger des tables étrangères sur S3 ou Azure Blob Storage, qu’elles soient au format CSV, JSON, Parquet et Iceberg. Pg_lake dispose de fonctions pour charger des données depuis des buckets&nbsp;S3 ou d’une URL dans des tables Iceberg ou PostgreSQL. Enfin, les résultats des requêtes peuvent être sauvegardés dans des buckets&nbsp;S3, afin de poursuivre les traitements. À noter que pg_lake lit des fichiers les formats géospatiaux, dont GeoJSON.</p> \n<section class=\"section main-article-chapter\" data-menu-title=\"Combiner les capacités de Postgres, de DuckDB et d’Iceberg\">\n <h2 class=\"section-title\"><i class=\"icon\" data-icon=\"1\"></i>Combiner les capacités de Postgres, de DuckDB et d’Iceberg</h2>\n <p>Les extensions pg_lake s’intègrent à PostgreSQL pour gérer «&nbsp;la planification des requêtes, les périmètres des transactions et l’orchestration générale des exécutions&nbsp;».</p>\n <p>Sous le capot, un serveur DuckDB (nommé pgduck_server) est utilisé pour externaliser les scans et les traitements parallèles. DuckDB est un moteur analytique orienté colonnes (OLAP) open source. Il est réputé pour gérer des requêtes complexes à partir de grandes bases de données.</p>\n <p>Le projet s’appuie sur le protocole TCP de PostgreSQL pour interconnecter PostgreSQL et DuckDB.</p>\n <p>&nbsp;«&nbsp;Toute personne disposant de Postgres et souhaitant la transformer en interface pour gérer un lac ouvert avec des tables Iceberg pourra le faire à sa guise&nbsp;», promet Christian Kleinerman, vice-président directeur du produit chez Snowflake, lors d’une conférence de presse en ligne.</p>\n <p>C’était une des fonctionnalités au cœur de Crunchy Bridge for Analytics, dont le développement a commencé au début de l’année 2024. Un moyen de simplifier la jonction entre les données opérationnelles et analytiques. Ces fonctionnalités sont ensuite devenues le cœur de l’offre Crunchy Data Warehouse, avant que Snowflake acquière la société.</p>\n <p>Pour rappel, ce n’est pas dans la nature de PostgreSQL que de traiter des charges de travail analytiques. C’est, à l’origine, une base de données transactionnelle.</p>\n <p>«&nbsp;L’industrie des bases de données est divisée en deux grandes factions&nbsp;: les systèmes OLTP et OLAP. Elles ne se parlent pas trop, mais elles sont d’accord sur la gestion de transactions et l’usage de SQL&nbsp;», <a href=\"https://www.youtube.com/watch?v=tpq4nfEoioE\">résume</a> Marco Slot, ingénieur logiciel principal chez Snowflake (ex-Crunchy Data) lors d’un événement Microsoft Developer. «&nbsp;Les systèmes OLTP comme PostgreSQL prennent en charge de grande quantité de transactions telles des petites mises à jour, des changements ligne à ligne, de petites sélections, etc., à une faible latence et à haut débit&nbsp;», poursuit-il. «&nbsp;Les systèmes OLAP se concentrent sur le scan à haute vitesse d’un grand volume de données en une seule requête. C’est un autre problème d’optimisation&nbsp;».</p>\n <p>Outre la différence d’orientation du format de stockage (en ligne pour les systèmes OLTP, en colonne pour les systèmes OLAP), les systèmes analytiques modernes, dont Databricks, Snowflake et DuckDB s’appuient sur des moteurs d’exécution vectorisée. Ceux-là permettent de traiter des données en lot plutôt que ligne par ligne. «&nbsp;Le moteur de requêtes prend un vecteur de valeurs – un lot de valeurs – et les expressions sont implémentées comme des boucles&nbsp;», explique Marco Slot. «&nbsp;Ils font la même comparaison sur toutes les valeurs du vecteur. Et le résultat peut être un nouveau vecteur avec, par exemple, les index des lignes qui correspondent aux filtres de la requête&nbsp;».</p>\n <p>Au lieu de changer d’expression constamment, ici le traitement en batch abaisse le nombre d’appels de fonctions, le processeur peut mieux prédire la suite de l’exécution, le cache CPU est utilisé à bon escient, tandis que les instructions SIMD des processeurs permettent de traiter plusieurs valeurs en une seule requête.</p>\n <p>«&nbsp;En couplant cela à la parallélisation, les bases de données OLAP dotées d’un tel moteur peuvent être 10, voire plus de 100&nbsp;fois plus rapides que PostgreSQL pour des requêtes analytiques&nbsp;», estime l’ingénieur principal.</p>\n <p>Or, il n’est pas simple de bâtir <a href=\"https://www.lemagit.fr/actualites/252522169/Databricks-cimente-les-briques-de-son-data-lakehouse\">un moteur à exécution vectorisée</a>. De plus,&nbsp;«&nbsp;d’autres composants de Postgres ne sont pas optimisés pour l’analytique&nbsp;», ajoute Marco Slot.</p>\n <p>D’où l’usage du «&nbsp;superpouvoir&nbsp;» de PostgreSQL&nbsp;: les extensions. «&nbsp;Il&nbsp;y a quelques années, ce n’était pas une base de données vectorielle, mais maintenant nous avons pg_vector. Ce n’était pas une base de données distribuée, mais le projet <a href=\"https://www.lemagit.fr/actualites/252456460/Microsoft-renforce-les-capacites-de-dimensionnement-de-Postgresql-sur-Azure\">Citus a changé la donne&nbsp;</a>», illustre l’ingénieur principal. «&nbsp;De la même manière, nous pouvons faire de Postgres une bonne base de données analytique à travers les extensions&nbsp;».</p>\n <p>En tant que <a href=\"https://www.lemagit.fr/actualites/366627790/Apptio-affine-sa-gestion-des-couts-de-lIA-et-du-cloud-hybride\">moteur d’exécution vectorisée embarquée, DuckDB</a> était considéré comme le plus adapté à cette tâche. «&nbsp;La licence est similaire, il cible une seule machine, son dialecte SQL est dérivé de PostgreSQL&nbsp;et il a aussi des extensions&nbsp;», justifie Marco Slot.</p>\n</section>              \n<section class=\"section main-article-chapter\" data-menu-title=\"Rapprocher PostgreSQL des lakehouse, une initiative commune à l’écosystème Data\">\n <h2 class=\"section-title\"><i class=\"icon\" data-icon=\"1\"></i>Rapprocher PostgreSQL des lakehouse, une initiative commune à l’écosystème Data</h2>\n <p>Christian Kleinerman envisage que les développeurs utilisent cette extension dans le cadre de développement d’applications. «&nbsp;Je pense que l’un des cas d’usage le plus courant consiste pour les développeurs à créer des applications avec Postgres, puis à effectuer un ETL ou à copier les données à des fins d’analyse dans une plateforme de données telle que Snowflake ou, de plus en plus, dans un lac de données ouvert, avec S3 Table sur AWS ou avec Iceberg, <a href=\"https://www.lemagit.fr/actualites/366631554/Fabric-face-a-Snowflake-et-Databricks-Microsoft-met-les-bouchees-doubles\">sur Microsoft OneLake&nbsp;</a>», indique-t-il, dans un briefing avec la presse. «&nbsp;C’est ce que permet PG Lake. […] Nous avons pensé que c’était plus important que Snowflake et que Snowflake Postgres. D’où la libération du projet&nbsp;».</p>\n <p>Ce n’est pas le premier projet du genre. À travers MotherDuck, l’éditeur DuckDB propose une implémentation différente. Il embarque directement une instance de son moteur dans PostgreSQL. Il faut également noter que Databricks a mis la main au début du mois d’octobre sur Mooncake Labs, une startup basée à San Francisco. Celle-ci a développé pg_mooncake, une extension open source permettant de créer un miroir inversé de tables Postgres dans un format orienté colonne Apache Iceberg. Les données sont ingérées avec sa technologie moonlink et traitées avec… DuckDB.</p>\n</section>   \n<section class=\"section main-article-chapter\" data-menu-title=\"À la croisée des changements de paradigme\">\n <h2 class=\"section-title\"><i class=\"icon\" data-icon=\"1\"></i>À la croisée des changements de paradigme</h2>\n <p>Il y a quelques limitations. La première étant qu’il n’est pas encore possible d’utiliser de catalogue externe à PostgreSQL pour Iceberg dans le cas de pg_lake. Databricks, Cloudera, Dremio, Snowflake et d’autres utilisent désormais des catalogues de métadonnées externes comme Hive, <a href=\"https://www.lemagit.fr/actualites/366599652/Apache-Iceberg-Snowflake-libere-Polaris-et-sallie-a-Dremio\">Polaris ou Unity</a>. Cette gestion de la gouvernance des tables Iceberg (et leur contrôle) n’est pas encore standard.&nbsp;</p>\n <p>Néanmoins, les experts du milieu décrivent deux phénomènes qui convergent. Après la naissance des lakehouse – qui combinent les capacités des entrepôts et des lacs de données – la frontière entre la base et le lac de données s’effacerait, selon Manish K., architecte Big Data chez Accenture.</p>\n <p>«&nbsp;DuckDB a prouvé que nous pouvons exécuter des analyses sérieuses directement sur des fichiers&nbsp;», <a href=\"https://www.linkedin.com/posts/activity-7393243881011798016-YogO?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABfZ9MABbYoGamxFbwWKF3lQzOgcuryCmSk\">écrit</a> Manish K. sur LinkedIn. «&nbsp;pg_lake et pg_mooncake ont prouvé que Postgres peut parler [le langage] des tables de style Iceberg dans un espace de stockage objet. Les services Postgres cloud-first (calcul/stockage découplé, branchable, élastique) ressemble désormais à un lakehouse sur le plan architectural&nbsp;», ajoute-t-il. «&nbsp;Le schéma est donc clair&nbsp;: la base de données se déplace là où se trouve déjà le lac&nbsp;». Et d’utiliser l’expression «&nbsp;lakebase&nbsp;».</p>\n <p>Bhairav Metha, directeur data science chez JPMorganChase,&nbsp;voit un autre changement. Alors que les systèmes <a href=\"https://www.lemagit.fr/actualites/252473646/Selon-Forrester-les-bases-de-donnees-translytiques-ont-le-vent-en-poupe\">translytiques</a> sont jusqu’à présent majoritairement propriétaires, ils pourraient demain s’appuyer sur un projet open source&nbsp;: PostgreSQL.</p>\n <p>«&nbsp;Nous n’y sommes peut-être pas encore tout à fait – l’OLAP et l’OLTP répondent encore à des objectifs d’optimisation distincts – mais la trajectoire est claire&nbsp;», <a href=\"https://www.linkedin.com/posts/activity-7393518197771182080-E9o3?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABfZ9MABbYoGamxFbwWKF3lQzOgcuryCmSk\">avance</a>-t-il. «&nbsp;Postgres pourrait bientôt gérer 90&nbsp;% des charges de travail modernes sans avoir besoin d’un entrepôt ou d’un Lakehouse séparé&nbsp;».</p>\n</section>",
      "published_ts": 1762757220,
      "source_name": "LeMagIT",
      "score": 56.93469998240471
    },
    {
      "url": "https://www.databricks.com/blog/explore-data-instantly-databricks-assistant-unity-catalog",
      "title": "Explore Data Instantly with Databricks Assistant in Unity Catalog",
      "summary": "Every analyst knows the feeling: you open a dataset in your catalog, scroll through...",
      "published_ts": 1763042400,
      "source_name": "Databricks Blog",
      "score": 56.777297139167786
    }
  ]
}